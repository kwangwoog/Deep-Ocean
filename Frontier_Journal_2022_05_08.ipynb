{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "#import tensorflow as tf\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "#!pip install netCDF4\n",
    "#!pip install -U matplotlib==3.2\n",
    "#!pip uninstall -y numpy\n",
    "#!pip install numpy\n",
    "#!pip install pyproj==1.9.6\n",
    "#!apt-get install libgeos-3.5.0\n",
    "#!apt-get install libgeos-dev\n",
    "#!pip install https://github.com/matplotlib/basemap/archive/master.zip\n",
    "#!pip install sdv\n",
    "from netCDF4 import Dataset, num2date\n",
    "import pandas as pd\n",
    "#from matplotlib.cbook import dedent\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import xarray\n",
    "import netCDF4 as nc\n",
    "#conda install xarray or pip install xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_sst_lp=sst_dataset.sst.sel(lon=114.125,lat=14.125, method='nearest',time='1993-03-01')\n",
    "#dataset_sst_lp\n",
    "#dataset_sst_lp.plot()\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "month=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "for year in range (1982,1984):\n",
    "        for mo in month:\n",
    "            sst_nc_file ='sst_merge_month_'+str(year)+mo+'.nc'\n",
    "            print(sst_nc_file)\n",
    "            sst_dataset=xr.open_dataset(sst_nc_file)\n",
    "            #sst_dataset\n",
    "            dataset_sst=sst_dataset.sst\n",
    "            dataset_sst_lp = sst_dataset.sst.sel(lon=slice(125, 140), lat=slice(30, 45))\n",
    "            output_file='sst_east_sea_month_'+str(year)+mo+'.nc'\n",
    "            print(output_file)\n",
    "            dataset_sst_lp.to_netcdf(output_file)\n",
    "            #dataset_sst_lp.plot()\n",
    "        dataset_sst_lp.plot(col=\"time\",col_wrap=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst=dataset_sst_lp.to_dataframe()\n",
    "np_sst=df_sst.to_numpy()\n",
    "print(np_sst.shape)\n",
    "print(np_sst)\n",
    "#sst_numpy_array = np.stack([data_array.values for data_array in df_sst['sst'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sst.to_csv('sst.csv')\n",
    "df_sst=pd.read_csv('sst.csv')\n",
    "df_sst[['lat','lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-supervisor",
   "metadata": {},
   "source": [
    "## Make Raw Data without SSH (profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TF to use only the CPU\n",
    "\n",
    "#plot obs station\n",
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "\n",
    "print(df_station_point)\n",
    "print(df_station_point[['latitude']])\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "\n",
    "lat_ls=df_station_point_38[['latitude']].values.flatten()\n",
    "lon_ls=df_station_point_38[['longitude']].values.flatten()\n",
    "\n",
    "\n",
    "# Read Temperature Profile\n",
    "\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "col_names=['AREA','Station','Point','Station-Point','Latitude','Longitude','DATE','DEPTH','TEMPERATURE','TEMP/QC','SALINITY','SALINITY/QC','DISOLVED-OXYGEN','DISOLVED-OXYGEN/QC','QC Level','PHOROSPATE',' nitrite nitrogen','nitric acid nitrogen','silicic acid silicon','pH','Transparency','Pressure','Ship']\n",
    "\n",
    "path = '/gpu_deep/Deep_Ocean/obs_east/obs_*_east.xls'\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "files=glob.glob(path)\n",
    "for file in sorted(files):\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file[30:43])\n",
    "        Name=file[30:43]\n",
    "        #print(Name)\n",
    "        statement='/gpu_deep/Deep_Ocean/obs_east/'+ Name + '.xls'\n",
    "        print(statement)\n",
    "#        pd.read_excel('/content/drive/My Drive/Deep_Ocean/obs_east/obs_1965_east.xls', header=1,names=col_names)\n",
    "        df=df.append(pd.read_excel(statement, header=1,names=col_names))\n",
    "\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "print(df_station_point_38)\n",
    "\n",
    "df_station_point=df_station_point_38['Point'].to_list()\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "df_38=pd.DataFrame()\n",
    "\n",
    "print(\"------------------------DF_STATION_POINT_38----------------------------------------------\")\n",
    "for i in df_station_point:\n",
    "    #print(df[df[\"Station-Point\"]==i])\n",
    "    print(i)\n",
    "    df_38=df_38.append(df[df[\"Station-Point\"]==i])\n",
    "\n",
    "#print(df)\n",
    "temp=df_38\n",
    "print(temp)\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#ocean_df=temp[['DATE','Station','Point','Station-Point','Latitude','Longitude','DEPTH','TEMPERATURE','SALINITY']]\n",
    "ocean_df=temp[['DATE','Station-Point','DEPTH','TEMPERATURE']]\n",
    "ocean_df['DATE']=pd.to_datetime(ocean_df['DATE'],format='%Y-%m-%d %H:%M')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m')\n",
    "ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#select Feb\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02|-03|-04|-06\")]\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-\")]\n",
    "ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-|-04-|-06-|-08-|-10-|-12-\")]\n",
    "print(ocean_df)\n",
    "\n",
    "\n",
    "#ocean_df.pivot(index=['DATE','Station-Point'],columns='DEPTH',values='TEMPERATURE')\n",
    "\n",
    "ocean_grouped = ocean_df[\"DATE\"].unique()\n",
    "\n",
    "#np.unique(ocean_df[['DATE', 'Station-Point']].values)\n",
    "\n",
    "#print(np.unique(ocean_df[['DATE', 'Station-Point']].values))\n",
    "\n",
    "\n",
    "\n",
    "##2중 Loop\n",
    "reshape_ocean_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "obs_station= ocean_df[\"Station-Point\"].unique()\n",
    "\n",
    "#depth_criteria=[0,10,25,50,100,200,250,300,400,500]\n",
    "\n",
    "depth_criteria=[0,10,20,30,50,75,100,125,150,200,250,300,400,500]\n",
    "print(obs_station)\n",
    "\n",
    "#DEPTH=ocean_df_temp['DEPTH'][ocean_df_temp[\"DATE\"]==i].to_list()\n",
    "\n",
    "\n",
    "#tempDf = pd.DataFrame(columns=['PRODUCT','CAT_ID','MARKET_ID'])\n",
    "#tempDf['PRODUCT'] = df['PRODUCT']\n",
    "#tempDf['CAT_ID'] = catid\n",
    "#tempDf['MARKET_ID'] = 13\n",
    "\n",
    "#finalDf = pd.concat([finalDf,tempDf])\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point=pd.DataFrame(df_station_point)\n",
    "\n",
    "\n",
    "print(\"--------DF Station Point-------\")\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "from datetime import date\n",
    "#f_date = date(2014, 7, 2)\n",
    "#l_date = date(2014, 7, 11)\n",
    "#delta = l_date - f_date\n",
    "#print(delta.days)\n",
    "\n",
    "for i in obs_station :\n",
    "        obs_station_date=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]>'1965-01-01') & (ocean_df[\"DATE\"]<'2018-01-01'), \"DATE\"].unique()\n",
    "        print(obs_station_date)\n",
    "        for j in obs_station_date :\n",
    "                    DEPTH=ocean_df['DEPTH'][(ocean_df[\"DATE\"]==j) & (ocean_df[\"Station-Point\"]==i)].to_list()\n",
    "                    DEPTH.sort()\n",
    "                    if DEPTH==depth_criteria:\n",
    "                        print(j,i)\n",
    "                        start_dt=date(int(j[0:4]),1,1)\n",
    "                        print(j[5:7], j[8:10])\n",
    "                        end_dt=date(int(j[0:4]),int(j[5:7]), int(j[8:10]))\n",
    "                        delta=end_dt-start_dt\n",
    "                        ssh_days=delta.days\n",
    "                        obs_lat=df_station_point.loc[(df_station_point['Point']==i),\"latitude\"].values\n",
    "                        obs_lon=df_station_point.loc[(df_station_point['Point']==i),\"longitude\"].values\n",
    "                        #print(\"LAT,LON:\",obs_lat,obs_lon)\n",
    "                        #sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        #sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_adt_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        #print(sst_nc_file)\n",
    "                        #ssh_dataset=xr.open_dataset(sst_nc_file)\n",
    "                        #print(ssh_dataset)\n",
    "                        #dataset_ssh=ssh_dataset.sla\n",
    "                        #dataset_ssh=ssh_dataset.adt\n",
    "                        #dataset_ssh_lp = ssh_dataset.sla.sel(longitude=slice(120, 140), latitude=slice(30, 40))\n",
    "                        #dataset_ssh_lp = ssh_dataset.adt.sel(time=j, latitude=obs_lat, longitude=obs_lon, method=\"nearest\")\n",
    "                        #print(\"SSH:\", dataset_ssh_lp)\n",
    "                        #output_file='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(year)+'.nc'\n",
    "                        #print(output_file)\n",
    "                        #dataset_ssh_lp.to_netcdf(output_file)                                                \n",
    "                        temp_df[\"DATE\"]=j\n",
    "                        temp_df[\"Station-Point\"]=i\n",
    "                        temp_df[\"Latitude\"]=obs_lat\n",
    "                        temp_df[\"Longitude\"]=obs_lon\n",
    "                        #temp_df[\"SSH\"]=dataset_ssh_lp.values\n",
    "                        temp_df[\"0\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==0),\"TEMPERATURE\"].values \n",
    "                        temp_df[\"10\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==10),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"20\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==20),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"30\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==30),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"50\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==50),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"75\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==75),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"100\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==100),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"125\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==125),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"150\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==150),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"200\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==200),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"250\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==250),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"300\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==300),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"400\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==400),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"500\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==500),\"TEMPERATURE\"].values\n",
    "                        #print(temp_df)\n",
    "                        reshape_ocean_df=pd.concat([reshape_ocean_df,temp_df])\n",
    "                        \n",
    "print(reshape_ocean_df[reshape_ocean_df['DATE']!=reshape_ocean_df['DATE']])\n",
    "\n",
    "reshape_ocean_df.reset_index()\n",
    "                        \n",
    "                        \n",
    "#                    for k in depth_criteria:\n",
    "#                        if DEPTH==depth_criteria:\n",
    "#                            temp_depth=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==k)]\n",
    "#                            print(temp_depth)\n",
    "#                            #print(i,j,k,temp_depth)\n",
    "                    \n",
    "\n",
    "print(reshape_ocean_df)\n",
    "\n",
    "reshape_ocean_df_no_ssh=pd.read_csv('reshape_ocean_df_no_ssh.csv')  \n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "print(reshape_ocean_df.drop([0,0]))\n",
    "\n",
    "\n",
    "reshape_ocean_df_1993_no_ssh=reshape_ocean_df_no_ssh[reshape_ocean_df_no_ssh[\"DATE\"]>'1993-01-01']\n",
    "\n",
    "reshape_ocean_df_1993_no_ssh.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print(reshape_ocean_df_1993_no_ssh)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ls_ocean_dt=ocean_df['DATE'].unique()\n",
    "ls_station_df=ocean_df['Station-Point'].unique()\n",
    "print(ls_station_df.shape)\n",
    "obs_station_lst=ls_station_df.tolist()\n",
    "print(obs_station_lst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import dedent\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "#m = Basemap(projection='lcc', resolution='f',\n",
    "#            width=1E6, height=1E6, \n",
    "#            lat_0=37, lon_0=132,)\n",
    "#m.etopo(scale=0.5, alpha=0.5)\n",
    "m = Basemap(projection='lcc', resolution='f',\n",
    "          lat_0=37, lon_0=132, \n",
    "          llcrnrlon=120.25, llcrnrlat=30.0,\n",
    "          urcrnrlon=140.25, urcrnrlat=40.75)\n",
    "m.etopo(scale=3.0, alpha=2.0)\n",
    "\n",
    "\n",
    "lons, lats = m(lon_ls, lat_ls)\n",
    "#m.scatter(lons, lats, marker = 'o', color='r', zorder=5)\n",
    "m.plot(lons,lats,'ro',markersize=18)\n",
    "plt.show()\n",
    "\n",
    "# Map (long, lat) to (x, y) for plotting\n",
    "#x, y = m(132, 37)\n",
    "#plt.plot(x, y, 'ok', markersize=10)\n",
    "#plt.text(x, y, ' Seoul', fontsize=12);\n",
    "\n",
    "#m.scatter(lon, lat, marker = 'o', color='r', zorder=5)\n",
    "\n",
    "#m.scatter(lon, lat, latlon=True,\n",
    "#          c=np.log10(population), s=area,\n",
    "#          cmap='Reds', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_no_ssh=reshape_ocean_df_no_ssh.dropna()\n",
    "reshape_ocean_df_no_ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TF to use only the CPU\n",
    "\n",
    "#plot obs station\n",
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "\n",
    "print(df_station_point)\n",
    "print(df_station_point[['latitude']])\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "\n",
    "lat_ls=df_station_point_38[['latitude']].values.flatten()\n",
    "lon_ls=df_station_point_38[['longitude']].values.flatten()\n",
    "\n",
    "\n",
    "# Read Temperature Profile\n",
    "\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "col_names=['AREA','Station','Point','Station-Point','Latitude','Longitude','DATE','DEPTH','TEMPERATURE','TEMP/QC','SALINITY','SALINITY/QC','DISOLVED-OXYGEN','DISOLVED-OXYGEN/QC','QC Level','PHOROSPATE',' nitrite nitrogen','nitric acid nitrogen','silicic acid silicon','pH','Transparency','Pressure','Ship']\n",
    "\n",
    "path = '/gpu_deep/Deep_Ocean/obs_east/obs_*_east.xls'\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "files=glob.glob(path)\n",
    "for file in sorted(files):\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file[30:43])\n",
    "        Name=file[30:43]\n",
    "        #print(Name)\n",
    "        statement='/gpu_deep/Deep_Ocean/obs_east/'+ Name + '.xls'\n",
    "        print(statement)\n",
    "#        pd.read_excel('/content/drive/My Drive/Deep_Ocean/obs_east/obs_1965_east.xls', header=1,names=col_names)\n",
    "        df=df.append(pd.read_excel(statement, header=1,names=col_names))\n",
    "\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "print(df_station_point_38)\n",
    "\n",
    "df_station_point=df_station_point_38['Point'].to_list()\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "df_38=pd.DataFrame()\n",
    "\n",
    "print(\"------------------------DF_STATION_POINT_38----------------------------------------------\")\n",
    "for i in df_station_point:\n",
    "    #print(df[df[\"Station-Point\"]==i])\n",
    "    print(i)\n",
    "    df_38=df_38.append(df[df[\"Station-Point\"]==i])\n",
    "\n",
    "#print(df)\n",
    "temp=df_38\n",
    "print(temp)\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#ocean_df=temp[['DATE','Station','Point','Station-Point','Latitude','Longitude','DEPTH','TEMPERATURE','SALINITY']]\n",
    "ocean_df=temp[['DATE','Station-Point','DEPTH','TEMPERATURE']]\n",
    "ocean_df['DATE']=pd.to_datetime(ocean_df['DATE'],format='%Y-%m-%d %H:%M')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m')\n",
    "ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#select Feb\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02|-03|-04|-06\")]\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-\")]\n",
    "ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-|-04-|-06-|-08-|-10-|-12-\")]\n",
    "#print(ocean_df)\n",
    "\n",
    "\n",
    "#ocean_df.pivot(index=['DATE','Station-Point'],columns='DEPTH',values='TEMPERATURE')\n",
    "\n",
    "ocean_grouped = ocean_df[\"DATE\"].unique()\n",
    "\n",
    "#np.unique(ocean_df[['DATE', 'Station-Point']].values)\n",
    "\n",
    "#print(np.unique(ocean_df[['DATE', 'Station-Point']].values))\n",
    "\n",
    "\n",
    "\n",
    "##2중 Loop\n",
    "reshape_ocean_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "obs_station= ocean_df[\"Station-Point\"].unique()\n",
    "\n",
    "#depth_criteria=[0,10,25,50,100,200,250,300,400,500]\n",
    "\n",
    "depth_criteria=[0,10,20,30,50,75,100,125,150,200,250,300,400,500]\n",
    "print(obs_station)\n",
    "\n",
    "#DEPTH=ocean_df_temp['DEPTH'][ocean_df_temp[\"DATE\"]==i].to_list()\n",
    "\n",
    "\n",
    "#tempDf = pd.DataFrame(columns=['PRODUCT','CAT_ID','MARKET_ID'])\n",
    "#tempDf['PRODUCT'] = df['PRODUCT']\n",
    "#tempDf['CAT_ID'] = catid\n",
    "#tempDf['MARKET_ID'] = 13\n",
    "\n",
    "#finalDf = pd.concat([finalDf,tempDf])\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point=pd.DataFrame(df_station_point)\n",
    "\n",
    "\n",
    "print(\"--------DF Station Point-------\")\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "from datetime import date\n",
    "#f_date = date(2014, 7, 2)\n",
    "#l_date = date(2014, 7, 11)\n",
    "#delta = l_date - f_date\n",
    "#print(delta.days)\n",
    "\n",
    "for i in obs_station :\n",
    "        obs_station_date=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]>'1993-01-01') & (ocean_df[\"DATE\"]<'2018-01-01'), \"DATE\"].unique()\n",
    "        print(obs_station_date)\n",
    "        for j in obs_station_date :\n",
    "                    DEPTH=ocean_df['DEPTH'][(ocean_df[\"DATE\"]==j) & (ocean_df[\"Station-Point\"]==i)].to_list()\n",
    "                    DEPTH.sort()\n",
    "                    if DEPTH==depth_criteria:\n",
    "                        print(j,i)\n",
    "                        start_dt=date(int(j[0:4]),1,1)\n",
    "                        print(j[5:7], j[8:10])\n",
    "                        end_dt=date(int(j[0:4]),int(j[5:7]), int(j[8:10]))\n",
    "                        delta=end_dt-start_dt\n",
    "                        ssh_days=delta.days\n",
    "                        obs_lat=df_station_point.loc[(df_station_point['Point']==i),\"latitude\"].values\n",
    "                        obs_lon=df_station_point.loc[(df_station_point['Point']==i),\"longitude\"].values\n",
    "                        #print(\"LAT,LON:\",obs_lat,obs_lon)\n",
    "                        sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_adt_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        #print(sst_nc_file)\n",
    "                        ssh_dataset=xr.open_dataset(sst_nc_file)\n",
    "                        #print(ssh_dataset)\n",
    "                        #dataset_ssh=ssh_dataset.sla\n",
    "                        dataset_ssh=ssh_dataset.adt\n",
    "                        #dataset_ssh_lp = ssh_dataset.sla.sel(longitude=slice(120, 140), latitude=slice(30, 40))\n",
    "                        dataset_ssh_lp = ssh_dataset.adt.sel(time=j, latitude=obs_lat, longitude=obs_lon, method=\"nearest\")\n",
    "                        #print(\"SSH:\", dataset_ssh_lp)\n",
    "                        output_file='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(year)+'.nc'\n",
    "                        #print(output_file)\n",
    "                        dataset_ssh_lp.to_netcdf(output_file)                                                \n",
    "                        temp_df[\"DATE\"]=j\n",
    "                        temp_df[\"Station-Point\"]=i\n",
    "                        temp_df[\"Latitude\"]=obs_lat\n",
    "                        temp_df[\"Longitude\"]=obs_lon\n",
    "                        temp_df[\"SSH\"]=dataset_ssh_lp.values\n",
    "                        temp_df[\"0\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==0),\"TEMPERATURE\"].values \n",
    "                        temp_df[\"10\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==10),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"20\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==20),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"30\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==30),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"50\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==50),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"75\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==75),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"100\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==100),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"125\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==125),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"150\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==150),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"200\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==200),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"250\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==250),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"300\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==300),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"400\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==400),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"500\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==500),\"TEMPERATURE\"].values\n",
    "                        #print(temp_df)\n",
    "                        reshape_ocean_df=pd.concat([reshape_ocean_df,temp_df])\n",
    "                        \n",
    "#print(reshape_ocean_df[reshape_ocean_df['DATE']!=reshape_ocean_df['DATE']])\n",
    "\n",
    "reshape_ocean_df.reset_index()\n",
    "                        \n",
    "                        \n",
    "#                    for k in depth_criteria:\n",
    "#                        if DEPTH==depth_criteria:\n",
    "#                            temp_depth=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==k)]\n",
    "#                            print(temp_depth)\n",
    "#                            #print(i,j,k,temp_depth)\n",
    "                    \n",
    "\n",
    "#print(reshape_ocean_df)\n",
    "\n",
    "reshape_ocean_df.to_csv('reshape_ocean_df.csv',index=False)\n",
    "\n",
    "reshape_ocean_df=pd.read_csv('reshape_ocean_df.csv')  \n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "#print(reshape_ocean_df.drop([0,0]))\n",
    "\n",
    "\n",
    "reshape_ocean_df_1993=reshape_ocean_df[reshape_ocean_df[\"DATE\"]>'1993-01-01']\n",
    "\n",
    "reshape_ocean_df_1993.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#print(reshape_ocean_df_1993)\n",
    "\n",
    "#ls_ocean_dt=ocean_df['DATE'].unique()\n",
    "ls_station_df=ocean_df['Station-Point'].unique()\n",
    "#print(ls_station_df.shape)\n",
    "obs_station_lst=ls_station_df.tolist()\n",
    "#print(obs_station_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sst.to_csv('sst.csv')\n",
    "df_sst=pd.read_csv('sst.csv')\n",
    "df_sst[['lat','lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['latitude','longitude']]\n",
    "print(df_station_point)\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<38) & (df_station_point['longitude']>130.0)]\n",
    "\n",
    "print(df_station_point_38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "WIDTH=10\n",
    "HEIGHT=10\n",
    "\n",
    "plt.figure(figsize=(WIDTH, HEIGHT))\n",
    "map = Basemap(projection='merc', lat_0 = 37, lon_0 = 135,\n",
    "    resolution = 'h', area_thresh = 0.1,\n",
    "    llcrnrlon=120.25, llcrnrlat=30.0,\n",
    "    urcrnrlon=140.25, urcrnrlat=42.75)\n",
    "lat=station_point[['latitude']].values.flatten()\n",
    "lon=station_point[['longitude']].values.flatten()\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<37.6) & (df_station_point['longitude']>130.0) & (df_station_point['longitude']<131.2)]\n",
    "\n",
    "print(df_station_point_38['Point'])\n",
    "\n",
    "lat_38=df_station_point_38[['latitude']].values.flatten()\n",
    "lon_38=df_station_point_38[['longitude']].values.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'gray')\n",
    "map.drawmapboundary(fill_color='aqua')\n",
    "map.drawcoastlines()\n",
    "#map.drawmeridians(np.arange(0, 360, 30))\n",
    "#map.drawparallels(np.arange(-90, 90, 30))\n",
    "\n",
    "parallels = np.arange(30.,42.1,2.)\n",
    "# labels = [left,right,top,bottom]\n",
    "map.drawparallels(parallels,labels=[True,False,True,True])\n",
    "meridians = np.arange(99.,150.,2.)\n",
    "map.drawmeridians(meridians,labels=[True,False,False,True])\n",
    "\n",
    "x,y = map(lon_38, lat_38)\n",
    "map.plot(x, y, 'ro', markersize=3)\n",
    "#map.bluemarble()\n",
    "map.etopo()\n",
    "#map.drawparallels(np.arange(10,70,20),labels=[1,1,0,0])\n",
    "#map.drawmeridians(np.arange(-100,0,20),labels=[0,0,0,1])\n",
    "#plt.title('Atlantic Hurricane Tracks (Storms Reaching Category 4, 1851-2004)')\n",
    "title_font = {\n",
    "    'fontsize': 16,\n",
    "    'fontweight': 'bold'\n",
    "}\n",
    "plt.title('Graph Title', fontdict=title_font, loc='center', pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape_ocean_df=pd.read_csv('/gpu_deep/Deep_Ocean/reshape_ocean_df_full.csv')\n",
    "reshape_ocean_df_no_ssh=pd.read_csv('/gpu_deep/Deep_Ocean/reshape_ocean_df_no_ssh.csv')\n",
    "reshape_ocean_df=pd.read_csv('/gpu_deep/Deep_Ocean/reshape_ocean_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_no_ssh=reshape_ocean_df_no_ssh[reshape_ocean_df_no_ssh[\"DATE\"]>'1965-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-silicon",
   "metadata": {},
   "source": [
    "## Preparing Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_no_ssh_test=reshape_ocean_df_1965_no_ssh[(reshape_ocean_df_1965_no_ssh[\"DATE\"]>'2012-01-01')&(reshape_ocean_df_1965_no_ssh[\"DATE\"]<'2018-01-01')]\n",
    "reshape_ocean_df_1965_test=reshape_ocean_df_1965_no_ssh_test[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_1965_FEB=reshape_ocean_df_1965_no_ssh_test[reshape_ocean_df_1965_no_ssh_test['DATE'].str.contains(\"-02-\")]\n",
    "reshape_ocean_df_test_1965_APR=reshape_ocean_df_1965_no_ssh_test[reshape_ocean_df_1965_no_ssh_test['DATE'].str.contains(\"-04-\")]\n",
    "reshape_ocean_df_test_1965_JUN=reshape_ocean_df_1965_no_ssh_test[reshape_ocean_df_1965_no_ssh_test['DATE'].str.contains(\"-06-\")]\n",
    "reshape_ocean_df_test_1965_AUG=reshape_ocean_df_1965_no_ssh_test[reshape_ocean_df_1965_no_ssh_test['DATE'].str.contains(\"-08-\")]\n",
    "reshape_ocean_df_test_1965_OCT=reshape_ocean_df_1965_no_ssh_test[reshape_ocean_df_1965_no_ssh_test['DATE'].str.contains(\"-10-\")]\n",
    "reshape_ocean_df_test_1965_DEC=reshape_ocean_df_1965_no_ssh_test[reshape_ocean_df_1965_no_ssh_test['DATE'].str.contains(\"-12-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_1965_DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_no_ssh=reshape_ocean_df_no_ssh[(reshape_ocean_df_no_ssh[\"DATE\"]>'1965-01-01')&(reshape_ocean_df_no_ssh[\"DATE\"]<'2012-01-01')]\n",
    "reshape_ocean_df_1965=reshape_ocean_df_1965_no_ssh[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-undergraduate",
   "metadata": {},
   "source": [
    "## Seasonal Dataset 1965_dataaset (Spring, Summer, Autumn, Winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_FEB=reshape_ocean_df_1965_no_ssh[reshape_ocean_df_1965_no_ssh['DATE'].str.contains(\"-02-\")]\n",
    "reshape_ocean_df_1965_APR=reshape_ocean_df_1965_no_ssh[reshape_ocean_df_1965_no_ssh['DATE'].str.contains(\"-04-\")]\n",
    "reshape_ocean_df_1965_JUN=reshape_ocean_df_1965_no_ssh[reshape_ocean_df_1965_no_ssh['DATE'].str.contains(\"-06-\")]\n",
    "reshape_ocean_df_1965_AUG=reshape_ocean_df_1965_no_ssh[reshape_ocean_df_1965_no_ssh['DATE'].str.contains(\"-08-\")]\n",
    "reshape_ocean_df_1965_OCT=reshape_ocean_df_1965_no_ssh[reshape_ocean_df_1965_no_ssh['DATE'].str.contains(\"-10-\")]\n",
    "reshape_ocean_df_1965_DEC=reshape_ocean_df_1965_no_ssh[reshape_ocean_df_1965_no_ssh['DATE'].str.contains(\"-12-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-roller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_FEB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-grammar",
   "metadata": {},
   "source": [
    "## Seasonal Dataset  (Spring, Summer, Autumn, Winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_ssh=reshape_ocean_df\n",
    "reshape_ocean_df_ssh=reshape_ocean_df[(reshape_ocean_df[\"DATE\"]>'1993-01-01')&(reshape_ocean_df[\"DATE\"]<'2013-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_ssh=reshape_ocean_df_ssh.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-florida",
   "metadata": {},
   "source": [
    "## Preparing Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_ssh=reshape_ocean_df[(reshape_ocean_df[\"DATE\"]>'2013-01-01')&(reshape_ocean_df[\"DATE\"]<'2018-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_FEB=reshape_ocean_df_test_ssh[reshape_ocean_df_test_ssh['DATE'].str.contains(\"-02-\")]\n",
    "reshape_ocean_df_test_APR=reshape_ocean_df_test_ssh[reshape_ocean_df_test_ssh['DATE'].str.contains(\"-04-\")]\n",
    "reshape_ocean_df_test_JUN=reshape_ocean_df_test_ssh[reshape_ocean_df_test_ssh['DATE'].str.contains(\"-06-\")]\n",
    "reshape_ocean_df_test_AUG=reshape_ocean_df_test_ssh[reshape_ocean_df_test_ssh['DATE'].str.contains(\"-08-\")]\n",
    "reshape_ocean_df_test_OCT=reshape_ocean_df_test_ssh[reshape_ocean_df_test_ssh['DATE'].str.contains(\"-10-\")]\n",
    "reshape_ocean_df_test_DEC=reshape_ocean_df_test_ssh[reshape_ocean_df_test_ssh['DATE'].str.contains(\"-12-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_FEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-explorer",
   "metadata": {},
   "source": [
    "## Preparing Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_FEB=reshape_ocean_df_ssh[reshape_ocean_df_ssh['DATE'].str.contains(\"-02-\")]\n",
    "reshape_ocean_df_APR=reshape_ocean_df_ssh[reshape_ocean_df_ssh['DATE'].str.contains(\"-04-\")]\n",
    "reshape_ocean_df_JUN=reshape_ocean_df_ssh[reshape_ocean_df_ssh['DATE'].str.contains(\"-06-\")]\n",
    "reshape_ocean_df_AUG=reshape_ocean_df_ssh[reshape_ocean_df_ssh['DATE'].str.contains(\"-08-\")]\n",
    "reshape_ocean_df_OCT=reshape_ocean_df_ssh[reshape_ocean_df_ssh['DATE'].str.contains(\"-10-\")]\n",
    "reshape_ocean_df_DEC=reshape_ocean_df_ssh[reshape_ocean_df_ssh['DATE'].str.contains(\"-12-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_FEB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-pressure",
   "metadata": {},
   "source": [
    "## Compare distribution of 1965_FEB vs 1993_FEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "for depth in ['10','20','30','50','75','100','125','150','200','250']:\n",
    "    #reshape_ocean_df_1965_FEB[[depth]].hist(bins=50,grid=False, xlabelsize=12, ylabelsize=12)\n",
    "    sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=False)\n",
    "    sns.distplot(reshape_ocean_df_FEB[depth],bins=30,kde=False)\n",
    "    plt.xlabel(\"Temperature (C)\", fontsize=15)\n",
    "    plt.ylabel(\"Frequency\",fontsize=15)\n",
    "    plt.title('Orignial Dataset Depth:'+depth+'m')\n",
    "    plt.show()\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth], kde=False, color='red', bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.constraints import GreaterThan\n",
    "depth_temp_0_10_const = GreaterThan(\n",
    "low='10',\n",
    "high='0',\n",
    "handling_strategy='reject_sampling')\n",
    "#0\t10\t20\t30\t50\t75\t100\t125\t150\t200\t250\t300\t400\t500\n",
    "depth_temp_20_10_const = GreaterThan(\n",
    "low='20',\n",
    "high='10',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_30_20_const = GreaterThan(\n",
    "low='30',\n",
    "high='20',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_50_30_const = GreaterThan(\n",
    "low='50',\n",
    "high='30',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_75_50_const = GreaterThan(\n",
    "low='75',\n",
    "high='50',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_100_75_const = GreaterThan(\n",
    "low='100',\n",
    "high='75',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_125_100_const = GreaterThan(\n",
    "low='125',\n",
    "high='100',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_150_125_const = GreaterThan(\n",
    "low='150',\n",
    "high='125',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_200_150_const = GreaterThan(\n",
    "low='200',\n",
    "high='150',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_250_200_const = GreaterThan(\n",
    "low='250',\n",
    "high='200',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_300_250_const = GreaterThan(\n",
    "low='300',\n",
    "high='250',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_400_300_const = GreaterThan(\n",
    "low='400',\n",
    "high='300',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_500_400_const = GreaterThan(\n",
    "low='500',\n",
    "high='400',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "constraints_upper = [depth_temp_0_10_const,\n",
    "                     depth_temp_20_10_const,\n",
    "                     depth_temp_30_20_const,\n",
    "                     depth_temp_50_30_const,\n",
    "                     depth_temp_75_50_const,\n",
    "                     depth_temp_100_75_const,\n",
    "                     depth_temp_125_100_const,\n",
    "                     depth_temp_150_125_const,\n",
    "                     depth_temp_200_150_const,\n",
    "                     depth_temp_250_200_const,\n",
    "                     depth_temp_300_250_const,\n",
    "                     depth_temp_400_300_const,\n",
    "                     depth_temp_500_400_const]\n",
    "\n",
    "constraints_upper2 = [depth_temp_0_10_const,\n",
    "                     depth_temp_20_10_const,\n",
    "                     depth_temp_30_20_const,\n",
    "                     depth_temp_50_30_const,\n",
    "                     depth_temp_75_50_const,\n",
    "                     depth_temp_100_75_const,\n",
    "                     depth_temp_125_100_const,\n",
    "                     depth_temp_150_125_const,\n",
    "                     depth_temp_200_150_const,\n",
    "                     depth_temp_250_200_const,\n",
    "                     depth_temp_300_250_const]\n",
    "\n",
    "\n",
    "\n",
    "depth_temp_20_0_const = GreaterThan(\n",
    "low='20',\n",
    "high='0',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_50_20_const = GreaterThan(\n",
    "low='50',\n",
    "high='0',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_100_50_const = GreaterThan(\n",
    "low='100',\n",
    "high='50',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_150_100_const = GreaterThan(\n",
    "low='150',\n",
    "high='100',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_200_150_const = GreaterThan(\n",
    "low='200',\n",
    "high='150',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "constraints_upper3 = [depth_temp_20_0_const,\n",
    "                      depth_temp_50_20_const,\n",
    "                      depth_temp_100_50_const]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import TVAE\n",
    "#model_upper = CopulaGAN(constraints=constraints_upper)\n",
    "#model=CopulaGAN()\n",
    "#model_upper.fit(ocean_train_upper)\n",
    "#model.fit(reshape_ocean_df_1965)\n",
    "#model_upper.sample(100, max_retries=100000, conditions=conditions)\n",
    "#model_output=model.sample(1000, max_retries=1000000)\n",
    "#reshape_ocean_df_1965.hist()\n",
    "#model_output.hist()\n",
    "\n",
    "#model=CopulaGAN(constraints=constraints_upper3)\n",
    "\n",
    "model=TVAE()\n",
    "\n",
    "for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "    model.fit(globals()['reshape_ocean_df_1965_'+mon])\n",
    "    model_output=model.sample(2000, max_retries=1000000)\n",
    "    model_output['10']=np.where((model_output['10']>model_output['0']),model_output['0'],model_output['10'])\n",
    "    model_output['20']=np.where((model_output['20']>model_output['10']),model_output['10'],model_output['20'])\n",
    "    model_output['30']=np.where((model_output['30']>model_output['20']),model_output['20'],model_output['30'])\n",
    "    model_output['50']=np.where((model_output['50']>model_output['30']),model_output['30'],model_output['50'])\n",
    "    model_output['75']=np.where((model_output['75']>model_output['50']),model_output['50'],model_output['75'])\n",
    "    model_output['100']=np.where((model_output['100']>model_output['75']),model_output['75'],model_output['100'])\n",
    "    model_output['125']=np.where((model_output['125']>model_output['100']),model_output['100'],model_output['125'])\n",
    "    model_output['150']=np.where((model_output['150']>model_output['125']),model_output['125'],model_output['150'])\n",
    "    model_output['200']=np.where((model_output['200']>model_output['150']),model_output['150'],model_output['200'])\n",
    "    model_output['250']=np.where((model_output['250']>model_output['200']),model_output['200'],model_output['250'])  \n",
    "    globals()['model_output_1965_'+mon]=model_output\n",
    "    \n",
    "#model=CopulaGAN(constraints=constraints_upper3)\n",
    "model=TVAE()\n",
    "for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "    model.fit(globals()['reshape_ocean_df_'+mon])\n",
    "    model_output=model.sample(2000, max_retries=1000000)\n",
    "    model_output['10']=np.where((model_output['10']>model_output['0']),model_output['0'],model_output['10'])\n",
    "    model_output['20']=np.where((model_output['20']>model_output['10']),model_output['10'],model_output['20'])\n",
    "    model_output['30']=np.where((model_output['30']>model_output['20']),model_output['20'],model_output['30'])\n",
    "    model_output['50']=np.where((model_output['50']>model_output['30']),model_output['30'],model_output['50'])\n",
    "    model_output['75']=np.where((model_output['75']>model_output['50']),model_output['50'],model_output['75'])\n",
    "    model_output['100']=np.where((model_output['100']>model_output['75']),model_output['75'],model_output['100'])\n",
    "    model_output['125']=np.where((model_output['125']>model_output['100']),model_output['100'],model_output['125'])\n",
    "    model_output['150']=np.where((model_output['150']>model_output['125']),model_output['125'],model_output['150'])\n",
    "    model_output['200']=np.where((model_output['200']>model_output['150']),model_output['150'],model_output['200'])\n",
    "    model_output['250']=np.where((model_output['250']>model_output['200']),model_output['200'],model_output['250'])  \n",
    "    globals()['model_output_df_'+mon]=model_output\n",
    "    globals()['temp_tvae_model_output_df_'+mon]=model_output\n",
    "\n",
    "    \n",
    "model=GaussianCopula(constraints=constraints_upper3)\n",
    "for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "    model.fit(globals()['reshape_ocean_df_'+mon])\n",
    "    model_output=model.sample(2000, max_retries=1000000)\n",
    "    model_output['10']=np.where((model_output['10']>model_output['0']),model_output['0'],model_output['10'])\n",
    "    model_output['20']=np.where((model_output['20']>model_output['10']),model_output['10'],model_output['20'])\n",
    "    model_output['30']=np.where((model_output['30']>model_output['20']),model_output['20'],model_output['30'])\n",
    "    model_output['50']=np.where((model_output['50']>model_output['30']),model_output['30'],model_output['50'])\n",
    "    model_output['75']=np.where((model_output['75']>model_output['50']),model_output['50'],model_output['75'])\n",
    "    model_output['100']=np.where((model_output['100']>model_output['75']),model_output['75'],model_output['100'])\n",
    "    model_output['125']=np.where((model_output['125']>model_output['100']),model_output['100'],model_output['125'])\n",
    "    model_output['150']=np.where((model_output['150']>model_output['125']),model_output['125'],model_output['150'])\n",
    "    model_output['200']=np.where((model_output['200']>model_output['150']),model_output['150'],model_output['200'])\n",
    "    model_output['250']=np.where((model_output['250']>model_output['200']),model_output['200'],model_output['250'])\n",
    "    globals()['model_output_df_'+mon]=pd.concat([model_output,pd.DataFrame(globals()['temp_tvae_model_output_df_'+mon])])\n",
    "    #globals()['model_output_df_'+mon]=model_output    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in ['104-08','104-09','104-10','105-08','105-09','105-10']:\n",
    "    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "        new_station=station.replace('-','_')\n",
    "        station_select=str('reshape_ocean_df_1965_'+mon)\n",
    "        #print(station_select)\n",
    "        #globals()['reshape_ocean_df_1965_'+new_station+'_'+mon]=reshape_ocean_df_1965_FEB[reshape_ocean_df_1965_FEB['Station-Point']==station]\n",
    "        globals()['reshape_ocean_df_1965_'+new_station+'_'+mon]=globals()['reshape_ocean_df_1965_'+mon][globals()['reshape_ocean_df_1965_'+mon]['Station-Point']==station]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in ['104-08','104-09','104-10','105-08','105-09','105-10']:\n",
    "    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "        new_station=station.replace('-','_')\n",
    "        station_select=str('reshape_ocean_df_'+mon)\n",
    "        #print(station_select)\n",
    "        #globals()['reshape_ocean_df_1965_'+new_station+'_'+mon]=reshape_ocean_df_1965_FEB[reshape_ocean_df_1965_FEB['Station-Point']==station]\n",
    "        globals()['reshape_ocean_df_'+new_station+'_'+mon]=globals()['reshape_ocean_df_'+mon][globals()['reshape_ocean_df_'+mon]['Station-Point']==station]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-active",
   "metadata": {},
   "source": [
    "## Station-Point (ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for station in ['104-07','104-08','104-09','104-10','104-11','105-07','105-08','105-09','105-10','105-11','106-07','106-08','106-09','106-10','106-11']:\n",
    "#    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "#        new_station=station.replace('-','_')\n",
    "#        station_select=str('reshape_ocean_df_1965_'+mon)\n",
    "#        #print(station_select)\n",
    "#        #globals()['reshape_ocean_df_1965_'+new_station+'_'+mon]=reshape_ocean_df_1965_FEB[reshape_ocean_df_1965_FEB['Station-Point']==station]\n",
    "#        globals()['reshape_ocean_df_1965_'+new_station+'_'+mon]=globals()['reshape_ocean_df_1965_'+mon][globals()['reshape_ocean_df_1965_'+mon]['Station-Point']==station]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for station in ['104-07','104-08','104-09','104-10','104-11','105-07','105-08','105-09','105-10','105-11','106-07','106-08','106-09','106-10','106-11']:\n",
    "#    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "#        new_station=station.replace('-','_')\n",
    "#        station_select=str('reshape_ocean_df_'+mon)\n",
    "#        #print(station_select)\n",
    "#        #globals()['reshape_ocean_df_1965_'+new_station+'_'+mon]=reshape_ocean_df_1965_FEB[reshape_ocean_df_1965_FEB['Station-Point']==station]\n",
    "#        globals()['reshape_ocean_df_'+new_station+'_'+mon]=globals()['reshape_ocean_df_'+mon][globals()['reshape_ocean_df_'+mon]['Station-Point']==station]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_105_08_APR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-estimate",
   "metadata": {},
   "source": [
    "## Station-Point Synthetic (104-105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-contract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import TVAE\n",
    "#model_upper = CopulaGAN(constraints=constraints_upper)\n",
    "#model=CopulaGAN()\n",
    "#model_upper.fit(ocean_train_upper)\n",
    "#model.fit(reshape_ocean_df_1965)\n",
    "#model_upper.sample(100, max_retries=100000, conditions=conditions)\n",
    "#model_output=model.sample(1000, max_retries=1000000)\n",
    "#reshape_ocean_df_1965.hist()\n",
    "#model_output.hist()\n",
    "\n",
    "#model=CopulaGAN(constraints=constraints_upper3)\n",
    "\n",
    "model=TVAE()\n",
    "for station in ['104-08','104-09','104-10','105-08','105-09','105-10']:\n",
    "    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "        new_station=station.replace('-','_')\n",
    "        model.fit(globals()['reshape_ocean_df_1965_'+new_station+'_'+mon])\n",
    "        model_output=model.sample(2000, max_retries=1000000)\n",
    "        model_output['10']=np.where((model_output['10']>model_output['0']),model_output['0'],model_output['10'])\n",
    "        model_output['20']=np.where((model_output['20']>model_output['10']),model_output['10'],model_output['20'])\n",
    "        model_output['30']=np.where((model_output['30']>model_output['20']),model_output['20'],model_output['30'])\n",
    "        model_output['50']=np.where((model_output['50']>model_output['30']),model_output['30'],model_output['50'])\n",
    "        model_output['75']=np.where((model_output['75']>model_output['50']),model_output['50'],model_output['75'])\n",
    "        model_output['100']=np.where((model_output['100']>model_output['75']),model_output['75'],model_output['100'])\n",
    "        model_output['125']=np.where((model_output['125']>model_output['100']),model_output['100'],model_output['125'])\n",
    "        model_output['150']=np.where((model_output['150']>model_output['125']),model_output['125'],model_output['150'])\n",
    "        model_output['200']=np.where((model_output['200']>model_output['150']),model_output['150'],model_output['200'])\n",
    "        model_output['250']=np.where((model_output['250']>model_output['200']),model_output['200'],model_output['250'])  \n",
    "        globals()['model_output_df_1965_'+new_station+'_'+mon]=model_output\n",
    "    \n",
    "#model=CopulaGAN(constraints=constraints_upper3)\n",
    "model=TVAE()\n",
    "for station in ['104-08','104-09','104-10','105-08','105-09','105-10']:\n",
    "    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "        new_station=station.replace('-','_')\n",
    "        model.fit(globals()['reshape_ocean_df_'+new_station+'_'+mon])\n",
    "        model_output=model.sample(2000, max_retries=1000000)\n",
    "        model_output['10']=np.where((model_output['10']>model_output['0']),model_output['0'],model_output['10'])\n",
    "        model_output['20']=np.where((model_output['20']>model_output['10']),model_output['10'],model_output['20'])\n",
    "        model_output['30']=np.where((model_output['30']>model_output['20']),model_output['20'],model_output['30'])\n",
    "        model_output['50']=np.where((model_output['50']>model_output['30']),model_output['30'],model_output['50'])\n",
    "        model_output['75']=np.where((model_output['75']>model_output['50']),model_output['50'],model_output['75'])\n",
    "        model_output['100']=np.where((model_output['100']>model_output['75']),model_output['75'],model_output['100'])\n",
    "        model_output['125']=np.where((model_output['125']>model_output['100']),model_output['100'],model_output['125'])\n",
    "        model_output['150']=np.where((model_output['150']>model_output['125']),model_output['125'],model_output['150'])\n",
    "        model_output['200']=np.where((model_output['200']>model_output['150']),model_output['150'],model_output['200'])\n",
    "        model_output['250']=np.where((model_output['250']>model_output['200']),model_output['200'],model_output['250'])  \n",
    "        globals()['model_output_df_'+new_station+'_'+mon]=model_output\n",
    "        globals()['temp_tvae_model_output_df_'+new_station+'_'+mon]=model_output\n",
    "\n",
    "    \n",
    "model=GaussianCopula()\n",
    "for station in ['104-08','104-09','104-10','105-08','105-09','105-10']:\n",
    "    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "        new_station=station.replace('-','_')\n",
    "        model.fit(globals()['reshape_ocean_df_'+new_station+'_'+mon])\n",
    "        model_output=model.sample(2000, max_retries=1000000)\n",
    "        model_output['10']=np.where((model_output['10']>model_output['0']),model_output['0'],model_output['10'])\n",
    "        model_output['20']=np.where((model_output['20']>model_output['10']),model_output['10'],model_output['20'])\n",
    "        model_output['30']=np.where((model_output['30']>model_output['20']),model_output['20'],model_output['30'])\n",
    "        model_output['50']=np.where((model_output['50']>model_output['30']),model_output['30'],model_output['50'])\n",
    "        model_output['75']=np.where((model_output['75']>model_output['50']),model_output['50'],model_output['75'])\n",
    "        model_output['100']=np.where((model_output['100']>model_output['75']),model_output['75'],model_output['100'])\n",
    "        model_output['125']=np.where((model_output['125']>model_output['100']),model_output['100'],model_output['125'])\n",
    "        model_output['150']=np.where((model_output['150']>model_output['125']),model_output['125'],model_output['150'])\n",
    "        model_output['200']=np.where((model_output['200']>model_output['150']),model_output['150'],model_output['200'])\n",
    "        model_output['250']=np.where((model_output['250']>model_output['200']),model_output['200'],model_output['250'])\n",
    "        globals()['model_output_df_'+new_station+'_'+mon]=pd.concat([model_output,pd.DataFrame(globals()['temp_tvae_model_output_df_'+new_station+'_'+mon])])\n",
    "        #globals()['model_output_df_'+mon]=model_output    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-perth",
   "metadata": {},
   "source": [
    "## Station Point (All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-carry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in ['104_08','104_09','104_10','105_08','105_09','105_10']:\n",
    "    for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "        globals()['model_output_df_1965_'+station+'_'+mon].to_csv(str('/gpu_deep/Deep_Ocean/Synthetic/model_output_df_1965_syn_'+station+'_'+mon+'.csv'))\n",
    "        globals()['model_output_df_'+station+'_'+mon].to_csv(str('/gpu_deep/Deep_Ocean/Synthetic/model_output_df_syn_'+station+'_'+mon+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mon in ['FEB','APR','JUN','AUG','OCT','DEC']:\n",
    "    globals()['model_output_1965_'+mon]=pd.DataFrame()\n",
    "    globals()['model_output_df_'+mon]=pd.DataFrame()\n",
    "    for station in ['104_08','104_09','104_10','105_08','105_09','105_10']:\n",
    "        temp_model_1965=pd.read_csv(str('/gpu_deep/Deep_Ocean/Synthetic/model_output_df_1965_syn_'+station+'_'+mon+'.csv'))\n",
    "        globals()['model_output_1965_'+mon]=pd.concat([globals()['model_output_1965_'+mon],temp_model_1965])\n",
    "        temp_model=pd.read_csv(str('/gpu_deep/Deep_Ocean/Synthetic/model_output_df_syn_'+station+'_'+mon+'.csv'))\n",
    "        globals()['model_output_df_'+mon]=pd.concat([globals()['model_output_df_'+mon],temp_model])\n",
    "        #temp_model=pd.read_csv(str('/gpu_deep/Deep_Ocean/Synthetic/model_output_df_syn_'+station+'_'+mon+'.csv'))\n",
    "        #reshape_ocean_df=pd.concat([reshape_ocean_df,temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-greensboro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_1965_FEB=model_output_1965_FEB[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_1965_APR=model_output_1965_APR[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_1965_JUN=model_output_1965_JUN[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_1965_AUG=model_output_1965_AUG[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_1965_OCT=model_output_1965_OCT[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_1965_DEC=model_output_1965_DEC[['Station-Point','DATE','0','10','20','30','50','75','100','125','150','200','250','300']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_df_FEB=model_output_df_FEB[['Station-Point','DATE','SSH','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_df_APR=model_output_df_APR[['Station-Point','DATE','SSH','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_df_JUN=model_output_df_JUN[['Station-Point','DATE','SSH','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_df_AUG=model_output_df_AUG[['Station-Point','DATE','SSH','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_df_OCT=model_output_df_OCT[['Station-Point','DATE','SSH','0','10','20','30','50','75','100','125','150','200','250','300']]\n",
    "model_output_df_DEC=model_output_df_DEC[['Station-Point','DATE','SSH','0','10','20','30','50','75','100','125','150','200','250','300']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_FEB=reshape_ocean_df_1965_FEB[(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_1965_APR=reshape_ocean_df_1965_APR[(reshape_ocean_df_1965_APR[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_APR[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_APR[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_APR[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_APR[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_APR[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_1965_JUN=reshape_ocean_df_1965_JUN[(reshape_ocean_df_1965_JUN[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_JUN[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_JUN[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_JUN[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_JUN[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_JUN[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_1965_AUG=reshape_ocean_df_1965_AUG[(reshape_ocean_df_1965_AUG[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_AUG[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_AUG[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_AUG[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_AUG[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_AUG[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_1965_OCT=reshape_ocean_df_1965_OCT[(reshape_ocean_df_1965_OCT[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_OCT[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_OCT[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_OCT[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_OCT[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_OCT[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_1965_DEC=reshape_ocean_df_1965_DEC[(reshape_ocean_df_1965_DEC[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_DEC[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_DEC[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_DEC[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_DEC[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_DEC[\"Station-Point\"]=='105-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_FEB=reshape_ocean_df_FEB[(reshape_ocean_df_FEB[\"Station-Point\"]=='104-08')|(reshape_ocean_df_FEB[\"Station-Point\"]=='104-09')|(reshape_ocean_df_FEB[\"Station-Point\"]=='104-10')|(reshape_ocean_df_FEB[\"Station-Point\"]=='105-08')|(reshape_ocean_df_FEB[\"Station-Point\"]=='105-09')|(reshape_ocean_df_FEB[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_APR=reshape_ocean_df_APR[(reshape_ocean_df_APR[\"Station-Point\"]=='104-08')|(reshape_ocean_df_APR[\"Station-Point\"]=='104-09')|(reshape_ocean_df_APR[\"Station-Point\"]=='104-10')|(reshape_ocean_df_APR[\"Station-Point\"]=='105-08')|(reshape_ocean_df_APR[\"Station-Point\"]=='105-09')|(reshape_ocean_df_APR[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_JUN=reshape_ocean_df_JUN[(reshape_ocean_df_JUN[\"Station-Point\"]=='104-08')|(reshape_ocean_df_JUN[\"Station-Point\"]=='104-09')|(reshape_ocean_df_JUN[\"Station-Point\"]=='104-10')|(reshape_ocean_df_JUN[\"Station-Point\"]=='105-08')|(reshape_ocean_df_JUN[\"Station-Point\"]=='105-09')|(reshape_ocean_df_JUN[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_AUG=reshape_ocean_df_AUG[(reshape_ocean_df_AUG[\"Station-Point\"]=='104-08')|(reshape_ocean_df_AUG[\"Station-Point\"]=='104-09')|(reshape_ocean_df_AUG[\"Station-Point\"]=='104-10')|(reshape_ocean_df_AUG[\"Station-Point\"]=='105-08')|(reshape_ocean_df_AUG[\"Station-Point\"]=='105-09')|(reshape_ocean_df_AUG[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_OCT=reshape_ocean_df_OCT[(reshape_ocean_df_OCT[\"Station-Point\"]=='104-08')|(reshape_ocean_df_OCT[\"Station-Point\"]=='104-09')|(reshape_ocean_df_OCT[\"Station-Point\"]=='104-10')|(reshape_ocean_df_OCT[\"Station-Point\"]=='105-08')|(reshape_ocean_df_OCT[\"Station-Point\"]=='105-09')|(reshape_ocean_df_OCT[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_DEC=reshape_ocean_df_DEC[(reshape_ocean_df_DEC[\"Station-Point\"]=='104-08')|(reshape_ocean_df_DEC[\"Station-Point\"]=='104-09')|(reshape_ocean_df_DEC[\"Station-Point\"]=='104-10')|(reshape_ocean_df_DEC[\"Station-Point\"]=='105-08')|(reshape_ocean_df_DEC[\"Station-Point\"]=='105-09')|(reshape_ocean_df_DEC[\"Station-Point\"]=='105-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "index=0\n",
    "f,axes=plt.subplots(10,2, figsize=(8,40))\n",
    "plt.subplots_adjust(hspace = 0.3,wspace=0.5)\n",
    "for depth in ['10','20','30','50','75','100','125','150','200','250']:\n",
    "    #reshape_ocean_df_1965_FEB[[depth]].hist(bins=50,grid=False, xlabelsize=12, ylabelsize=12)\n",
    "    #plt.xlabel(\"Temperature (C)\", fontsize=15)\n",
    "    #plt.ylabel(\"Frequency\",fontsize=15)\n",
    "    #plt.title('Orignial Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=True,ax=axes[index,0],axlabel=depth+'m',color='b')\n",
    "    #plt.title('Model Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(model_output_1965_FEB[depth],bins=30,kde=True, ax=axes[index, 1],color='r',axlabel=depth+'m')\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=False,ax=axes[index,0])\n",
    "    #sns.distplot(model_output_df_FEB[depth],bins=30,kde=False, ax=axes[index,1])\n",
    "    #plt.show()\n",
    "    index=index+1;\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth], kde=False, color='red', bins=30)\n",
    "    #create seaborn barplot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "index=0\n",
    "f,axes=plt.subplots(10,2, figsize=(8,40))\n",
    "plt.subplots_adjust(hspace = 0.3,wspace=0.5)\n",
    "for depth in ['10','20','30','50','75','100','125','150','200','250']:\n",
    "    #reshape_ocean_df_1965_FEB[[depth]].hist(bins=50,grid=False, xlabelsize=12, ylabelsize=12)\n",
    "    #plt.xlabel(\"Temperature (C)\", fontsize=15)\n",
    "    #plt.ylabel(\"Frequency\",fontsize=15)\n",
    "    #plt.title('Orignial Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(reshape_ocean_df_FEB[depth],bins=30,kde=True,ax=axes[index,0],axlabel=depth+'m',color='blue')\n",
    "    #plt.title('Model Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(model_output_df_FEB[depth],bins=30,kde=True, ax=axes[index, 1],color='red',axlabel=depth+'m')\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=False,ax=axes[index,0])\n",
    "    #sns.distplot(model_output_df_FEB[depth],bins=30,kde=False, ax=axes[index,1])\n",
    "    #plt.show()\n",
    "    index=index+1;\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth], kde=False, color='red', bins=30)\n",
    "    #create seaborn barplot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "index=0\n",
    "f,axes=plt.subplots(10,2, figsize=(8,40))\n",
    "plt.subplots_adjust(hspace = 0.3,wspace=0.5)\n",
    "for depth in ['10','20','30','50','75','100','125','150','200','250']:\n",
    "    #reshape_ocean_df_1965_FEB[[depth]].hist(bins=50,grid=False, xlabelsize=12, ylabelsize=12)\n",
    "    #plt.xlabel(\"Temperature (C)\", fontsize=15)\n",
    "    #plt.ylabel(\"Frequency\",fontsize=15)\n",
    "    #plt.title('Orignial Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(reshape_ocean_df_1965_AUG[depth],bins=30,kde=True,ax=axes[index,0],axlabel=depth+'m',color='b')\n",
    "    #plt.title('Model Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(model_output_1965_AUG[depth],bins=30,kde=True, ax=axes[index, 1],color='r',axlabel=depth+'m')\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=False,ax=axes[index,0])\n",
    "    #sns.distplot(model_output_df_FEB[depth],bins=30,kde=False, ax=axes[index,1])\n",
    "    #plt.show()\n",
    "    index=index+1;\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth], kde=False, color='red', bins=30)\n",
    "    #create seaborn barplot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "index=0\n",
    "f,axes=plt.subplots(10,2, figsize=(8,40))\n",
    "plt.subplots_adjust(hspace = 0.3,wspace=0.5)\n",
    "for depth in ['10','20','30','50','75','100','125','150','200','250']:\n",
    "    #reshape_ocean_df_1965_FEB[[depth]].hist(bins=50,grid=False, xlabelsize=12, ylabelsize=12)\n",
    "    #plt.xlabel(\"Temperature (C)\", fontsize=15)\n",
    "    #plt.ylabel(\"Frequency\",fontsize=15)\n",
    "    #plt.title('Orignial Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(reshape_ocean_df_AUG[depth],bins=30,kde=True,ax=axes[index,0],axlabel=depth+'m',color='b')\n",
    "    #plt.title('Model Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(model_output_df_AUG[depth],bins=30,kde=True, ax=axes[index, 1],color='r',axlabel=depth+'m')\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=False,ax=axes[index,0])\n",
    "    #sns.distplot(model_output_df_FEB[depth],bins=30,kde=False, ax=axes[index,1])\n",
    "    #plt.show()\n",
    "    index=index+1;\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth], kde=False, color='red', bins=30)\n",
    "    #create seaborn barplot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "index=0\n",
    "f,axes=plt.subplots(10,2, figsize=(8,40))\n",
    "plt.subplots_adjust(hspace = 0.3,wspace=0.5)\n",
    "for depth in ['10','20','30','50','75','100','125','150','200','250']:\n",
    "    #reshape_ocean_df_1965_FEB[[depth]].hist(bins=50,grid=False, xlabelsize=12, ylabelsize=12)\n",
    "    #plt.xlabel(\"Temperature (C)\", fontsize=15)\n",
    "    #plt.ylabel(\"Frequency\",fontsize=15)\n",
    "    #plt.title('Orignial Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(reshape_ocean_df_1965_AUG[depth],bins=30,kde=False,ax=axes[index,0],axlabel=depth+'m',color='b')\n",
    "    #plt.title('Model Dataset Depth:'+depth+'m')\n",
    "    ax=sns.distplot(model_output_1965_AUG[depth],bins=30,kde=False, ax=axes[index, 1],color='r',axlabel=depth+'m')\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth],bins=30,kde=False,ax=axes[index,0])\n",
    "    #sns.distplot(model_output_df_FEB[depth],bins=30,kde=False, ax=axes[index,1])\n",
    "    #plt.show()\n",
    "    index=index+1;\n",
    "    #sns.distplot(reshape_ocean_df_1965_FEB[depth], kde=False, color='red', bins=30)\n",
    "    #create seaborn barplot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_FEB=reshape_ocean_df_1965_FEB[(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='104-08')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='104-09')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='104-10')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='105-08')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='105-09')|(reshape_ocean_df_1965_FEB[\"Station-Point\"]=='105-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_FEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-carry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1965_FEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import load_data, TableEvaluator\n",
    "#real_data=data_deep\n",
    "real_data=reshape_ocean_df_FEB[['10','20','30','50','75','100','125','150','200','250']]\n",
    "synthetic_data=model_output_df_FEB[['10','20','30','50','75','100','125','150','200','250']]\n",
    "table_evaluator = TableEvaluator(real_data, synthetic_data)\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import load_data, TableEvaluator\n",
    "#real_data=data_deep\n",
    "real_data=reshape_ocean_df_1965_FEB[['10','20','30','50','75','100','125','150','200','250']]\n",
    "synthetic_data=model_output_1965_FEB[['10','20','30','50','75','100','125','150','200','250']]\n",
    "table_evaluator = TableEvaluator(real_data, synthetic_data)\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import load_data, TableEvaluator\n",
    "#real_data=data_deep\n",
    "real_data=reshape_ocean_df_1965_AUG[['10','20','30','50','75','100','125','150','200','250']]\n",
    "synthetic_data=model_output_1965_AUG[['10','20','30','50','75','100','125','150','200','250']]\n",
    "table_evaluator = TableEvaluator(real_data, synthetic_data)\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import load_data, TableEvaluator\n",
    "#real_data=data_deep\n",
    "real_data=reshape_ocean_df_AUG[['10','20','30','50','75','100','125','150','200','250']]\n",
    "synthetic_data=model_output_df_AUG[['10','20','30','50','75','100','125','150','200','250']]\n",
    "table_evaluator = TableEvaluator(real_data, synthetic_data)\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation import evaluate\n",
    "evaluate(synthetic_data, real_data)\n",
    "evaluate(synthetic_data, real_data, aggregate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metrics.tabular import CSTest, KSTest\n",
    "CSTest.compute(real_data, synthetic_data)\n",
    "evaluate(synthetic_data, real_data, metrics=['CSTest', 'KSTest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_FEB=reshape_ocean_df_test_FEB[(reshape_ocean_df_test_FEB[\"Station-Point\"]=='104-08')|(reshape_ocean_df_test_FEB[\"Station-Point\"]=='104-09')|(reshape_ocean_df_test_FEB[\"Station-Point\"]=='104-10')|(reshape_ocean_df_test_FEB[\"Station-Point\"]=='105-08')|(reshape_ocean_df_test_FEB[\"Station-Point\"]=='105-09')|(reshape_ocean_df_test_FEB[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_test_APR=reshape_ocean_df_test_APR[(reshape_ocean_df_test_APR[\"Station-Point\"]=='104-08')|(reshape_ocean_df_test_APR[\"Station-Point\"]=='104-09')|(reshape_ocean_df_test_APR[\"Station-Point\"]=='104-10')|(reshape_ocean_df_test_APR[\"Station-Point\"]=='105-08')|(reshape_ocean_df_test_APR[\"Station-Point\"]=='105-09')|(reshape_ocean_df_test_APR[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_test_JUN=reshape_ocean_df_test_JUN[(reshape_ocean_df_test_JUN[\"Station-Point\"]=='104-08')|(reshape_ocean_df_test_JUN[\"Station-Point\"]=='104-09')|(reshape_ocean_df_test_JUN[\"Station-Point\"]=='104-10')|(reshape_ocean_df_test_JUN[\"Station-Point\"]=='105-08')|(reshape_ocean_df_test_JUN[\"Station-Point\"]=='105-09')|(reshape_ocean_df_test_JUN[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_test_AUG=reshape_ocean_df_test_AUG[(reshape_ocean_df_test_AUG[\"Station-Point\"]=='104-08')|(reshape_ocean_df_test_AUG[\"Station-Point\"]=='104-09')|(reshape_ocean_df_test_AUG[\"Station-Point\"]=='104-10')|(reshape_ocean_df_test_AUG[\"Station-Point\"]=='105-08')|(reshape_ocean_df_test_AUG[\"Station-Point\"]=='105-09')|(reshape_ocean_df_test_AUG[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_test_OCT=reshape_ocean_df_test_OCT[(reshape_ocean_df_test_OCT[\"Station-Point\"]=='104-08')|(reshape_ocean_df_test_OCT[\"Station-Point\"]=='104-09')|(reshape_ocean_df_test_OCT[\"Station-Point\"]=='104-10')|(reshape_ocean_df_test_OCT[\"Station-Point\"]=='105-08')|(reshape_ocean_df_test_OCT[\"Station-Point\"]=='105-09')|(reshape_ocean_df_test_OCT[\"Station-Point\"]=='105-10')]\n",
    "reshape_ocean_df_test_DEC=reshape_ocean_df_test_DEC[(reshape_ocean_df_test_DEC[\"Station-Point\"]=='104-08')|(reshape_ocean_df_test_DEC[\"Station-Point\"]=='104-09')|(reshape_ocean_df_test_DEC[\"Station-Point\"]=='104-10')|(reshape_ocean_df_test_DEC[\"Station-Point\"]=='105-08')|(reshape_ocean_df_test_DEC[\"Station-Point\"]=='105-09')|(reshape_ocean_df_test_DEC[\"Station-Point\"]=='105-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_test_AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_syn_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='FEB') : temp=model_output_df_FEB[(model_output_df_FEB['Station-Point']==station)&(model_output_df_FEB['DATE'].str.contains(\"-02-\"))]\n",
    "    if (season=='APR') : temp=model_output_df_APR[(model_output_df_APR['Station-Point']==station)&(model_output_df_APR['DATE'].str.contains(\"-04-\"))]\n",
    "    if (season=='JUN') : temp=model_output_df_JUN[(model_output_df_JUN['Station-Point']==station)&(model_output_df_JUN['DATE'].str.contains(\"-06-\"))]\n",
    "    if (season=='AUG') : temp=model_output_df_AUG[(model_output_df_AUG['Station-Point']==station)&(model_output_df_AUG['DATE'].str.contains(\"-08-\"))]\n",
    "    if (season=='OCT') : temp=model_output_df_OCT[(model_output_df_OCT['Station-Point']==station)&(model_output_df_OCT['DATE'].str.contains(\"-10-\"))]\n",
    "    if (season=='DEC') : temp=model_output_df_DEC[(model_output_df_DEC['Station-Point']==station)&(model_output_df_DEC['DATE'].str.contains(\"-12-\"))]            \n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=temp[['0','SSH']]\n",
    "    y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_org_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='FEB') : temp=reshape_ocean_df_FEB[(reshape_ocean_df_FEB['Station-Point']==station)&(reshape_ocean_df_FEB['DATE'].str.contains(\"-02-\"))]\n",
    "    elif (season=='APR') : temp=reshape_ocean_df_APR[(reshape_ocean_df_APR['Station-Point']==station)&(reshape_ocean_df_APR['DATE'].str.contains(\"-04-\"))]\n",
    "    elif (season=='JUN') : temp=reshape_ocean_df_JUN[(reshape_ocean_df_JUN['Station-Point']==station)&(reshape_ocean_df_JUN['DATE'].str.contains(\"-06-\"))]\n",
    "    elif (season=='AUG') : temp=reshape_ocean_df_AUG[(reshape_ocean_df_AUG['Station-Point']==station)&(reshape_ocean_df_AUG['DATE'].str.contains(\"-08-\"))]\n",
    "    elif (season=='OCT') : temp=reshape_ocean_df_OCT[(reshape_ocean_df_OCT['Station-Point']==station)&(reshape_ocean_df_OCT['DATE'].str.contains(\"-10-\"))]\n",
    "    elif (season=='DEC') : temp=reshape_ocean_df_DEC[(reshape_ocean_df_DEC['Station-Point']==station)&(reshape_ocean_df_DEC['DATE'].str.contains(\"-12-\"))]            \n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=temp[['0','SSH']]\n",
    "    y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_date_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='FEB') : temp=reshape_ocean_df_FEB[(reshape_ocean_df_FEB['Station-Point']==station)&(reshape_ocean_df_FEB['DATE'].str.contains(\"-02-\"))]\n",
    "    elif (season=='APR') : temp=reshape_ocean_df_APR[(reshape_ocean_df_APR['Station-Point']==station)&(reshape_ocean_df_APR['DATE'].str.contains(\"-04-\"))]\n",
    "    elif (season=='JUN') : temp=reshape_ocean_df_JUN[(reshape_ocean_df_JUN['Station-Point']==station)&(reshape_ocean_df_JUN['DATE'].str.contains(\"-06-\"))]\n",
    "    elif (season=='AUG') : temp=reshape_ocean_df_AUG[(reshape_ocean_df_AUG['Station-Point']==station)&(reshape_ocean_df_AUG['DATE'].str.contains(\"-08-\"))]\n",
    "    elif (season=='OCT') : temp=reshape_ocean_df_OCT[(reshape_ocean_df_OCT['Station-Point']==station)&(reshape_ocean_df_OCT['DATE'].str.contains(\"-10-\"))]\n",
    "    elif (season=='DEC') : temp=reshape_ocean_df_DEC[(reshape_ocean_df_DEC['Station-Point']==station)&(reshape_ocean_df_DEC['DATE'].str.contains(\"-12-\"))]            \n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=temp[['Station-Point','DATE']]\n",
    "    #y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_syn_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ensemble to each standalone models for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from matplotlib import pyplot\n",
    "from sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models=dict()\n",
    "    models['knn']=MultiOutputRegressor(KNeighborsRegressor())\n",
    "#    models['cart']=MultiOutputRegressor(DecisionTreeRegressor())\n",
    "    models['svm']=MultiOutputRegressor(LinearSVR())\n",
    "    models['rf']=MultiOutputRegressor(RandomForestRegressor())\n",
    "    models['lr']=MultiOutputRegressor(LinearRegression())\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluation procedure\n",
    "    cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=42)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluation procedure\n",
    "    cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=42)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y,scoring='neg_mean_squared_error',cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_org_dataset(\"104-08\",\"AUG\")\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(abs(scores))\n",
    "    names.append(name)\n",
    "    #summarize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(abs(scores)), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "df = pd.DataFrame (data=names, columns = ['Single Model'])\n",
    "df\n",
    "plt.xticks(range(5), ['KNN', 'CART', 'SVM', 'RF', 'LR'])\n",
    "sns.boxplot(data=results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_syn_dataset(\"104-08\",\"AUG\")\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(abs(scores))\n",
    "    names.append(name)\n",
    "    #summarize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(abs(scores)), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n",
    "\n",
    "df = pd.DataFrame (data=names, columns = ['Single Model'])\n",
    "df\n",
    "plt.xticks(range(5), ['KNN', 'CART', 'SVM', 'RF', 'LR'])\n",
    "sns.boxplot(data=results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_org_dataset(\"104-08\",\"AUG\")\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(abs(scores))\n",
    "    names.append(name)\n",
    "    #summarize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(abs(scores)), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "df = pd.DataFrame (data=names, columns = ['Single Model'])\n",
    "df\n",
    "plt.xticks(range(5), ['KNN', 'CART', 'SVM', 'RF', 'LR'])\n",
    "sns.boxplot(data=results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_syn_dataset(\"104-08\",\"AUG\")\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(abs(scores))\n",
    "    names.append(name)\n",
    "    #summarize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(abs(scores)), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n",
    "\n",
    "df = pd.DataFrame (data=names, columns = ['Single Model'])\n",
    "df\n",
    "plt.xticks([1],['ln', 'cart', 'svm', 'stacking'])\n",
    "sns.boxplot(data=results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='FEB') : temp=reshape_ocean_df_FEB[(reshape_ocean_df_FEB['Station-Point']==station)&(reshape_ocean_df_FEB['DATE'].str.contains(\"-02-\"))]\n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=temp[['0','SSH']]\n",
    "    y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-storage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_syn_dataset(\"104-08\",\"FEB\")\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    #summhttp://147.46.93.143:8888/notebooks/Frontier_Jounral_2022_04_08.ipynb#arize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, abs(mean(scores)), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_org_dataset(\"104-08\",\"FEB\")\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    #summarize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    #define the base models\n",
    "    level0=list()\n",
    "    level0.append(('ln',LinearRegression()))\n",
    "    level0.append(('cart',DecisionTreeRegressor()))\n",
    "    level0.append(('svm', SVR()))\n",
    "    #define meta learner model\n",
    "    level1=MultiOutputRegressor(RandomForestRegressor())\n",
    "    #define the stacking ensemble\n",
    "    model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=10))\n",
    "    #model=StackingRegressor(estimators=level0, final_estimator=level1, cv=10)\n",
    "    model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "    return model\n",
    "\n",
    "#stack_reg.estimator.final_estimator_ = stack_reg.estimator.final_estimator\n",
    "\n",
    "#get a list of models to evaluate\n",
    "def get_models():\n",
    "    models=dict()\n",
    "    models['ln']=MultiOutputRegressor(LinearRegression())\n",
    "    models['cart']=MultiOutputRegressor(DecisionTreeRegressor())\n",
    "    models['svm']= MultiOutputRegressor(SVR())\n",
    "    models['stacking']=get_stacking()\n",
    "    return models\n",
    "\n",
    "\n",
    "#evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluation procedure\n",
    "    cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking():\n",
    "    #define the base models\n",
    "    level0=list()\n",
    "    level0.append(('knn',KNeighborsRegressor()))\n",
    "    level0.append(('cart',DecisionTreeRegressor()))\n",
    "    level0.append(('svm',SVR()))\n",
    "    #define meta learner model\n",
    "    level1=MultiOutputRegressor(LinearSVR())\n",
    "    #define the stacking ensemble\n",
    "    model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "    model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "    #model=StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    #model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "    return model\n",
    "\n",
    "#get a list of models to evaluate\n",
    "def get_models():\n",
    "    models=dict()\n",
    "    models['ln']=MultiOutputRegressor(LinearRegression())\n",
    "    models['cart']=MultiOutputRegressor(DecisionTreeRegressor())\n",
    "    models['svm']= MultiOutputRegressor(SVR())\n",
    "    models['stacking']=get_stacking()\n",
    "    return models\n",
    "\n",
    "#evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluation procedure\n",
    "    cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y = get_syn_dataset(\"104-08\",\"FEB\")\n",
    "#get the model to evaluate\n",
    "models=get_models()\n",
    "#evaluate the models and store results\n",
    "results, names=list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "    #evaluate the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    #summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, abs(mean(scores)), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usual Imports\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import VotingRegressor,GradientBoostingRegressor,HistGradientBoostingRegressor,StackingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "from sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-learn Models\n",
    "lin_reg= MultiOutputRegressor(LinearRegression())\n",
    "rnd_reg =MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=5))\n",
    "svr_reg = MultiOutputRegressor(SVR(kernel = 'rbf'))\n",
    "#Keras Model\n",
    "def build_nn():\n",
    "    model= Sequential(\n",
    "                [Dense(512,activation='sigmoid',input_shape=[5]),\n",
    "                 Dense(256,activation='sigmoid'),\n",
    "                 Dropout(0.2),\n",
    "                 Dense(128,activation='sigmoid'),\n",
    "                 Dense(64,activation='sigmoid'),\n",
    "                 Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['RootMeanSquaredError'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_syn_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from matplotlib import pyplot\n",
    "#Regression Chain Model\n",
    "\n",
    "level0 = list()\n",
    "#level0.append(('lin_reg', RegressorChain(SVR())))\n",
    "level0.append(('knn_reg', RegressorChain(KNeighborsRegressor())))\n",
    "level0.append(('rnd_reg', RegressorChain(RandomForestRegressor(n_estimators=100, random_state=5))))\n",
    "level0.append((('svr_reg',  RegressorChain(SVR(kernel = 'rbf')))))\n",
    "\n",
    "#in_reg=RegressorChain(LinearRegression())\n",
    "#nn_reg=RegressorChain(KNeighborsRegressor())\n",
    "#nd_reg=RegressorChain(RandomForestRegressor(n_estimators=100, random_state=5))\n",
    "#vr_reg=RegressorChain(SVR(kernel = 'rbf'))\n",
    "\n",
    "reg=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=LinearRegression()))\n",
    "\n",
    "reg.estimator.estimators_=reg.estimator.estimators\n",
    "reg.estimator.final_estimator_=reg.estimator.final_estimator\n",
    "reg.estimator.stack_method_=reg.estimator.stack_method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mounted-jamaica",
   "metadata": {},
   "source": [
    "reg_fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_org_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_syn_dataset(\"104-08\",\"AUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_syn_dataset(\"104-08\",\"AUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "#X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\n",
    "#define the base models\n",
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('rf',RandomForestRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=LinearRegression()\n",
    "#define the stacking ensemble\n",
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "model.fit(X,y)\n",
    "#scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "\n",
    "\n",
    "#make a prediction for one example\n",
    "val = [11.6300, 0.4363]\n",
    "\n",
    "yhat=model.predict([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "#X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\n",
    "#define the base models\n",
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('rf',RandomForestRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=LinearRegression()\n",
    "#define the stacking ensemble\n",
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "model.fit(X,y)\n",
    "#scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "#RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "#MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "#make a prediction for one example\n",
    "#val = [11.6300, 0.4363]\n",
    "\n",
    "#yhat=model.predict([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_syn_dataset(\"104-08\",\"AUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "#X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\n",
    "#define the base models\n",
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('rf',RandomForestRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=LinearRegression()\n",
    "#define the stacking ensemble\n",
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "model.fit(X,y)\n",
    "#scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-boxing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_org_dataset(\"104-08\",\"AUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking():\n",
    "    #define the base models\n",
    "    level0=list()\n",
    "    level0.append(('knn',KNeighborsRegressor()))\n",
    "    level0.append(('cart',DecisionTreeRegressor()))\n",
    "    level0.append(('svm',SVR()))\n",
    "    #define meta learner model\n",
    "    level1=LinearRegression()\n",
    "    #define the stacking ensemble\n",
    "    model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "    model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_x_list=X.to_numpy()\n",
    "arr_y_list=y.to_numpy()\n",
    "results_yhat=list()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_org_test_dataset(\"104-08\",\"AUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_x_list=X.to_numpy()\n",
    "arr_y_list=y.to_numpy()\n",
    "results_yhat=list()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for idx in range(len(X)):\n",
    "#make a prediction for X-array\n",
    "    val = [arr_x_list[idx][0], arr_x_list[idx][1]]\n",
    "    print(val)\n",
    "    yhat=model.predict([val])\n",
    "    results_yhat.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-stuart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_x_list=X.to_numpy()\n",
    "arr_y_list=y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list=y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list[[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-current",
   "metadata": {},
   "source": [
    "## Ensemble Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=mean_squared_error(arr_org_list[[0]], yhat)**0.5\n",
    "B=mean_squared_error(arr_org_list[[1]], yhat)**0.5\n",
    "C=mean_squared_error(arr_org_list[[2]], yhat)**0.5\n",
    "D=mean_squared_error(arr_org_list[[3]], yhat)**0.5\n",
    "E=mean_squared_error(arr_org_list[[4]], yhat)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((A+B+C+D+E)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=mean_absolute_error(arr_org_list[[0]], yhat)\n",
    "B=mean_absolute_error(arr_org_list[[1]], yhat)\n",
    "C=mean_absolute_error(arr_org_list[[2]], yhat)\n",
    "D=mean_absolute_error(arr_org_list[[3]], yhat)\n",
    "E=mean_absolute_error(arr_org_list[[4]], yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((A+B+C+D+E)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_syn_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org, y_org=get_org_test_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list=y_org.to_numpy()\n",
    "arr_org_x_list=X_org.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "sum_rmse=0\n",
    "sum_mae=0\n",
    "for idx in range(len(X_org)):\n",
    "    #mean_squared_error(arr_org_list[[0]], [results_yhat[idx][0]], squared=False)\n",
    "    sum_rmse=sum_rmse+mean_squared_error(arr_org_list[[0]], [results_yhat[idx][0]], squared=True)\n",
    "    sum_mae=sum_mae+mean_absolute_error(arr_org_list[[0]], [results_yhat[idx][0]])\n",
    "\n",
    "print('>%s %.3f' % ('RMSE:', sum_rmse/len(X)))\n",
    "print('>%s %.3f' % ('MAE:', sum_mae/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-oriental",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-count",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_df=pd.DataFrame(yhat,columns=['10','20','30','50','75','100','125','150','200','250'])\n",
    "surface={'0':val[0]}\n",
    "temp_df=pd.DataFrame()\n",
    "temp_df=temp_df.append(surface, ignore_index=True)\n",
    "\n",
    "data_df=pd.concat([temp_df,data_df], axis=1)\n",
    "\n",
    "df=pd.DataFrame(data_df)\n",
    "tmp=[]\n",
    "tmp2=[]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=pd.DataFrame([results_yhat[1][0]],columns=['10','20','30','50','75','100','125','150','200','250'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-onion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-cyprus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_df=pd.DataFrame(yhat,columns=['10','20','30','50','75','100','125','150','200','250'])\n",
    "surface={'0':val[0]}\n",
    "temp_df=pd.DataFrame()\n",
    "temp_df=temp_df.append(surface, ignore_index=True)\n",
    "\n",
    "data_df=pd.concat([temp_df,data_df], axis=1)\n",
    "\n",
    "df=pd.DataFrame(data_df)\n",
    "tmp=[]\n",
    "tmp2=[]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Date=get_date_dataset('104-08','FEB')\n",
    "X_Date['DATE'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_date_test_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='FEB') : temp=reshape_ocean_df_test_FEB[(reshape_ocean_df_test_FEB['Station-Point']==station)&(reshape_ocean_df_test_FEB['DATE'].str.contains(\"-02-\"))]\n",
    "    elif (season=='APR') : temp=reshape_ocean_df_test_APR[(reshape_ocean_df_test_APR['Station-Point']==station)&(reshape_ocean_df_test_APR['DATE'].str.contains(\"-04-\"))]\n",
    "    elif (season=='JUN') : temp=reshape_ocean_df_test_JUN[(reshape_ocean_df_test_JUN['Station-Point']==station)&(reshape_ocean_df_test_JUN['DATE'].str.contains(\"-06-\"))]\n",
    "    elif (season=='AUG') : temp=reshape_ocean_df_test_AUG[(reshape_ocean_df_test_AUG['Station-Point']==station)&(reshape_ocean_df_test_AUG['DATE'].str.contains(\"-08-\"))]\n",
    "    elif (season=='OCT') : temp=reshape_ocean_df_test_OCT[(reshape_ocean_df_test_OCT['Station-Point']==station)&(reshape_ocean_df_test_OCT['DATE'].str.contains(\"-10-\"))]\n",
    "    elif (season=='DEC') : temp=reshape_ocean_df_test_DEC[(reshape_ocean_df_test_DEC['Station-Point']==station)&(reshape_ocean_df_test_DEC['DATE'].str.contains(\"-12-\"))]            \n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=temp[['Station-Point','DATE']]\n",
    "    #y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Date=get_date_test_dataset('104-08','FEB')\n",
    "X_Date['DATE'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[]\n",
    "tmp2=[]\n",
    "for idx in range(len(X_org)):\n",
    "        data_df=pd.DataFrame([results_yhat[idx][0]],columns=['10','20','30','50','75','100','125','150','200','250'])\n",
    "        surface={'0':arr_x_list[idx][0]}\n",
    "        temp_df=pd.DataFrame()\n",
    "        temp_df=temp_df.append(surface, ignore_index=True)\n",
    "        data_df=pd.concat([temp_df,data_df], axis=1)\n",
    "        df=pd.DataFrame(data_df)\n",
    "        for index, row in df.iterrows():\n",
    "            tmp.append(row['0'])\n",
    "            tmp.append(row['10'])\n",
    "            tmp.append(row['20'])\n",
    "            tmp.append(row['30'])\n",
    "            tmp.append(row['50'])\n",
    "            tmp.append(row['75'])\n",
    "            tmp.append(row['100'])\n",
    "            tmp.append(row['125'])\n",
    "            tmp.append(row['150'])\n",
    "            tmp.append(row['200'])\n",
    "            tmp.append(row['250'])\n",
    "    \n",
    "            tmp2.append(arr_org_x_list[idx][0])\n",
    "            tmp2.append(arr_org_list[idx][0])\n",
    "            tmp2.append(arr_org_list[idx][1])\n",
    "            tmp2.append(arr_org_list[idx][2])\n",
    "            tmp2.append(arr_org_list[idx][3])\n",
    "            tmp2.append(arr_org_list[idx][4])\n",
    "            tmp2.append(arr_org_list[idx][5])\n",
    "            tmp2.append(arr_org_list[idx][6])\n",
    "            tmp2.append(arr_org_list[idx][7])\n",
    "            tmp2.append(arr_org_list[idx][8])\n",
    "            tmp2.append(arr_org_list[idx][9])\n",
    "    \n",
    "            Depth  = [0, 10,  20,  30,  50, 75, 100, 125, 150, 200, 250 ]\n",
    "            fig = plt.figure()\n",
    "            plt.title(X_Date['DATE'].iloc[idx],fontsize=15, fontstyle='italic',color='Black')\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(tmp, Depth, 'go--')\n",
    "            ax.plot(tmp2, Depth, 'bo--')\n",
    "            ax.xaxis.tick_top()\n",
    "            ax.set_ylabel('depth')\n",
    "            ax.set_ylim(500, 0)\n",
    "            ax.set_xlim(0, 25)\n",
    "            ax.set_xlabel('Sea Temp level [C]')\n",
    "            ax.figure.legend(['Predicted','True'],bbox_to_anchor=(1,1),bbox_transform=ax.transAxes)\n",
    "            plt.show()\n",
    "            tmp=[]\n",
    "            tmp2=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-telling",
   "metadata": {},
   "source": [
    "## Original DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_org_test_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='FEB') : temp=reshape_ocean_df_test_FEB[(reshape_ocean_df_test_FEB['Station-Point']==station)&(reshape_ocean_df_test_FEB['DATE'].str.contains(\"-02-\"))]\n",
    "    elif (season=='APR') : temp=reshape_ocean_df_test_APR[(reshape_ocean_df_test_APR['Station-Point']==station)&(reshape_ocean_df_test_APR['DATE'].str.contains(\"-04-\"))]\n",
    "    elif (season=='JUN') : temp=reshape_ocean_df_test_JUN[(reshape_ocean_df_test_JUN['Station-Point']==station)&(reshape_ocean_df_test_JUN['DATE'].str.contains(\"-06-\"))]\n",
    "    elif (season=='AUG') : temp=reshape_ocean_df_test_AUG[(reshape_ocean_df_test_AUG['Station-Point']==station)&(reshape_ocean_df_test_AUG['DATE'].str.contains(\"-08-\"))]\n",
    "    elif (season=='OCT') : temp=reshape_ocean_df_test_OCT[(reshape_ocean_df_test_OCT['Station-Point']==station)&(reshape_ocean_df_test_OCT['DATE'].str.contains(\"-10-\"))]\n",
    "    elif (season=='DEC') : temp=reshape_ocean_df_test_DEC[(reshape_ocean_df_test_DEC['Station-Point']==station)&(reshape_ocean_df_test_DEC['DATE'].str.contains(\"-12-\"))]            \n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=temp[['0','SSH']]\n",
    "    y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=get_org_test_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-ceremony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org, y_org=get_org_dataset(\"104-08\",\"FEB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list=y_org.to_numpy()\n",
    "arr_org_x_list=X_org.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_x_list\n",
    "arr_org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "#X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\n",
    "#define the base models\n",
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('cart',DecisionTreeRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=LinearRegression()\n",
    "#define the stacking ensemble\n",
    "model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "model.fit(X,y)\n",
    "\n",
    "#make a prediction for one example\n",
    "val = [11.6300, 0.4363]\n",
    "\n",
    "yhat=model.predict([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_yhat=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for idx in range(len(X)):\n",
    "#make a prediction for X-array\n",
    "    val = [arr_x_list[idx][0], arr_x_list[idx][1]]\n",
    "    print(val)\n",
    "    yhat=model.predict([val])\n",
    "    results_yhat.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_yhat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sum_rmse=0\n",
    "sum_mae=0\n",
    "for idx in range(len(X_org)):\n",
    "    #mean_squared_error(arr_org_list[[0]], [results_yhat[idx][0]], squared=False)\n",
    "    sum_rmse=sum_rmse+mean_squared_error(arr_org_list[[0]], [results_yhat[idx][0]])\n",
    "    sum_mae=sum_mae+mean_absolute_error(arr_org_list[[0]], [results_yhat[idx][0]])\n",
    "\n",
    "print('>%s %.3f' % ('RMSE:', sum_rmse/len(X)))\n",
    "print('>%s %.3f' % ('MAE:', sum_mae/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_org_list=y_org.to_numpy()\n",
    "arr_org_x_list=X_org.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[]\n",
    "tmp2=[]\n",
    "for idx in range(len(X_org)):\n",
    "        data_df=pd.DataFrame([results_yhat[idx][0]],columns=['10','20','30','50','75','100','125','150','200','250'])\n",
    "        surface={'0':arr_x_list[idx][0]}\n",
    "        temp_df=pd.DataFrame()\n",
    "        temp_df=temp_df.append(surface, ignore_index=True)\n",
    "        data_df=pd.concat([temp_df,data_df], axis=1)\n",
    "        df=pd.DataFrame(data_df)\n",
    "        for index, row in df.iterrows():\n",
    "            tmp.append(row['0'])\n",
    "            tmp.append(row['10'])\n",
    "            tmp.append(row['20'])\n",
    "            tmp.append(row['30'])\n",
    "            tmp.append(row['50'])\n",
    "            tmp.append(row['75'])\n",
    "            tmp.append(row['100'])\n",
    "            tmp.append(row['125'])\n",
    "            tmp.append(row['150'])\n",
    "            tmp.append(row['200'])\n",
    "            tmp.append(row['250'])\n",
    "    \n",
    "            tmp2.append(arr_org_x_list[idx][0])\n",
    "            tmp2.append(arr_org_list[idx][0])\n",
    "            tmp2.append(arr_org_list[idx][1])\n",
    "            tmp2.append(arr_org_list[idx][2])\n",
    "            tmp2.append(arr_org_list[idx][3])\n",
    "            tmp2.append(arr_org_list[idx][4])\n",
    "            tmp2.append(arr_org_list[idx][5])\n",
    "            tmp2.append(arr_org_list[idx][6])\n",
    "            tmp2.append(arr_org_list[idx][7])\n",
    "            tmp2.append(arr_org_list[idx][8])\n",
    "            tmp2.append(arr_org_list[idx][9])\n",
    "    \n",
    "            Depth  = [0, 10,  20,  30,  50, 75, 100, 125, 150, 200, 250 ]\n",
    "            fig = plt.figure()\n",
    "            plt.title(X_Date['DATE'].iloc[idx],fontsize=15, fontstyle='italic',color='Black')\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(tmp, Depth, 'go--')\n",
    "            ax.plot(tmp2, Depth, 'bo--')\n",
    "            ax.xaxis.tick_top()\n",
    "            ax.set_ylabel('depth')\n",
    "            ax.set_ylim(500, 0)\n",
    "            ax.set_xlim(0, 25)\n",
    "            ax.set_xlabel('Sea Temp level [C]')\n",
    "            ax.figure.legend(['Predicted','True'],bbox_to_anchor=(1,1),bbox_transform=ax.transAxes)\n",
    "            plt.show()\n",
    "            tmp=[]\n",
    "            tmp2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-insulin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset(station,season):\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    #X,y=\n",
    "    if (season=='AUG') : temp=model_output_df_AUG[(model_output_df_AUG['Station-Point']==station)&(model_output_df_AUG['DATE'].str.contains(\"-08-\"))]\n",
    "    #elif (season=='APR'):\n",
    "    #                     temp=reshape_ocean_df[(reshape_ocean_df['Station-Point']==str(station))&(reshape_ocean_df['DATE'].str.contains(\"-04-\"))]\n",
    "    \n",
    "    X=temp[['0','SSH']]\n",
    "    y=temp[['10','20','30','50','75','100','125','150','200','250']]\n",
    "    #y=temp\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-washington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_df_AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "#X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\n",
    "#define the base models\n",
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('cart',DecisionTreeRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=RandomForestRegressor()\n",
    "#define the stacking ensemble\n",
    "model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "model.fit(X,y)\n",
    "\n",
    "#make a prediction for one example\n",
    "val = [18.8900, 0.4060]\n",
    "\n",
    "yhat=model.predict([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define base model\n",
    "model=LinearSVR()\n",
    "#define the chained multioutput wrapper model\n",
    "wrapper=RegressorChain(model)\n",
    "\n",
    "chain = RegressorChain(base_estimator=logreg, order=[0, 1]).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('cart',DecisionTreeRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=RandomForestRegressor()\n",
    "#define the stacking ensemble\n",
    "model=MultiOutputRegressor(StackingRegressor(estimators=level0, final_estimator=level1, cv=5))\n",
    "model.estimator.final_estimator_ = model.estimator.final_estimator\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list_x=X.to_numpy()\n",
    "arr_list_x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_df=pd.DataFrame(yhat,columns=['10','20','30','50','75','100','125','150','200','250'])\n",
    "surface={'0':val[0]}\n",
    "temp_df=pd.DataFrame()\n",
    "temp_df=temp_df.append(surface, ignore_index=True)\n",
    "\n",
    "data_df=pd.concat([temp_df,data_df], axis=1)\n",
    "\n",
    "df=pd.DataFrame(data_df)\n",
    "tmp=[]\n",
    "tmp2=[]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-medicine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-genius",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-diversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-reception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-meditation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-category",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-longer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
