{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall -y tensorflow keras tf-nightly keras-nightly\n",
    "# pip install tensorflow\n",
    "# library 에러시 conda 밑에 env에 있는 lib 버전 체크 (/usr/local/cuda/lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "#import tensorflow as tf\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "#import tensorflow as tf\n",
    "#devices= tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# 텐서 생성\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "#!pip install netCDF4\n",
    "#!pip install -U matplotlib==3.2\n",
    "#!pip uninstall -y numpy\n",
    "#!pip install numpy\n",
    "#!pip install pyproj==1.9.6\n",
    "#!apt-get install libgeos-3.5.0\n",
    "#!apt-get install libgeos-dev\n",
    "#!pip install https://github.com/matplotlib/basemap/archive/master.zip\n",
    "#!pip install sdv\n",
    "from netCDF4 import Dataset, num2date\n",
    "import pandas as pd\n",
    "#from matplotlib.cbook import dedent\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import xarray\n",
    "import netCDF4 as nc\n",
    "#conda install xarray or pip install xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "sst_dataset=xr.open_dataset('sst_east_sea_month_198311.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_sst_lp=sst_dataset.sst.sel(lon=114.125,lat=14.125, method='nearest',time='1993-03-01')\n",
    "#dataset_sst_lp\n",
    "#dataset_sst_lp.plot()\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "month=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "for year in range (1982,1984):\n",
    "        for mo in month:\n",
    "            sst_nc_file ='sst_merge_month_'+str(year)+mo+'.nc'\n",
    "            print(sst_nc_file)\n",
    "            sst_dataset=xr.open_dataset(sst_nc_file)\n",
    "            #sst_dataset\n",
    "            dataset_sst=sst_dataset.sst\n",
    "            dataset_sst_lp = sst_dataset.sst.sel(lon=slice(125, 140), lat=slice(30, 45))\n",
    "            output_file='sst_east_sea_month_'+str(year)+mo+'.nc'\n",
    "            print(output_file)\n",
    "            dataset_sst_lp.to_netcdf(output_file)\n",
    "            #dataset_sst_lp.plot()\n",
    "        dataset_sst_lp.plot(col=\"time\",col_wrap=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-mobile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst=dataset_sst_lp.to_dataframe()\n",
    "np_sst=df_sst.to_numpy()\n",
    "print(np_sst.shape)\n",
    "print(np_sst)\n",
    "#sst_numpy_array = np.stack([data_array.values for data_array in df_sst['sst'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_sst_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sst_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sst.to_csv('sst.csv')\n",
    "df_sst=pd.read_csv('sst.csv')\n",
    "df_sst[['lat','lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip (df_sst['lon'], df_sst['lat']):\n",
    "  print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/station_103_107_loc.xls',header=0)\n",
    "print(station_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst_dataset.sst.sel(time='1983-12-31').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/station_103_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['latitude','longitude']]\n",
    "print(df_station_point)\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<38) & (df_station_point['longitude']>130)]\n",
    "\n",
    "print(df_station_point_38)\n",
    "\n",
    "#conda install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [0, 1, 2]\n",
    "num_ls=[num for num in lst if num != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cbook import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TF to use only the CPU\n",
    "\n",
    "#plot obs station\n",
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_103_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "\n",
    "print(df_station_point)\n",
    "print(df_station_point[['latitude']])\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "\n",
    "lat_ls=df_station_point_38[['latitude']].values.flatten()\n",
    "lon_ls=df_station_point_38[['longitude']].values.flatten()\n",
    "\n",
    "\n",
    "# Read Temperature Profile\n",
    "\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "col_names=['AREA','Station','Point','Station-Point','Latitude','Longitude','DATE','DEPTH','TEMPERATURE','TEMP/QC','SALINITY','SALINITY/QC','DISOLVED-OXYGEN','DISOLVED-OXYGEN/QC','QC Level','PHOROSPATE',' nitrite nitrogen','nitric acid nitrogen','silicic acid silicon','pH','Transparency','Pressure','Ship']\n",
    "\n",
    "path = '/gpu_deep/Deep_Ocean/obs_east/obs_*_east.xls'\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "files=glob.glob(path)\n",
    "for file in sorted(files):\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file[30:43])\n",
    "        Name=file[30:43]\n",
    "        #print(Name)\n",
    "        statement='/gpu_deep/Deep_Ocean/obs_east/'+ Name + '.xls'\n",
    "        print(statement)\n",
    "#        pd.read_excel('/content/drive/My Drive/Deep_Ocean/obs_east/obs_1965_east.xls', header=1,names=col_names)\n",
    "        df=df.append(pd.read_excel(statement, header=1,names=col_names))\n",
    "\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_103_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "print(df_station_point_38)\n",
    "\n",
    "df_station_point=df_station_point_38['Point'].to_list()\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "df_38=pd.DataFrame()\n",
    "\n",
    "print(\"------------------------DF_STATION_POINT_38----------------------------------------------\")\n",
    "for i in df_station_point:\n",
    "    #print(df[df[\"Station-Point\"]==i])\n",
    "    print(i)\n",
    "    df_38=df_38.append(df[df[\"Station-Point\"]==i])\n",
    "\n",
    "#print(df)\n",
    "temp=df_38\n",
    "print(temp)\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#ocean_df=temp[['DATE','Station','Point','Station-Point','Latitude','Longitude','DEPTH','TEMPERATURE','SALINITY']]\n",
    "ocean_df=temp[['DATE','Station-Point','DEPTH','TEMPERATURE']]\n",
    "ocean_df['DATE']=pd.to_datetime(ocean_df['DATE'],format='%Y-%m-%d %H:%M')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m')\n",
    "ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#select Feb\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02|-03|-04|-06\")]\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-\")]\n",
    "\n",
    "#select From Feb to June\n",
    "ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-|-04|-06\")]\n",
    "\n",
    "print(ocean_df)\n",
    "\n",
    "\n",
    "#ocean_df.pivot(index=['DATE','Station-Point'],columns='DEPTH',values='TEMPERATURE')\n",
    "\n",
    "ocean_grouped = ocean_df[\"DATE\"].unique()\n",
    "\n",
    "#np.unique(ocean_df[['DATE', 'Station-Point']].values)\n",
    "\n",
    "#print(np.unique(ocean_df[['DATE', 'Station-Point']].values))\n",
    "\n",
    "\n",
    "\n",
    "##2중 Loop\n",
    "reshape_ocean_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "obs_station= ocean_df[\"Station-Point\"].unique()\n",
    "\n",
    "#depth_criteria=[0,10,25,50,100,200,250,300,400,500]\n",
    "\n",
    "depth_criteria=[0,10,20,30,50,75,100,125,150,200,250,300,400,500]\n",
    "print(obs_station)\n",
    "\n",
    "#DEPTH=ocean_df_temp['DEPTH'][ocean_df_temp[\"DATE\"]==i].to_list()\n",
    "\n",
    "\n",
    "#tempDf = pd.DataFrame(columns=['PRODUCT','CAT_ID','MARKET_ID'])\n",
    "#tempDf['PRODUCT'] = df['PRODUCT']\n",
    "#tempDf['CAT_ID'] = catid\n",
    "#tempDf['MARKET_ID'] = 13\n",
    "\n",
    "#finalDf = pd.concat([finalDf,tempDf])\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point=pd.DataFrame(df_station_point)\n",
    "\n",
    "\n",
    "print(\"--------DF Station Point-------\")\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "from datetime import date\n",
    "#f_date = date(2014, 7, 2)\n",
    "#l_date = date(2014, 7, 11)\n",
    "#delta = l_date - f_date\n",
    "#print(delta.days)\n",
    "\n",
    "for i in obs_station :\n",
    "        obs_station_date=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]>'1993-01-01') & (ocean_df[\"DATE\"]<'2019-01-01'), \"DATE\"].unique()\n",
    "        print(obs_station_date)\n",
    "        for j in obs_station_date :\n",
    "                    DEPTH=ocean_df['DEPTH'][(ocean_df[\"DATE\"]==j) & (ocean_df[\"Station-Point\"]==i)].to_list()\n",
    "                    DEPTH.sort()\n",
    "                    if DEPTH==depth_criteria:\n",
    "                        print(j,i)\n",
    "                        start_dt=date(int(j[0:4]),1,1)\n",
    "                        print(j[5:7], j[8:10])\n",
    "                        end_dt=date(int(j[0:4]),int(j[5:7]), int(j[8:10]))\n",
    "                        delta=end_dt-start_dt\n",
    "                        ssh_days=delta.days\n",
    "                        obs_lat=df_station_point.loc[(df_station_point['Point']==i),\"latitude\"].values\n",
    "                        obs_lon=df_station_point.loc[(df_station_point['Point']==i),\"longitude\"].values\n",
    "                        print(\"LAT,LON:\",obs_lat,obs_lon)\n",
    "                        #sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_adt_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        print(sst_nc_file)\n",
    "                        ssh_dataset=xr.open_dataset(sst_nc_file)\n",
    "                        print(ssh_dataset)\n",
    "                        #dataset_ssh=ssh_dataset.sla\n",
    "                        dataset_ssh=ssh_dataset.adt\n",
    "                        #dataset_ssh_lp = ssh_dataset.sla.sel(longitude=slice(120, 140), latitude=slice(30, 40))\n",
    "                        dataset_ssh_lp = ssh_dataset.adt.sel(time=j, latitude=obs_lat, longitude=obs_lon, method=\"nearest\")\n",
    "                        print(\"SSH:\", dataset_ssh_lp)\n",
    "                        #output_file='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(year)+'.nc'\n",
    "                        #print(output_file)\n",
    "                        #dataset_ssh_lp.to_netcdf(output_file)                                                \n",
    "                        temp_df[\"DATE\"]=j\n",
    "                        temp_df[\"Station-Point\"]=i\n",
    "                        temp_df[\"Latitude\"]=obs_lat\n",
    "                        temp_df[\"Longitude\"]=obs_lon\n",
    "                        temp_df[\"SSH\"]=dataset_ssh_lp.values\n",
    "                        temp_df[\"0\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==0),\"TEMPERATURE\"].values \n",
    "                        temp_df[\"10\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==10),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"20\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==20),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"30\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==30),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"50\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==50),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"75\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==75),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"100\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==100),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"125\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==125),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"150\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==150),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"200\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==200),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"250\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==250),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"300\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==300),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"400\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==400),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"500\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==500),\"TEMPERATURE\"].values\n",
    "                        print(temp_df)\n",
    "                        reshape_ocean_df=pd.concat([reshape_ocean_df,temp_df])\n",
    "                        \n",
    "print(reshape_ocean_df[reshape_ocean_df['DATE']!=reshape_ocean_df['DATE']])\n",
    "\n",
    "reshape_ocean_df.reset_index()\n",
    "                        \n",
    "                        \n",
    "#                    for k in depth_criteria:\n",
    "#                        if DEPTH==depth_criteria:\n",
    "#                            temp_depth=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==k)]\n",
    "#                            print(temp_depth)\n",
    "#                            #print(i,j,k,temp_depth)\n",
    "                    \n",
    "\n",
    "print(reshape_ocean_df)\n",
    "\n",
    "reshape_ocean_df.to_csv('reshape_ocean_df.csv',index=False)\n",
    "\n",
    "reshape_ocean_df=pd.read_csv('reshape_ocean_df.csv')  \n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "print(reshape_ocean_df.drop([0,0]))\n",
    "\n",
    "\n",
    "reshape_ocean_df_1993=reshape_ocean_df[reshape_ocean_df[\"DATE\"]>'1993-01-01']\n",
    "\n",
    "reshape_ocean_df_1993.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print(reshape_ocean_df_1993)\n",
    "\n",
    "#ls_ocean_dt=ocean_df['DATE'].unique()\n",
    "ls_station_df=ocean_df['Station-Point'].unique()\n",
    "print(ls_station_df.shape)\n",
    "obs_station_lst=ls_station_df.tolist()\n",
    "print(obs_station_lst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import dedent\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "#m = Basemap(projection='lcc', resolution='f',\n",
    "#            width=1E6, height=1E6, \n",
    "#            lat_0=37, lon_0=132,)\n",
    "#m.etopo(scale=0.5, alpha=0.5)\n",
    "m = Basemap(projection='lcc', resolution='f',\n",
    "          lat_0=37, lon_0=132, \n",
    "          llcrnrlon=120.25, llcrnrlat=30.0,\n",
    "          urcrnrlon=140.25, urcrnrlat=40.75)\n",
    "m.etopo(scale=3.0, alpha=2.0)\n",
    "\n",
    "\n",
    "lons, lats = m(lon_ls, lat_ls)\n",
    "#m.scatter(lons, lats, marker = 'o', color='r', zorder=5)\n",
    "m.plot(lons,lats,'ro',markersize=18)\n",
    "plt.show()\n",
    "\n",
    "# Map (long, lat) to (x, y) for plotting\n",
    "#x, y = m(132, 37)\n",
    "#plt.plot(x, y, 'ok', markersize=10)\n",
    "#plt.text(x, y, ' Seoul', fontsize=12);\n",
    "\n",
    "#m.scatter(lon, lat, marker = 'o', color='r', zorder=5)\n",
    "\n",
    "#m.scatter(lon, lat, latlon=True,\n",
    "#          c=np.log10(population), s=area,\n",
    "#          cmap='Reds', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map.drawcoastlines()\n",
    "#map.drawcountries()\n",
    "#map.fillcontinents(color='coral')\n",
    "#map.drawmapboundary()\n",
    " \n",
    "#map.drawmeridians(np.arange(0, 360, 30))\n",
    "#map.drawparallels(np.arange(-90, 90, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df=reshape_ocean_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import dedent\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "#m = Basemap(projection='lcc', resolution='f',\n",
    "#            width=1E6, height=1E6, \n",
    "#            lat_0=37, lon_0=132,)\n",
    "#m.etopo(scale=0.5, alpha=0.5)\n",
    "m = Basemap(projection='lcc', resolution='f',\n",
    "          lat_0=37, lon_0=132, \n",
    "          llcrnrlon=115.25, llcrnrlat=25.0,\n",
    "          urcrnrlon=150.25, urcrnrlat=40.75)\n",
    "#m.etopo(scale=3.0, alpha=2.0)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "#m.fillcontinents(color='coral')\n",
    "#m.drawmapboundary(fill_color='aqua')\n",
    "#m.drawmapboundary()\n",
    "m.shadedrelief()\n",
    "#m.bluemarble()\n",
    "\n",
    "lons, lats = m(lon_ls, lat_ls)\n",
    "#m.scatter(lons, lats, marker = 'o', color='r', zorder=5)\n",
    "m.plot(lons,lats,'bo',markersize=6)\n",
    "plt.show()\n",
    "\n",
    "# Map (long, lat) to (x, y) for plotting\n",
    "#x, y = m(132, 37)\n",
    "#plt.plot(x, y, 'ok', markersize=10)\n",
    "#plt.text(x, y, ' Seoul', fontsize=12);\n",
    "\n",
    "#m.scatter(lon, lat, marker = 'o', color='r', zorder=5)\n",
    "\n",
    "#m.scatter(lon, lat, latlon=True,\n",
    "#          c=np.log10(population), s=area,\n",
    "#          cmap='Reds', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define data\n",
    "\n",
    "Oxygen = [ 0.1 , 0.5, 1, 10, 15, 20, 15, 10, 1, 0.5, 0.5]\n",
    "Depth  = [ 0,     1,  2,  4,  8, 10, 12, 14, 16, 20, 40 ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(Oxygen, Depth, 'go--')\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_ylabel('depth')\n",
    "ax.set_ylim(50, 0)\n",
    "ax.set_xlim(0, 25)\n",
    "ax.set_xlabel('Oxygen level [ppm]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst=dataset_sst_lp.to_dataframe()\n",
    "np_sst=df_sst.to_numpy()\n",
    "print(np_sst.shape)\n",
    "print(np_sst)\n",
    "#sst_numpy_array = np.stack([data_array.values for data_array in df_sst['sst'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reshape_ocean_df=pd.read_csv('reshape_ocean_df.csv')\n",
    "reshape_ocean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df=reshape_ocean_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1993\n",
    "reshape_ocean_df_1993_104_07=reshape_ocean_df_1993[reshape_ocean_df_1993['Station-Point']=='104-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_df_1993_104_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn=reshape_ocean_df[[\"DATE\",\"Station-Point\",\"SSH\",\"0\",\"10\",\"20\",\"30\",\"50\",\"75\",\"100\",\"125\",\"150\",\"200\",\"250\",\"300\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_103_07_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='103-07']\n",
    "reshape_103_08_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='103-08']\n",
    "reshape_103_09_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='103-09']\n",
    "reshape_103_10_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='103-10']\n",
    "reshape_103_11_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='103-11']\n",
    "reshape_104_07_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='104-07']\n",
    "reshape_104_08_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='104-08']\n",
    "reshape_104_09_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='104-09']\n",
    "reshape_104_10_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='104-10']\n",
    "reshape_104_11_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='104-11']\n",
    "reshape_105_07_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='105-07']\n",
    "reshape_105_08_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='105-08']\n",
    "reshape_105_09_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='105-09']\n",
    "reshape_105_10_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='105-10']\n",
    "reshape_105_11_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='105-11']\n",
    "reshape_106_07_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='106-07']\n",
    "reshape_106_08_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='106-08']\n",
    "reshape_106_09_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='106-09']\n",
    "reshape_106_10_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='106-10']\n",
    "reshape_106_11_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"Station-Point\"]=='106-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mon_103_07=reshape_103_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_08=reshape_103_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_09=reshape_103_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_10=reshape_103_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_11=reshape_103_11_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_07=reshape_104_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_08=reshape_104_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_09=reshape_104_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_10=reshape_104_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_11=reshape_104_11_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_07=reshape_105_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_08=reshape_105_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_09=reshape_105_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_10=reshape_105_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_11=reshape_105_11_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_07=reshape_106_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_08=reshape_106_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_09=reshape_106_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_10=reshape_106_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_11=reshape_106_11_temp[\"DATE\"].str.slice(0,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_103_07=set(year_mon_103_07)\n",
    "set_103_08=set(year_mon_103_08)\n",
    "set_103_09=set(year_mon_103_09)\n",
    "set_103_10=set(year_mon_103_10)\n",
    "set_103_11=set(year_mon_103_11)\n",
    "\n",
    "\n",
    "set_104_07=set(year_mon_104_07)\n",
    "set_104_08=set(year_mon_104_08)\n",
    "set_104_09=set(year_mon_104_09)\n",
    "set_104_10=set(year_mon_104_10)\n",
    "set_104_11=set(year_mon_104_11)\n",
    "\n",
    "set_105_07=set(year_mon_105_07)\n",
    "set_105_08=set(year_mon_105_08)\n",
    "set_105_09=set(year_mon_105_09)\n",
    "set_105_10=set(year_mon_105_10)\n",
    "set_105_11=set(year_mon_105_11)\n",
    "\n",
    "set_106_07=set(year_mon_106_07)\n",
    "set_106_08=set(year_mon_106_08)\n",
    "set_106_09=set(year_mon_106_09)\n",
    "set_106_10=set(year_mon_106_10)\n",
    "set_106_11=set(year_mon_106_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_103_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_common_103=(set_103_07&set_103_08&set_103_09&set_103_10&set_103_11)\n",
    "set_common_104=(set_104_07&set_104_08&set_104_09&set_104_10&set_104_11)\n",
    "set_common_105=(set_105_07&set_105_08&set_105_09&set_105_10&set_105_11)\n",
    "set_common_106=(set_106_07&set_106_08&set_106_09&set_106_10&set_106_11)\n",
    "#set_common=set(set_common&set_104_09)\n",
    "\n",
    "set_common=(set_common_103&set_common_104&set_common_105&set_common_106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mon=list(set_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "reshape_ocean_cnn.dropna()\n",
    "type(reshape_ocean_cnn)\n",
    "reshape_ocean_cnn=reshape_ocean_cnn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn\n",
    "reshape_ocean_cnn.to_csv('reshape_ocean_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common=pd.DataFrame()\n",
    "\n",
    "for year in year_mon:\n",
    "    reshape_ocean_cnn_temp=reshape_ocean_cnn[reshape_ocean_cnn[\"DATE\"].str.contains(year)]\n",
    "    reshape_ocean_cnn_common=pd.concat([reshape_ocean_cnn_temp,reshape_ocean_cnn_common])\n",
    "\n",
    "    \n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "reshape_ocean_cnn_common.shape\n",
    "reshape_ocean_cnn_common\n",
    "reshape_ocean_cnn_common.to_csv('reshape_ocean_cnn_common.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_list=['10','20','30','50','75','100','125','150','200','250','300']\n",
    "for depth in depth_list:\n",
    "    print(depth)\n",
    "    reshape_ocean_cnn_common=pd.DataFrame()\n",
    "    reshape_ocean_cnn_temp=pd.DataFrame()\n",
    "    for year in year_mon:\n",
    "        print(year)\n",
    "        reshape_ocean_cnn_temp['DATE']=year\n",
    "        reshape_ocean_cnn_temp[\"103-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-07'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"103-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-08'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"103-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-09'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"103-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-10'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"103-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-11'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"104-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-07'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"104-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-08'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"104-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-09'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"104-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-10'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"104-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-11'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"105-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-07'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"105-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-08'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"105-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-09'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"105-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-10'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"105-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-11'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"106-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-07'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"106-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-08'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"106-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-09'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"106-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-10'),depth].values\n",
    "        reshape_ocean_cnn_temp[\"106-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-11'),depth].values\n",
    "        reshape_ocean_cnn_common=pd.concat([reshape_ocean_cnn_temp,reshape_ocean_cnn_common])\n",
    "        #print(reshape_ocean_cnn_common)\n",
    "    reshape_ocean_cnn_common.reset_index()\n",
    "    #print(depth)\n",
    "    reshape_ocean_cnn_common.to_csv('reshape_ocean_common_sub_'+depth+'.csv',index=False)\n",
    "    reshape_ocean_cnn_common_sub=pd.read_csv('reshape_ocean_common_sub_'+depth+'.csv')\n",
    "    reshape_ocean_cnn_common_sub=reshape_ocean_cnn_common_sub[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11','103-07','103-08','103-09','103-10','103-11']]\n",
    "    globals()['reshape_ocean_cnn_common_sub_'+depth]=reshape_ocean_cnn_common_sub.dropna()\n",
    "    \n",
    "reshape_ocean_cnn_common_sub_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common=pd.DataFrame()\n",
    "reshape_ocean_cnn_temp=pd.DataFrame()\n",
    "for year in year_mon:\n",
    "    print(year)\n",
    "    reshape_ocean_cnn_temp['DATE']=year\n",
    "    reshape_ocean_cnn_temp[\"103-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-07'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-08'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-09'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-10'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-11'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-07'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-08'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-09'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-10'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-11'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-07'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-08'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-09'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-10'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-11'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-07'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-08'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-09'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-10'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-11'),\"SSH\"].values\n",
    "    reshape_ocean_cnn_common=pd.concat([reshape_ocean_cnn_temp,reshape_ocean_cnn_common])\n",
    "    print(reshape_ocean_cnn_common)\n",
    "\n",
    "reshape_ocean_cnn_common.reset_index()\n",
    "reshape_ocean_cnn_common.to_csv('reshape_ocean_common_ssh.csv',index=False)\n",
    "reshape_ocean_cnn_common_ssh=pd.read_csv('reshape_ocean_common_ssh.csv')\n",
    "reshape_ocean_cnn_common_ssh=reshape_ocean_cnn_common_ssh[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11','103-07','103-08','103-09','103-10','103-11']]\n",
    "globals()['reshape_ocean_cnn_common_ssh']=reshape_ocean_cnn_common_sub.dropna()\n",
    "#reshape_ocean_cnn_common_ssh=reshape_ocean_cnn_common_ssh.dropna()\n",
    "reshape_ocean_cnn_common_ssh\n",
    "#reshape_ocean_cnn_common_ssh=reshape_ocean_cnn_common_ssh[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11']]\n",
    "#reshape_ocean_cnn_common_ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv import load_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import CopulaGAN\n",
    "#model_upper = CopulaGAN(constraints=constraints_upper)\n",
    "model=CopulaGAN()\n",
    "#model_upper.fit(ocean_train_upper)\n",
    "model.fit(reshape_ocean_cnn_common_ssh)\n",
    "#model_upper.sample(100, max_retries=100000, conditions=conditions)\n",
    "model.sample(1000, max_retries=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common_sst=pd.DataFrame()\n",
    "reshape_ocean_cnn_temp=pd.DataFrame()\n",
    "for year in year_mon:\n",
    "    reshape_ocean_cnn_temp[\"DATE\"]=year\n",
    "    reshape_ocean_cnn_temp[\"103-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-07'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-08'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-09'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-10'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"103-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='103-11'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-07'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-08'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-09'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-10'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"104-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='104-11'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-07'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-08'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-09'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-10'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"105-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='105-11'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-07\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-07'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-08\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-08'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-09\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-09'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-10\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-10'),\"0\"].values\n",
    "    reshape_ocean_cnn_temp[\"106-11\"]=reshape_ocean_cnn.loc[(reshape_ocean_cnn[\"DATE\"].str.contains(year)) & (reshape_ocean_cnn[\"Station-Point\"]=='106-11'),\"0\"].values\n",
    "    reshape_ocean_cnn_common_sst=pd.concat([reshape_ocean_cnn_temp,reshape_ocean_cnn_common_sst])\n",
    "    #reshape_ocean_cnn_common_sst=pd.concat([reshape_ocean_cnn_temp,reshape_ocean_cnn_common_sst])\n",
    "\n",
    "reshape_ocean_cnn_common_sst.reset_index()\n",
    "reshape_ocean_cnn_common_sst.to_csv('reshape_ocean_cnn_common_sst.csv',index=False)\n",
    "reshape_ocean_cnn_common_sst=pd.read_csv('reshape_ocean_cnn_common_sst.csv')\n",
    "#reshape_ocean_cnn_common_sst=reshape_ocean_cnn_common_sst.dropna()\n",
    "reshape_ocean_cnn_common_sst=reshape_ocean_cnn_common_sst[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11','103-07','103-08','103-09','103-10','103-11']]\n",
    "#reshape_ocean_cnn_common_sst\n",
    "globals()['reshape_ocean_cnn_common_sst']=reshape_ocean_cnn_common_sst.dropna()\n",
    "#reshape_ocean_cnn_common_sst=reshape_ocean_cnn_common_sst[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11']]\n",
    "reshape_ocean_cnn_common_sst\n",
    "#reshape_ocean_cnn_common_sst=reshape_ocean_cnn_common_sst[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11']]\n",
    "reshape_ocean_cnn_common_sst\n",
    "#reshape_ocean_cnn_common_sst['DATE','106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-8','104-09','104-10','104-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common_sub_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-spelling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "\n",
    "# row vector via reshape\n",
    "x.reshape((1, 3))\n",
    "\n",
    "reshape_ocean_cnn_sst_array=reshape_ocean_cnn_common_sst.to_numpy()\n",
    "reshape_ocean_cnn_sst_array.shape\n",
    "reshape_ocean_cnn_sst_array.reshape(33,4,5)\n",
    "\n",
    "reshape_ocean_cnn_ssh_array=reshape_ocean_cnn_common_ssh.to_numpy()\n",
    "reshape_ocean_cnn_ssh_array.shape\n",
    "reshape_ocean_cnn_ssh_array.reshape(33,4,5)\n",
    "\n",
    "\n",
    "reshape_ocean_cnn_sub_10_array=reshape_ocean_cnn_common_sub_10.to_numpy()\n",
    "reshape_ocean_cnn_sub_10_array.shape\n",
    "reshape_ocean_cnn_sub_10_array.reshape(33,4,5)\n",
    "\n",
    "\n",
    "reshape_ocean_cnn_sub_250_array=reshape_ocean_cnn_common_sub_250.to_numpy()\n",
    "reshape_ocean_cnn_sub_250_array.shape\n",
    "reshape_ocean_cnn_sub_250_array.reshape(33,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Conv2D\n",
    "from keras.models import Model\n",
    "\n",
    "#red   = np.array([1]*9).reshape((3,3))\n",
    "#green = np.array([100]*9).reshape((3,3))\n",
    "#blue  = np.array([10000]*9).reshape((3,3))\n",
    "\n",
    "inputs = Input((2,5,3))\n",
    "conv = Conv2D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=2, \n",
    "              kernel_initializer='ones', \n",
    "              bias_initializer='zeros', )(inputs)\n",
    "model = Model(inputs,conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.constraints import GreaterThan\n",
    "depth_temp_0_10_const = GreaterThan(\n",
    "low='10',\n",
    "high='0',\n",
    "handling_strategy='reject_sampling')\n",
    "#0\t10\t20\t30\t50\t75\t100\t125\t150\t200\t250\t300\t400\t500\n",
    "depth_temp_20_10_const = GreaterThan(\n",
    "low='20',\n",
    "high='10',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_30_20_const = GreaterThan(\n",
    "low='30',\n",
    "high='20',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_50_30_const = GreaterThan(\n",
    "low='50',\n",
    "high='30',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_75_50_const = GreaterThan(\n",
    "low='75',\n",
    "high='50',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_100_75_const = GreaterThan(\n",
    "low='100',\n",
    "high='75',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_125_100_const = GreaterThan(\n",
    "low='125',\n",
    "high='100',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_150_125_const = GreaterThan(\n",
    "low='150',\n",
    "high='125',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_200_150_const = GreaterThan(\n",
    "low='200',\n",
    "high='150',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "\n",
    "depth_temp_250_200_const = GreaterThan(\n",
    "low='250',\n",
    "high='200',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_300_250_const = GreaterThan(\n",
    "low='300',\n",
    "high='250',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_400_300_const = GreaterThan(\n",
    "low='400',\n",
    "high='300',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "depth_temp_500_400_const = GreaterThan(\n",
    "low='500',\n",
    "high='400',\n",
    "handling_strategy='reject_sampling')\n",
    "\n",
    "constraints_upper = [depth_temp_0_10_const,\n",
    "                     depth_temp_20_10_const,\n",
    "                     depth_temp_30_20_const,\n",
    "                     depth_temp_50_30_const,\n",
    "                     depth_temp_75_50_const,\n",
    "                     depth_temp_100_75_const,\n",
    "                     depth_temp_125_100_const,\n",
    "                     depth_temp_150_125_const,\n",
    "                     depth_temp_200_150_const,\n",
    "                     depth_temp_250_200_const,\n",
    "                     depth_temp_300_250_const,\n",
    "                     depth_temp_400_300_const,\n",
    "                     depth_temp_500_400_const]\n",
    "\n",
    "constraints_upper2 = [depth_temp_0_10_const,\n",
    "                     depth_temp_20_10_const,\n",
    "                     depth_temp_30_20_const,\n",
    "                     depth_temp_50_30_const,\n",
    "                     depth_temp_75_50_const,\n",
    "                     depth_temp_100_75_const,\n",
    "                     depth_temp_125_100_const,\n",
    "                     depth_temp_150_125_const,\n",
    "                     depth_temp_200_150_const,\n",
    "                     depth_temp_250_200_const,\n",
    "                     depth_temp_300_250_const]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common_sub_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_104_07=reshape_ocean_cnn[reshape_ocean_cnn['Station-Point']=='104-07']\n",
    "reshape_ocean_cnn_104_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "model = GaussianCopula(constraints=constraints_upper2)\n",
    "model.fit(reshape_ocean_cnn)\n",
    "model_ctn_output = model.sample(1000, max_retries=1000000)\n",
    "\n",
    "from sdv.tabular import TVAE\n",
    "model = TVAE(constraints=constraints_upper2,cuda=True)\n",
    "model.fit(reshape_ocean_cnn_104_07)\n",
    "model_ctn_output2 = model.sample(1000, max_retries=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output=model_ctn_output.reset_index(drop=True)\n",
    "model_ctn_output2=model_ctn_output2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-majority",
   "metadata": {},
   "source": [
    "## Ensemble of VAE and GaussianCopula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "model_ctn_output_ensemble=pd.DataFrame()\n",
    "for i in range (1,10):\n",
    "    print(\"Gaussian-Iteration Time\", i)\n",
    "    model = GaussianCopula(constraints=constraints_upper2)\n",
    "    model.fit(reshape_ocean_cnn)\n",
    "    model_ctn_output_gaussian = model.sample(1000, max_retries=1000000)\n",
    "    model_ctn_output_ensemble = pd.concat([model_ctn_output_ensemble,model_ctn_output_gaussian])\n",
    "\n",
    "from sdv.tabular import TVAE\n",
    "for i in range (1,10):\n",
    "    print(\"TVAE-Iteration Time\", i)\n",
    "    model = TVAE(constraints=constraints_upper2,cuda=True)\n",
    "    model.fit(reshape_ocean_cnn)\n",
    "    model_ctn_output_tvae = model.sample(1000, max_retries=1000000)\n",
    "    model_ctn_output_ensemble = pd.concat([model_ctn_output_ensemble,model_ctn_output_tvae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "model_ctn_output_ensemble=pd.DataFrame()\n",
    "for i in range (1,10):\n",
    "    print(\"Gaussian-Iteration Time\", i)\n",
    "    model = GaussianCopula(constraints=constraints_upper2)\n",
    "    model.fit(reshape_ocean_cnn)\n",
    "    model_ctn_output_gaussian = model.sample(1000, max_retries=1000000)\n",
    "    model_ctn_output_ensemble = pd.concat([model_ctn_output_ensemble,model_ctn_output_gaussian])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output_tvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ctn_output.compare(model_ctn_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ctn_output_sum=pd.concat([model_ctn_output_tvae,model_ctn_output_gaussian])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output=model_ctn_output_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_104_07_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='103-07']\n",
    "reshape_104_08_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='103-08']\n",
    "reshape_104_09_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='103-09']\n",
    "reshape_104_10_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='103-10']\n",
    "reshape_104_11_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='103-11']\n",
    "reshape_104_07_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='104-07']\n",
    "reshape_104_08_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='104-08']\n",
    "reshape_104_09_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='104-09']\n",
    "reshape_104_10_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='104-10']\n",
    "reshape_104_11_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='104-11']\n",
    "reshape_105_07_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='105-07']\n",
    "reshape_105_08_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='105-08']\n",
    "reshape_105_09_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='105-09']\n",
    "reshape_105_10_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='105-10']\n",
    "reshape_105_11_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='105-11']\n",
    "reshape_106_07_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='106-07']\n",
    "reshape_106_08_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='106-08']\n",
    "reshape_106_09_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='106-09']\n",
    "reshape_106_10_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='106-10']\n",
    "reshape_106_11_temp=model_ctn_output[model_ctn_output[\"Station-Point\"]=='106-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mon_103_07=reshape_103_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_08=reshape_103_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_09=reshape_103_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_10=reshape_103_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_103_11=reshape_103_11_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_07=reshape_104_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_08=reshape_104_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_09=reshape_104_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_10=reshape_104_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_104_11=reshape_104_11_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_07=reshape_105_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_08=reshape_105_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_09=reshape_105_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_10=reshape_105_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_105_11=reshape_105_11_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_07=reshape_106_07_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_08=reshape_106_08_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_09=reshape_106_09_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_10=reshape_106_10_temp[\"DATE\"].str.slice(0,7)\n",
    "year_mon_106_11=reshape_106_11_temp[\"DATE\"].str.slice(0,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_104_07_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_103_07=set(year_mon_103_07)\n",
    "set_103_08=set(year_mon_103_08)\n",
    "set_103_09=set(year_mon_103_09)\n",
    "set_103_10=set(year_mon_103_10)\n",
    "set_103_11=set(year_mon_103_11)\n",
    "\n",
    "set_104_07=set(year_mon_104_07)\n",
    "set_104_08=set(year_mon_104_08)\n",
    "set_104_09=set(year_mon_104_09)\n",
    "set_104_10=set(year_mon_104_10)\n",
    "set_104_11=set(year_mon_104_11)\n",
    "\n",
    "set_105_07=set(year_mon_105_07)\n",
    "set_105_08=set(year_mon_105_08)\n",
    "set_105_09=set(year_mon_105_09)\n",
    "set_105_10=set(year_mon_105_10)\n",
    "set_105_11=set(year_mon_105_11)\n",
    "\n",
    "set_106_07=set(year_mon_106_07)\n",
    "set_106_08=set(year_mon_106_08)\n",
    "set_106_09=set(year_mon_106_09)\n",
    "set_106_10=set(year_mon_106_10)\n",
    "set_106_11=set(year_mon_106_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_104_07\n",
    "set_104_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_common_103=(set_103_07&set_103_08&set_103_09&set_103_10&set_103_11)\n",
    "set_common_104=(set_104_07&set_104_08&set_104_09&set_104_10&set_104_11)\n",
    "set_common_105=(set_105_07&set_105_08&set_105_09&set_105_10&set_105_11)\n",
    "set_common_106=(set_106_07&set_106_08&set_106_09&set_106_10&set_106_11)\n",
    "#set_common=set(set_common&set_104_09)\n",
    "\n",
    "set_common=(set_common_103&set_common_104&set_common_105&set_common_106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set_common_106)\n",
    "print(set_common_105)\n",
    "print(set_common_104)\n",
    "print(set_common_103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mon=list(set_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_ctn_temp=pd.DataFrame()\n",
    "#reshape_ocean_ctn_temp[\"104-09\"]=model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),\"20\"].values\n",
    "reshape_ocean_ctn_temp[\"104-07\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),\"10\"].values)\n",
    "\n",
    "print(reshape_ocean_ctn_temp[\"104-07\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_list=['10','20','30','50','75','100','125','150','200','250','300']\n",
    "reshape_ocean_ctn_common=pd.DataFrame()\n",
    "reshape_ocean_ctn_temp=pd.DataFrame()\n",
    "for depth in depth_list:\n",
    "    print(depth)\n",
    "#    reshape_ocean_ctn_common=pd.DataFrame()\n",
    "#    reshape_ocean_ctn_temp=pd.DataFrame()\n",
    "    for year in year_mon:\n",
    "        print(year)\n",
    "        reshape_ocean_ctn_temp[\"DATE\"]=year\n",
    "        #print(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),depth].values))\n",
    "        reshape_ocean_ctn_temp[\"103-07\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-07'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"103-08\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-08'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"103-09\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-09'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"103-10\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-10'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"103-11\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-11'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"104-07\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"104-08\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-08'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"104-09\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-09'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"104-10\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-10'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"104-11\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-11'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"105-07\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-07'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"105-08\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-08'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"105-09\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-09'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"105-10\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-10'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"105-11\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-11'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"106-07\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-07'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"106-08\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-08'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"106-09\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-09'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"106-10\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-10'),depth].values)\n",
    "        reshape_ocean_ctn_temp[\"106-11\"]=np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-11'),depth].values)\n",
    "        print(reshape_ocean_ctn_temp[\"104-07\"])\n",
    "        reshape_ocean_ctn_common=pd.concat([reshape_ocean_ctn_temp,reshape_ocean_ctn_common])\n",
    "        #print(reshape_ocean_cnn_common)\n",
    "    reshape_ocean_ctn_common.reset_index()\n",
    "    print(depth)\n",
    "    reshape_ocean_ctn_common.to_csv('reshape_ocean_ctn_common_sub_'+depth+'.csv',index=False)\n",
    "    reshape_ocean_ctn_common_sub=pd.read_csv('reshape_ocean_ctn_common_sub_'+depth+'.csv')\n",
    "    reshape_ocean_ctn_common_sub=reshape_ocean_ctn_common_sub[['106-07','106-08','106-09','106-10','106-11','105-07','105-08','105-09','105-10','105-11','104-07','104-08','104-09','104-10','104-11','103-07','103-08','103-09','103-10','103-11']]\n",
    "    globals()['reshape_ocean_ctn_common_sub_'+depth]=reshape_ocean_ctn_common_sub.dropna()\n",
    "    \n",
    "reshape_ocean_ctn_common_sub_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),depth].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth='10'\n",
    "\n",
    "\n",
    "reshape_ocean_ctn_common=pd.DataFrame()\n",
    "reshape_ocean_ctn_temp=pd.DataFrame(columns=['DATE','103-07','103-08','103-09','103-10','103-11','104-07','104-08','104-09','104-10','104-11','105-07','105-08','105-09','105-10','105-11','106-07','106-08','106-09','106-10','106-11'])\n",
    "\n",
    "#np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),depth].values\n",
    "\n",
    "reshape_ocean_ctn_temp_list=[]\n",
    "print(reshape_ocean_ctn_temp)\n",
    "for year in year_mon:\n",
    "    print(year)\n",
    "    reshape_ocean_ctn_temp_list.append(year)\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-07'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-08'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-09'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-10'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-11'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-08'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-09'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-10'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-11'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-07'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-08'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-09'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-10'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-11'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-07'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-08'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-09'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-10'),depth].values))\n",
    "    reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-11'),depth].values))\n",
    "    a_series = pd.Series(reshape_ocean_ctn_temp_list, index = reshape_ocean_ctn_temp.columns)\n",
    "    reshape_ocean_ctn_temp.append(a_series,ignore_index=True)\n",
    "    print(reshape_ocean_ctn_temp)\n",
    "    reshape_ocean_ctn_common=pd.concat([reshape_ocean_ctn_temp,reshape_ocean_ctn_common])\n",
    "    reshape_ocean_ctn_temp_list=[]\n",
    "#   reshape_ocean_ctn_common=pd.concat([reshape_ocean_ctn_temp,reshape_ocean_ctn_common])\n",
    "        \n",
    "#print(reshape_ocean_ctn_temp_list)\n",
    "print(reshape_ocean_ctn_common)\n",
    "#a_series = pd.Series(reshape_ocean_ctn_temp_list, index = reshape_ocean_ctn_temp.columns)\n",
    "#reshape_ocean_ctn_temp.append(a_series,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth='10'\n",
    "\n",
    "\n",
    "reshape_ocean_ctn_common=pd.DataFrame()\n",
    "reshape_ocean_ctn_temp=pd.DataFrame(columns=['DATE','103-07','103-08','103-09','103-10','103-11','104-07','104-08','104-09','104-10','104-11','105-07','105-08','105-09','105-10','105-11','106-07','106-08','106-09','106-10','106-11'])\n",
    "\n",
    "reshape_ocean_ctn_temp_list=[]\n",
    "\n",
    "print(year)\n",
    "reshape_ocean_ctn_temp_list.append(year)\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-07'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-08'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-09'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-10'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='103-11'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-07'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-08'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-09'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-10'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='104-11'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-07'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-08'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-09'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-10'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='105-11'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-07'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-08'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-09'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-10'),depth].values))\n",
    "reshape_ocean_ctn_temp_list.append(np.mean(model_ctn_output.loc[(model_ctn_output[\"DATE\"].str.contains(year)) & (model_ctn_output[\"Station-Point\"]=='106-11'),depth].values))\n",
    "a_series = pd.Series(reshape_ocean_ctn_temp_list, index = reshape_ocean_ctn_temp.columns)\n",
    "reshape_ocean_ctn_temp.append(a_series,ignore_index=True)\n",
    "print(reshape_ocean_ctn_temp)\n",
    "\n",
    "#        reshape_ocean_ctn_common=pd.concat([reshape_ocean_ctn_temp,reshape_ocean_ctn_common])\n",
    "        \n",
    "#print(reshape_ocean_ctn_temp_list)\n",
    "print(reshape_ocean_ctn_temp)\n",
    "#a_series = pd.Series(reshape_ocean_ctn_temp_list, index = reshape_ocean_ctn_temp.columns)\n",
    "#reshape_ocean_ctn_temp.append(a_series,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output\n",
    "df=pd.DataFrame(model_ctn_output)\n",
    "tmp=[]\n",
    "for index, row in df.iterrows():\n",
    "    tmp.append(row['0'])\n",
    "    tmp.append(row['10'])\n",
    "    tmp.append(row['20'])\n",
    "    tmp.append(row['30'])\n",
    "    tmp.append(row['50'])\n",
    "    tmp.append(row['75'])\n",
    "    tmp.append(row['100'])\n",
    "    tmp.append(row['125'])\n",
    "    tmp.append(row['150'])\n",
    "    tmp.append(row['200'])\n",
    "    tmp.append(row['250'])\n",
    "    tmp.append(row['300'])\n",
    "    Depth  = [ 0, 10,  20,  30,  50, 75, 100, 125, 150, 200, 250, 300 ]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(tmp, Depth, 'go--')\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_ylabel('depth')\n",
    "    ax.set_ylim(500, 0)\n",
    "    ax.set_xlim(0, 25)\n",
    "    ax.set_xlabel('Sea Temp level [ppm]')\n",
    "    plt.show()\n",
    "    tmp=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output.shape\n",
    "height_list=[0,1,2,3,4]\n",
    "weight_list=[0,1,2]\n",
    "list=[0,1,2,3,4,5]\n",
    "for case in list:\n",
    "    for height in height_list:\n",
    "        for weight in weight_list:\n",
    "            temp_list=[]\n",
    "            for depth in depth_list:\n",
    "                temp=model_ctn_output[[depth]][case,0,1,:]\n",
    "                temp_list.append(temp)\n",
    "        \n",
    "    #print(temp_list)\n",
    "        print(temp_list)\n",
    "        Depth  = [ 10,  20,  30,  50, 75, 100, 125, 150, 200, 250, 300 ]\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(temp_list, Depth, 'go--')\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set_ylabel('depth')\n",
    "        ax.set_ylim(500, 0)\n",
    "        ax.set_xlim(0, 25)\n",
    "        ax.set_xlabel('Sea Temp level [ppm]')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_common_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sdv.tabular import CopulaGAN\n",
    "#model_upper = CopulaGAN(constraints=constraints_upper)\n",
    "#model=CopulaGAN()\n",
    "from sdv.tabular import CTGAN\n",
    "#model_upper.fit(ocean_train_upper)\n",
    "model=CTGAN()\n",
    "model.fit(reshape_ocean_cnn_common_sst)\n",
    "#model_upper.sample(100, max_retries=100000, conditions=conditions)\n",
    "#model.sample(1000, max_retries=1000000)\n",
    "model_ctn_output_sst=model.sample(1000, max_retries=1000000)\n",
    "\n",
    "from sdv.tabular import GaussianCopula\n",
    "model = GaussianCopula()\n",
    "model.fit(reshape_ocean_cnn_common_ssh)\n",
    "model_ctn_output_ssh= model.sample(1000, max_retries=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "model = GaussianCopula()\n",
    "model.fit(reshape_ocean_cnn_common_ssh)\n",
    "model_ctn_output_ssh= model.sample(1000, max_retries=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_ctn_output,columns=['103-07','103-08','103-09','103-10','103-11','104-07','104-08','104-09','104-10','104-11','105-07','105-08','105-09','105-10','105-11','106-07','106-08','106-09','106-10','106-11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "red   = np.array([1]*9).reshape((3,3))\n",
    "green = np.array([100]*9).reshape((3,3))\n",
    "blue  = np.array([10000]*9).reshape((3,3))\n",
    "img = np.stack([red, green, blue], axis=-1)\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "red\n",
    "green\n",
    "blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-liberia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "img\n",
    "inputs = Input((3,3,3))\n",
    "conv = Conv2D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=2, \n",
    "              kernel_initializer='ones', \n",
    "              bias_initializer='zeros', )(inputs)\n",
    "model = Model(inputs,conv)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "tf.config.get_visible_devices(\n",
    "    device_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(img)\n",
    "# array([[[[40404.],\n",
    "#          [40404.]],\n",
    "\n",
    "#         [[40404.],\n",
    "#          [40404.]]]], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3],[6,7,8],[9,10,11]])\n",
    "b = np.array([[2, 3, 4],[11,12,13],[15,16,17]])\n",
    "np.stack((a, b),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn_sst_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_sst=reshape_ocean_cnn_sst_array.reshape(33,4,5)\n",
    "reshape_ssh=reshape_ocean_cnn_ssh_array.reshape(33,4,5)\n",
    "reshape_sub_10=reshape_ocean_cnn_sub_10_array.reshape(33,4,5)\n",
    "reshape_sub_250=reshape_ocean_cnn_sub_250_array.reshape(33,4,5)\n",
    "img_profile = np.stack([reshape_sst, reshape_ssh], axis=-1)\n",
    "#img_profile= np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in depth_list:\n",
    "    globals()['reshape_ocean_cnn_sub_'+str(depth)+'_array']=globals()['reshape_ocean_cnn_common_sub_'+str(depth)].to_numpy()\n",
    "    globals()['reshape_ocean_cnn_sub_'+str(10)+'_array'].shape\n",
    "    globals()['reshape_sub_'+str(depth)]=globals()['reshape_ocean_cnn_sub_'+str(depth)+'_array'].reshape(33,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_profile.shape\n",
    "img_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((4,5,2))\n",
    "conv = Conv2D(filters=1, \n",
    "              strides=1, \n",
    "              padding='same', \n",
    "              activation='relu',\n",
    "              kernel_size=2, \n",
    "              kernel_initializer='ones', \n",
    "              bias_initializer='zeros', )(inputs)\n",
    "model = Model(inputs,conv)\n",
    "model.compile (optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(img_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\n",
    "device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "model.fit(img_profile,reshape_sub_10,epochs=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value=model.predict(img_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_sub_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_sub_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((4,5,2))\n",
    "conv = Conv2D(filters=1, \n",
    "              strides=1, \n",
    "              padding='same', \n",
    "              activation='relu',\n",
    "              kernel_size=2, \n",
    "              kernel_initializer='ones', \n",
    "              bias_initializer='zeros', )(inputs)\n",
    "model = Model(inputs,conv)\n",
    "model.compile (optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "depth_list=['10','20','30','50','75','100','125','150','200','250','300']\n",
    "for depth in depth_list:\n",
    "    inputs = Input((4,5,2))\n",
    "    conv = Conv2D(filters=1, \n",
    "                  strides=1, \n",
    "                  padding='same', \n",
    "                  activation='relu',\n",
    "                  kernel_size=2, \n",
    "                  kernel_initializer='ones', \n",
    "                  bias_initializer='zeros', )(inputs)\n",
    "    model = Model(inputs,conv)\n",
    "    model.compile (optimizer='adam',loss='mse')\n",
    "    train_y=globals()['reshape_sub_'+str(depth)]\n",
    "    print(train_y)\n",
    "    model.fit(img_profile,train_y,epochs=7000,verbose=1)\n",
    "    globals()['predicted_value'+str(depth)]=model.predict(img_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_value=model.predict(img_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value300[[[0,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define data\n",
    "\n",
    "#temp = [ 0.1 , 0.5, 1, 10, 15, 20, 15, 10, 1, 0.5, 0.5]\n",
    "\n",
    "#thislist.extend(thistuple)\n",
    "\n",
    "#for org_temp in org_104_ocean_df_list:\n",
    "#    print(org_temp)\n",
    "\n",
    "\n",
    "height_list=[0,1,2]\n",
    "weight_list=[0,1,2,3,4]\n",
    "list=[0,1,2,3,4,5,6,7,8,9,10]\n",
    "for case in list:\n",
    "    for height in height_list:\n",
    "        for weight in weight_list:\n",
    "            temp_list=[]\n",
    "            for depth in depth_list:\n",
    "                temp=globals()['predicted_value'+str(depth)][case,height,weight,:]\n",
    "                temp_list.append(temp)\n",
    "        \n",
    "    #print(temp_list)\n",
    "            print(case)\n",
    "            Depth  = [ 10,  20,  30,  50, 75, 100, 125, 150, 200, 250, 300 ]\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(temp_list, Depth, 'go--')\n",
    "            ax.plot(temp_list, Depth, 'bo--')\n",
    "            ax.xaxis.tick_top()\n",
    "            ax.set_ylabel('depth')\n",
    "            ax.set_ylim(500, 0)\n",
    "            ax.set_xlim(0, 25)\n",
    "            ax.set_xlabel('Sea Temp level')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_104_ocean_df_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define data\n",
    "\n",
    "#temp = [ 0.1 , 0.5, 1, 10, 15, 20, 15, 10, 1, 0.5, 0.5]\n",
    "\n",
    "#thislist.extend(thistuple)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for org_temp in org_104_ocean_df_list:\n",
    "#    print(org_temp)\n",
    "\n",
    "height_list=[0,1,2]\n",
    "weight_list=[0,1,2,3,4]\n",
    "#list=[0,1,2,3,4,5,6,7,8,9,10]\n",
    "list=[0]\n",
    "for case in list:\n",
    "    for height in height_list:\n",
    "        for weight in weight_list:\n",
    "            temp_list=[]\n",
    "            for depth in depth_list:\n",
    "                temp=globals()['predicted_value'+str(depth)][case,height,weight,:]\n",
    "                temp_list.append(temp)\n",
    "    #print(temp_list)\n",
    "            print(case,height,weight)\n",
    "            #print(temp_list)\n",
    "            #for org_temp in org_104_ocean_df_list:\n",
    "            Depth  = [ 10,  20,  30,  50, 75, 100, 125, 150, 200, 250, 300 ]\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(temp_list, Depth, 'go--')\n",
    "            ax.plot(org_104_ocean_df_list[weight], Depth, 'bo--')\n",
    "            ax.xaxis.tick_top()\n",
    "            ax.set_ylabel('depth')\n",
    "            ax.set_ylim(500, 0)\n",
    "            ax.set_xlim(0, 25)\n",
    "            ax.set_xlabel('Sea Temp level')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_pt_graph=reshape_ocean_cnn.dropna()[[\"DATE\",\"Station-Point\",\"10\",\"20\",\"30\",\"50\",\"75\",\"100\",\"125\",\"150\",\"200\",\"250\",\"300\"]]\n",
    "reshape_pt_graph_2=reshape_pt_graph[[\"Station-Point\",\"DATE\",\"10\",\"20\",\"30\",\"50\",\"75\",\"100\",\"125\",\"150\",\"200\",\"250\",\"300\"]]\n",
    "reshape_pt_graph_3=reshape_pt_graph_2[(reshape_pt_graph_2[\"Station-Point\"]=='104-07')|(reshape_pt_graph_2[\"Station-Point\"]=='104-08')|(reshape_pt_graph_2[\"Station-Point\"]=='104-09')|(reshape_pt_graph_2[\"Station-Point\"]=='104-10')|(reshape_pt_graph_2[\"Station-Point\"]=='104-11')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_pt_graph_3=reshape_pt_graph_3[reshape_pt_graph_3[\"DATE\"].str.contains('2011-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_ocean_gan_df_2=reshape_pt_graph_3[[\"10\",\"20\",\"30\",\"50\",\"75\",\"100\",\"125\",\"150\",\"200\",\"250\",\"300\"]]\n",
    "org_104_ocean_df_list=reshape_ocean_gan_df_2.values.tolist()\n",
    "for org_temp in org_104_ocean_df_list:\n",
    "    print(org_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-tower",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_pt_graph[reshape_pt_graph[\"DATE\"].str.contains('2011-02')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-layout",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "r1 = LinearRegression()\n",
    "r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n",
    "X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n",
    "y = np.array([2, 6, 12, 20, 30, 42])\n",
    "er = VotingRegressor([('lr', r1), ('rf', r2)])\n",
    "print(er.fit(X, y).predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "# example of saving sub-models for later use in a stacking ensemble\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from os import makedirs\n",
    " \n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy, epochs=500, verbose=0)\n",
    "\treturn model\n",
    " \n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# create directory for models\n",
    "makedirs('models')\n",
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "\t# fit model\n",
    "\tmodel = fit_model(trainX, trainy)\n",
    "\t# save model\n",
    "\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\tmodel.save(filename)\n",
    "\tprint('>Saved %s' % filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-gathering",
   "metadata": {},
   "source": [
    "# Ensemble for Ocean Temperature Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "# example of saving sub-models for later use in a stacking ensemble\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from os import makedirs\n",
    "\n",
    "\n",
    "#Base Code for the predicion of ocean temperature profile\n",
    "#inputs = Input((3,5,2))\n",
    "#    conv = Conv2D(filters=1, \n",
    "#                  strides=1, \n",
    "#                 padding='same', \n",
    "#                  activation='relu',\n",
    "#                  kernel_size=2, \n",
    "#                  kernel_initializer='ones', \n",
    "#                  bias_initializer='zeros', )(inputs)\n",
    "#    model = Model(inputs,conv)\n",
    "#    model.compile (optimizer='adam',loss='mse')\n",
    "#    train_y=globals()['reshape_sub_'+str(depth)]\n",
    "#    print(train_y)\n",
    "#    model.fit(img_profile,train_y,epochs=7000,verbose=1)\n",
    "#    globals()['predicted_value'+str(depth)]=model.predict(img_profile)\n",
    "\n",
    " \n",
    "# fit model on dataset\n",
    "def fit_model(img_profile, reshape_sub_10):\n",
    "\t# define model\n",
    "#\tmodel = Sequential()\n",
    "#\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "#\tmodel.add(Dense(3, activation='softmax'))\n",
    "#\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    inputs = Input((3,5,2))\n",
    "    conv = Conv2D(filters=1, \n",
    "                strides=1, \n",
    "                 padding='same', \n",
    "                  activation='relu',\n",
    "                  kernel_size=2, \n",
    "                  kernel_initializer='ones', \n",
    "                  bias_initializer='zeros', )(inputs)\n",
    "    model = Model(inputs,conv)\n",
    "    model.compile (optimizer='adam',loss='mse')\n",
    "#    model.compile (optimizer='adam',loss='mse')\n",
    "\t# fit model\n",
    "#\tmodel.fit(trainX, trainy, epochs=500, verbose=0)\n",
    "    model.fit(img_profile,reshape_sub_10,epochs=7000,verbose=1)\n",
    "    return model\n",
    " \n",
    "# generate 2d classification dataset\n",
    "#X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "#y = to_categorical(y)\n",
    "# split into train and test\n",
    "#n_train = 100\n",
    "#trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "#trainy, testy = y[:n_train], y[n_train:]\n",
    "#print(trainX.shape, testX.shape)\n",
    "\n",
    "\n",
    "# create directory for models\n",
    "makedirs('models')\n",
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "\t# fit model\n",
    "\tmodel = fit_model(img_profile, reshape_sub_10)\n",
    "\t# save model\n",
    "\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\tmodel.save(filename)\n",
    "\tprint('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_sub_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked generalization with neural net meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from numpy import argmax\n",
    " \n",
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "    #all_models = list()\n",
    "    all_models = []\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    " \n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(3, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "    plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "    inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(X, inputy_enc, epochs=300, verbose=0)\n",
    " \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(X, verbose=0)\n",
    " \n",
    "# generate 2d classification dataset\n",
    "#X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 5\n",
    "trainX, testX = img_profile[:n_train, :, :, :], img_profile[n_train:, :, :, :]\n",
    "trainy, testy = reshape_sub_10[:n_train,:,:], reshape_sub_10[n_train:,:,:]\n",
    "\n",
    "print(testy.shape)\n",
    "\n",
    "print(trainX.shape, testX.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, testX, testy)\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, testX)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-question",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare hard voting to standalone classifiers\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "\treturn X, y\n",
    " \n",
    "# get a voting ensemble of models\n",
    "def get_voting():\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\tmodels.append(('knn1', KNeighborsClassifier(n_neighbors=1)))\n",
    "\tmodels.append(('knn3', KNeighborsClassifier(n_neighbors=3)))\n",
    "\tmodels.append(('knn5', KNeighborsClassifier(n_neighbors=5)))\n",
    "\tmodels.append(('knn7', KNeighborsClassifier(n_neighbors=7)))\n",
    "\tmodels.append(('knn9', KNeighborsClassifier(n_neighbors=9)))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\treturn ensemble\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tmodels['knn1'] = KNeighborsClassifier(n_neighbors=1)\n",
    "\tmodels['knn3'] = KNeighborsClassifier(n_neighbors=3)\n",
    "\tmodels['knn5'] = KNeighborsClassifier(n_neighbors=5)\n",
    "\tmodels['knn7'] = KNeighborsClassifier(n_neighbors=7)\n",
    "\tmodels['knn9'] = KNeighborsClassifier(n_neighbors=9)\n",
    "\tmodels['hard_voting'] = get_voting()\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-pasta",
   "metadata": {},
   "source": [
    "# Blending of Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare standalone models for binary classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "    n_redundant=5, random_state=1)\n",
    "    return X, y\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['knn'] = KNeighborsClassifier()\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    return models\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    # summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-yield",
   "metadata": {},
   "source": [
    "# Stacking of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ensemble to each baseline classifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    #define the base models\n",
    "    level0=list()\n",
    "    level0.append(('lr',LogisticRegression()))\n",
    "    level0.append(('knn',KNeighborsClassifier()))\n",
    "    level0.append(('cart',DecisionTreeClassifier()))\n",
    "    level0.append(('svm',SVC()))\n",
    "    level0.append(('bayes',GaussianNB()))\n",
    "    #define meta learner model\n",
    "    level1=LogisticRegression()\n",
    "    #define the stacking ensemble\n",
    "    model=StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of models to evaluae\n",
    "def get_models():\n",
    "    models=dict()\n",
    "    models['lr']=LogisticRegression()\n",
    "    models['knn']=KNeighborsClassifier()\n",
    "    models['cart']=DecisionTreeClassifier()\n",
    "    models['svm']=SVC()\n",
    "    models['bayes']=GaussianNB()\n",
    "    models['stacking']=get_stacking()\n",
    "    return models\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluatioin precedure\n",
    "    cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y, scoring='accuracy',cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y=get_dataset()\n",
    "models=get_models()\n",
    "results,names=list(),list()\n",
    "for name, model in models.items():\n",
    "    #evaluate the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #score the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    #summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores),std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-queen",
   "metadata": {},
   "source": [
    "# Stacking of Ensemble for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare machine learning models for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dataset\n",
    "def get_dataset():\n",
    "    X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models=dict()\n",
    "    models['knn']=KNeighborsRegressor()\n",
    "    models['cart']=DecisionTreeRegressor()\n",
    "    models['svm']=SVR()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluation procedure\n",
    "    cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y=get_dataset()\n",
    "#get the models to evaluate\n",
    "models=get_models()\n",
    "#evalute the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evalute the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    #summarize the performance alog the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ensemble to each standalone models for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dataset\n",
    "def get_dataset():\n",
    "    X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    #define the base models\n",
    "    level0=list()\n",
    "    level0.append(('knn',KNeighborsRegressor()))\n",
    "    level0.append(('cart',DecisionTreeRegressor()))\n",
    "    level0.append(('svm',SVR()))\n",
    "    #define meta learner model\n",
    "    level1=LinearRegression()\n",
    "    #define the stacking ensemble\n",
    "    model=StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    "\n",
    "#get a list of models to evaluate\n",
    "def get_models():\n",
    "    models=dict()\n",
    "    models['knn']=KNeighborsRegressor()\n",
    "    models['cart']=DecisionTreeRegressor()\n",
    "    models['svm']=SVR()\n",
    "    models['stacking']=get_stacking()\n",
    "    return models\n",
    "\n",
    "#evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    #define the evaluation procedure\n",
    "    cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n",
    "    #evaluate the model and collect the results\n",
    "    scores=cross_val_score(model, X, y,scoring='neg_mean_absolute_error',cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y=get_dataset()\n",
    "#get the model to evaluate\n",
    "models=get_models()\n",
    "#evaluate the models and store results\n",
    "results, names=list(), list()\n",
    "for name, model in models.items():\n",
    "    #evaluate the model\n",
    "    scores=evaluate_model(model, X, y)\n",
    "    #store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    #summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "#plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacking ensemble\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y=make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\n",
    "#define the base models\n",
    "level0=list()\n",
    "level0.append(('knn',KNeighborsRegressor()))\n",
    "level0.append(('cart',DecisionTreeRegressor()))\n",
    "level0.append(('svm',SVR()))\n",
    "\n",
    "#define meta learner model\n",
    "level1=LinearRegression()\n",
    "#define the stacking ensemble\n",
    "model=StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "#make a prediction for one example\n",
    "row = [0.59332206, -0.56637507, 1.34808718, -0.57054047, -0.72480487, 1.05648449,\n",
    "0.77744852, 0.07361796, 0.88398267, 2.02843157, 1.01902732, 0.11227799, 0.94218853,\n",
    "0.26741783, 0.91458143, -0.72759572, 1.08842814, -0.61450942, -0.69387293, 1.69169009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=model.predict([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize prediction\n",
    "print('Prediced Value: %.3f' %(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-differential",
   "metadata": {},
   "source": [
    "# Example of Chained Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluating chained multioutput regression with an SVM model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "X, y=make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=2, random_state=1, noise=0.5)\n",
    "\n",
    "#define base model\n",
    "model=LinearSVR()\n",
    "#define the chained multioutput wrapper model\n",
    "wrapper=RegressorChain(model)\n",
    "\n",
    "#define the evaluation procedure\n",
    "cv=RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#evaluate the model and collect the scores\n",
    "n_scores=cross_val_score(wrapper, X, y, scoring='neg_mean_absolute_error',cv=cv,n_jobs=-1)\n",
    "\n",
    "#summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores),std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of making a prediction with the chained multioutput regression model\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.svm import LinearSVR\n",
    "#define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=2, random_state=1, noise=0.5)\n",
    "\n",
    "#define base model\n",
    "model=LinearSVR()\n",
    "\n",
    "#define the chained multioutput wrapper model\n",
    "wrapper=RegressorChain(model)\n",
    "\n",
    "#fit the model on the whole dataset\n",
    "wrapper.fit(X,y)\n",
    "\n",
    "#make a single prediction\n",
    "row = [0.21947749, 0.32948997, 0.81560036, 0.440956, -0.0606303, -0.29257894, -0.2820059,\n",
    "-0.00290545, 0.96402263, 0.04992249]\n",
    "\n",
    "yhat=wrapper.predict([row])\n",
    "\n",
    "#summarize the prediction\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-corporation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
