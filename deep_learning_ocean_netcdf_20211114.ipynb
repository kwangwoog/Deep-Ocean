{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from netCDF4 import Dataset, num2date\n",
    "import pandas as pd\n",
    "#from matplotlib.cbook import dedent\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "## Aftre install there is some error related with dedent\n",
    "## pip install -U matplotlib==3.2 and restart jupyter notebook &\n",
    "print (np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_merge = xr.open_mfdataset('/gpu_deep/Deep_Ocean/AVHRR/avhrr-only/avhrr-only/199303/avhrr-only-v2.*.nc',combine = 'by_coords', concat_dim=\"time\")\n",
    "sst_merge.to_netcdf('avhrr-month-199303.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "PATH='/gpu_deep/Deep_Ocean/AVHRR/avhrr-only/avhrr-only/'\n",
    "FILE='/avhrr-only-v2.*.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range (1982,2018):\n",
    "    for mo in month:\n",
    "       input=(PATH+str(y)+mo+FILE)\n",
    "       output_file='sst_merge_month_'+str(y)+mo\n",
    "       print(input)\n",
    "       print(output_file+'.nc')\n",
    "       sst_merge = xr.open_mfdataset(input,combine = 'by_coords', concat_dim=\"time\")\n",
    "       sst_merge.to_netcdf(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sst_nc_file ='D:\\\\Deep-Learning\\\\avhrr-month-199303.nc'\n",
    "#sst_dataset=xr.open_dataset(sst_nc_file)\n",
    "#sst_dataset\n",
    "#dataset_sst=sst_dataset.sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_sst_lp=sst_dataset.sst.sel(lon=114.125,lat=14.125, method='nearest',time='1993-03-01')\n",
    "#dataset_sst_lp\n",
    "#dataset_sst_lp.plot()\n",
    "month=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "for year in range (1982,1984):\n",
    "        for mo in month:\n",
    "            sst_nc_file ='sst_merge_month_'+str(year)+mo\n",
    "            print(sst_nc_file)\n",
    "            sst_dataset=xr.open_dataset(sst_nc_file)\n",
    "            #sst_dataset\n",
    "            dataset_sst=sst_dataset.sst\n",
    "            dataset_sst_lp = sst_dataset.sst.sel(lon=slice(125, 140), lat=slice(30, 45))\n",
    "            output_file='sst_east_sea_month_'+str(year)+mo+'.nc'\n",
    "            print(output_file)\n",
    "            dataset_sst_lp.to_netcdf(output_file)\n",
    "            #dataset_sst_lp.plot()\n",
    "        dataset_sst_lp.plot(col=\"time\",col_wrap=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst=dataset_sst_lp.to_dataframe()\n",
    "np_sst=df_sst.to_numpy()\n",
    "print(np_sst.shape)\n",
    "print(np_sst)\n",
    "#sst_numpy_array = np.stack([data_array.values for data_array in df_sst['sst'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_sst_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sst_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst.shape\n",
    "#df_array=df_sst.to_numpy()\n",
    "#df_sst_drop_na=df_sst.dropna\n",
    "#df_sst_drop_na\n",
    "df_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sst.to_csv('sst.csv')\n",
    "df_sst=pd.read_csv('sst.csv')\n",
    "df_sst[['lat','lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip (df_sst['lon'], df_sst['lat']):\n",
    "  print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst_dataset.sst.sel(time='1983-12-31').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['latitude','longitude']]\n",
    "print(df_station_point)\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<38) & (df_station_point['longitude']>130)]\n",
    "\n",
    "print(df_station_point_38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [0, 1, 2]\n",
    "num_ls=[num for num in lst if num != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TF to use only the CPU\n",
    "\n",
    "#plot obs station\n",
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "\n",
    "print(df_station_point)\n",
    "print(df_station_point[['latitude']])\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "\n",
    "lat_ls=df_station_point_38[['latitude']].values.flatten()\n",
    "lon_ls=df_station_point_38[['longitude']].values.flatten()\n",
    "\n",
    "\n",
    "# Read Temperature Profile\n",
    "\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "col_names=['AREA','Station','Point','Station-Point','Latitude','Longitude','DATE','DEPTH','TEMPERATURE','TEMP/QC','SALINITY','SALINITY/QC','DISOLVED-OXYGEN','DISOLVED-OXYGEN/QC','QC Level','PHOROSPATE',' nitrite nitrogen','nitric acid nitrogen','silicic acid silicon','pH','Transparency','Pressure','Ship']\n",
    "\n",
    "path = '/gpu_deep/Deep_Ocean/obs_east/obs_*_east.xls'\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "files=glob.glob(path)\n",
    "for file in sorted(files):\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file[30:43])\n",
    "        Name=file[30:43]\n",
    "        #print(Name)\n",
    "        statement='/gpu_deep/Deep_Ocean/obs_east/'+ Name + '.xls'\n",
    "        print(statement)\n",
    "#        pd.read_excel('/content/drive/My Drive/Deep_Ocean/obs_east/obs_1965_east.xls', header=1,names=col_names)\n",
    "        df=df.append(pd.read_excel(statement, header=1,names=col_names))\n",
    "\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<39) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<132)]\n",
    "\n",
    "print(df_station_point_38)\n",
    "\n",
    "df_station_point=df_station_point_38['Point'].to_list()\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "df_38=pd.DataFrame()\n",
    "\n",
    "print(\"------------------------DF_STATION_POINT_38----------------------------------------------\")\n",
    "for i in df_station_point:\n",
    "    #print(df[df[\"Station-Point\"]==i])\n",
    "    print(i)\n",
    "    df_38=df_38.append(df[df[\"Station-Point\"]==i])\n",
    "\n",
    "#print(df)\n",
    "temp=df_38\n",
    "print(temp)\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#ocean_df=temp[['DATE','Station','Point','Station-Point','Latitude','Longitude','DEPTH','TEMPERATURE','SALINITY']]\n",
    "ocean_df=temp[['DATE','Station-Point','DEPTH','TEMPERATURE']]\n",
    "ocean_df['DATE']=pd.to_datetime(ocean_df['DATE'],format='%Y-%m-%d %H:%M')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m')\n",
    "ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#select Feb\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02|-03|-04|-06\")]\n",
    "ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02-|-03-\")]\n",
    "\n",
    "print(ocean_df)\n",
    "\n",
    "\n",
    "#ocean_df.pivot(index=['DATE','Station-Point'],columns='DEPTH',values='TEMPERATURE')\n",
    "\n",
    "ocean_grouped = ocean_df[\"DATE\"].unique()\n",
    "\n",
    "#np.unique(ocean_df[['DATE', 'Station-Point']].values)\n",
    "\n",
    "#print(np.unique(ocean_df[['DATE', 'Station-Point']].values))\n",
    "\n",
    "\n",
    "\n",
    "##2중 Loop\n",
    "reshape_ocean_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "obs_station= ocean_df[\"Station-Point\"].unique()\n",
    "\n",
    "#depth_criteria=[0,10,25,50,100,200,250,300,400,500]\n",
    "\n",
    "depth_criteria=[0,10,20,30,50,75,100,125,150,200,250,300,400,500]\n",
    "print(obs_station)\n",
    "\n",
    "#DEPTH=ocean_df_temp['DEPTH'][ocean_df_temp[\"DATE\"]==i].to_list()\n",
    "\n",
    "\n",
    "#tempDf = pd.DataFrame(columns=['PRODUCT','CAT_ID','MARKET_ID'])\n",
    "#tempDf['PRODUCT'] = df['PRODUCT']\n",
    "#tempDf['CAT_ID'] = catid\n",
    "#tempDf['MARKET_ID'] = 13\n",
    "\n",
    "#finalDf = pd.concat([finalDf,tempDf])\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['DATE','Station-Point','Latitude','Longitude','0','10','20','30','50','75','100','125','150','200','250','300','400','500'])\n",
    "\n",
    "\n",
    "df_station_point=station_point[['Point','latitude','longitude']]\n",
    "\n",
    "df_station_point=pd.DataFrame(df_station_point)\n",
    "\n",
    "\n",
    "print(\"--------DF Station Point-------\")\n",
    "\n",
    "print(df_station_point)\n",
    "\n",
    "from datetime import date\n",
    "#f_date = date(2014, 7, 2)\n",
    "#l_date = date(2014, 7, 11)\n",
    "#delta = l_date - f_date\n",
    "#print(delta.days)\n",
    "\n",
    "for i in obs_station :\n",
    "        obs_station_date=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]>'1992-01-01') & (ocean_df[\"DATE\"]<'2019-01-01'), \"DATE\"].unique()\n",
    "        print(obs_station_date)\n",
    "        for j in obs_station_date :\n",
    "                    DEPTH=ocean_df['DEPTH'][(ocean_df[\"DATE\"]==j) & (ocean_df[\"Station-Point\"]==i)].to_list()\n",
    "                    DEPTH.sort()\n",
    "                    if DEPTH==depth_criteria:\n",
    "                        print(j,i)\n",
    "                        start_dt=date(int(j[0:4]),1,1)\n",
    "                        print(j[5:7], j[8:10])\n",
    "                        end_dt=date(int(j[0:4]),int(j[5:7]), int(j[8:10]))\n",
    "                        delta=end_dt-start_dt\n",
    "                        ssh_days=delta.days\n",
    "                        obs_lat=df_station_point.loc[(df_station_point['Point']==i),\"latitude\"].values\n",
    "                        obs_lon=df_station_point.loc[(df_station_point['Point']==i),\"longitude\"].values\n",
    "                        print(\"LAT,LON:\",obs_lat,obs_lon)\n",
    "                        #sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_adt_east_sea_point_'+str(j[0:4])+'.nc'\n",
    "                        print(sst_nc_file)\n",
    "                        ssh_dataset=xr.open_dataset(sst_nc_file)\n",
    "                        print(ssh_dataset)\n",
    "                        #dataset_ssh=ssh_dataset.sla\n",
    "                        dataset_ssh=ssh_dataset.adt\n",
    "                        #dataset_ssh_lp = ssh_dataset.sla.sel(longitude=slice(120, 140), latitude=slice(30, 40))\n",
    "                        dataset_ssh_lp = ssh_dataset.adt.sel(time=j, latitude=obs_lat, longitude=obs_lon, method=\"nearest\")\n",
    "                        print(\"SSH:\", dataset_ssh_lp)\n",
    "                        #output_file='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(year)+'.nc'\n",
    "                        #print(output_file)\n",
    "                        #dataset_ssh_lp.to_netcdf(output_file)                                                \n",
    "                        temp_df[\"DATE\"]=j\n",
    "                        temp_df[\"Station-Point\"]=i\n",
    "                        temp_df[\"Latitude\"]=obs_lat\n",
    "                        temp_df[\"Longitude\"]=obs_lon\n",
    "                        temp_df[\"SSH\"]=dataset_ssh_lp.values\n",
    "                        temp_df[\"0\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==0),\"TEMPERATURE\"].values \n",
    "                        temp_df[\"10\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==10),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"20\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==20),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"30\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==30),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"50\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==50),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"75\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==75),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"100\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==100),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"125\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==125),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"150\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==150),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"200\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==200),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"250\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==250),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"300\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==300),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"400\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==400),\"TEMPERATURE\"].values\n",
    "                        temp_df[\"500\"]=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==500),\"TEMPERATURE\"].values\n",
    "                        print(temp_df)\n",
    "                        reshape_ocean_df=pd.concat([reshape_ocean_df,temp_df])\n",
    "                        \n",
    "print(reshape_ocean_df[reshape_ocean_df['DATE']!=reshape_ocean_df['DATE']])\n",
    "\n",
    "reshape_ocean_df.reset_index()\n",
    "                        \n",
    "                        \n",
    "#                    for k in depth_criteria:\n",
    "#                        if DEPTH==depth_criteria:\n",
    "#                            temp_depth=ocean_df.loc[(ocean_df[\"Station-Point\"]==i) & (ocean_df[\"DATE\"]==j) & (ocean_df[\"DEPTH\"]==k)]\n",
    "#                            print(temp_depth)\n",
    "#                            #print(i,j,k,temp_depth)\n",
    "                    \n",
    "\n",
    "print(reshape_ocean_df)\n",
    "\n",
    "reshape_ocean_df.to_csv('reshape_ocean_df.csv',index=False)\n",
    "\n",
    "reshape_ocean_df=pd.read_csv('reshape_ocean_df.csv')  \n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "print(reshape_ocean_df.drop([0,0]))\n",
    "\n",
    "\n",
    "reshape_ocean_df_1993=reshape_ocean_df[reshape_ocean_df[\"DATE\"]>'1993-01-01']\n",
    "\n",
    "reshape_ocean_df_1993.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print(reshape_ocean_df_1993)\n",
    "\n",
    "#ls_ocean_dt=ocean_df['DATE'].unique()\n",
    "ls_station_df=ocean_df['Station-Point'].unique()\n",
    "print(ls_station_df.shape)\n",
    "obs_station_lst=ls_station_df.tolist()\n",
    "print(obs_station_lst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure(figsize=(64, 64))\n",
    "#m = Basemap(projection='lcc', resolution='f',\n",
    "#            width=1E6, height=1E6, \n",
    "#            lat_0=37, lon_0=132,)\n",
    "#m.etopo(scale=0.5, alpha=0.5)\n",
    "m = Basemap(projection='lcc', resolution='f',\n",
    "          lat_0=37, lon_0=132, \n",
    "          llcrnrlon=120.25, llcrnrlat=30.0,\n",
    "          urcrnrlon=140.25, urcrnrlat=40.75)\n",
    "m.etopo(scale=3.0, alpha=2.0)\n",
    "\n",
    "\n",
    "lons, lats = m(lon_ls, lat_ls)\n",
    "#m.scatter(lons, lats, marker = 'o', color='r', zorder=5)\n",
    "m.plot(lons,lats,'ro',markersize=18)\n",
    "plt.show()\n",
    "\n",
    "# Map (long, lat) to (x, y) for plotting\n",
    "#x, y = m(132, 37)\n",
    "#plt.plot(x, y, 'ok', markersize=10)\n",
    "#plt.text(x, y, ' Seoul', fontsize=12);\n",
    "\n",
    "#m.scatter(lon, lat, marker = 'o', color='r', zorder=5)\n",
    "\n",
    "#m.scatter(lon, lat, latlon=True,\n",
    "#          c=np.log10(population), s=area,\n",
    "#          cmap='Reds', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define data\n",
    "\n",
    "Oxygen = [ 0.1 , 0.5, 1, 10, 15, 20, 15, 10, 1, 0.5, 0.5]\n",
    "Depth  = [ 0,     1,  2,  4,  8, 10, 12, 14, 16, 20, 40 ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(Oxygen, Depth, 'go--')\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_ylabel('depth')\n",
    "ax.set_ylim(50, 0)\n",
    "ax.set_xlim(0, 25)\n",
    "ax.set_xlabel('Oxygen level [ppm]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####SKleran\n",
    "reshape_ocean_df_1993\n",
    "from sklearn.model_selection import train_test_split\n",
    "ocean_train, ocean_test = train_test_split(reshape_ocean_df_1993, test_size=0.2)\n",
    "\n",
    "ocean_train_surface_ssh=ocean_train[['0','SSH','10','20','30','50','75','100','125','150','200','250']]\n",
    "ocean_test_surface_ssh=ocean_test[['0','SSH','10','20','30','50','75','100','125','150','200','250']]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#ocean_train_surface_ssh\n",
    "ocean_test_surface_ssh\n",
    "\n",
    "X=ocean_train_surface_ssh[['0','SSH','10','20','30','50','75','100','125','150','200']]\n",
    "y=ocean_train_surface_ssh[['250']]\n",
    "\n",
    "X_test=ocean_test_surface_ssh[['0','SSH','10','20','30','50','75','100','125','150','200']]\n",
    "y_test=ocean_test_surface_ssh[['250']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a sample output\n",
    "print(X_train_scaled[0])\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape[1])\n",
    "\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train_scaled, y)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "y_pred = regr.predict(X_test_scaled)\n",
    "#scores = []\n",
    "#score = regr.score(X_test_scaled,y_test)\n",
    "#print(score)\n",
    "\n",
    "#print(y_test,y_pred)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_test,y_pred))\n",
    "\n",
    "test_rmse=np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"rmse=\", test_rmse)\n",
    "\n",
    "#print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<H2> Install SDV\n",
    "# <div class=\"alert alert-info\">\n",
    "\n",
    "**Note**\n",
    "\n",
    "Notice that the model `fitting` process took care of transforming the\n",
    "different fields using the appropriate [Reversible Data\n",
    "Transforms](http://github.com/sdv-dev/RDT) to ensure that the data has a\n",
    "format that the underlying CTGANSynthesizer class can handle.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Generate synthetic data from the model\n",
    "\n",
    "Once the modeling has finished you are ready to generate new synthetic\n",
    "data by calling the `sample` method from your model passing the number\n",
    "of rows that we want to generate.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)\n",
    "import numpy as np\n",
    "from sdv.tabular import CTGAN\n",
    "model = CTGAN()\n",
    "model.fit(ocean_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ocean_train_data = model.sample(20000)\n",
    "new_ocean_train_data.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conditions=pd.DataFrame({\n",
    "    'Station-Point':['104-07', '104-08', '104-09', '104-10', '104-11', '104-12', '104-13', '105-07', '105-08', '105-09', '105-10', '105-11', '105-12', '105-13', '106-07', '106-08', '106-09', '106-10', '106-11', '107-07'],\n",
    "#'Station-Point':['104-07','104-08']\n",
    "})\n",
    "model.sample(conditions=conditions)\n",
    "\n",
    "conditions={\n",
    "    'Station-Point':'104-07',\n",
    "    'Latitude':37.0567,\n",
    "    'Longitude':130.0000\n",
    "}\n",
    "model.sample(1000,conditions=conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####SKleran (NO-SSH)\n",
    "reshape_ocean_df_1993\n",
    "from sklearn.model_selection import train_test_split\n",
    "ocean_train, ocean_test = train_test_split(reshape_ocean_df_1993, test_size=0.2)\n",
    "\n",
    "ocean_train_surface_ssh=ocean_train[['0','10','20','30','50','75','100','125','150','200','250']]\n",
    "ocean_test_surface_ssh=ocean_test[['0','10','20','30','50','75','100','125','150','200','250']]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#ocean_train_surface_ssh\n",
    "ocean_test_surface_ssh\n",
    "\n",
    "X=ocean_train_surface_ssh[['0','10','20','30','50','75','100','125','150','200']]\n",
    "y=ocean_train_surface_ssh[['250']]\n",
    "\n",
    "X_test=ocean_test_surface_ssh[['0','10','20','30','50','75','100','125','150','200']]\n",
    "y_test=ocean_test_surface_ssh[['250']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a sample output\n",
    "print(X_train_scaled[0])\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape[1])\n",
    "\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train_scaled, y)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "y_pred = regr.predict(X_test_scaled)\n",
    "#scores = []\n",
    "#score = regr.score(X_test_scaled,y_test)\n",
    "#print(score)\n",
    "\n",
    "#print(y_test,y_pred)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_test,y_pred))\n",
    "\n",
    "test_rmse=np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"rmse=\", test_rmse)\n",
    "\n",
    "#print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Deep Learning (SSH)\n",
    "reshape_ocean_df_1993\n",
    "from sklearn.model_selection import train_test_split\n",
    "ocean_train, ocean_test = train_test_split(reshape_ocean_df_1993, test_size=0.2)\n",
    "\n",
    "ocean_train_surface_ssh=ocean_train[['0','SSH','10','20','30','50','75','100','125','150','200','250']]\n",
    "ocean_test_surface_ssh=ocean_test[['0','SSH','10','20','30','50','75','100','125','150','200','250']]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#ocean_train_surface_ssh\n",
    "ocean_test_surface_ssh\n",
    "\n",
    "X=ocean_train_surface_ssh[['0','SSH','10','20','30','50','75','100','125','150','200']]\n",
    "y=ocean_train_surface_ssh[['250']]\n",
    "\n",
    "X_test=ocean_test_surface_ssh[['0','SSH','10','20','30','50','75','100','125','150','200']]\n",
    "y_test=ocean_test_surface_ssh[['250']]\n",
    "\n",
    "# Regression Example With Boston Dataset: Baseline\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "# load dataset\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(11, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(22, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X_test, y_test, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "########################################################################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a sample output\n",
    "print(X_train_scaled[0])\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape[1])\n",
    "\n",
    "# array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
    "#      -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
    "#       1.14850044,  0.44807713,  0.8252202 ])\n",
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(11, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "#model.add(layers.Dense(256, activation='relu'))\n",
    "#model.add(layers.Dense(512, activation='relu'))\n",
    "#model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "# output layer\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=400)\n",
    "# we get a sample data (the first 2 inputs from the training data)\n",
    "\n",
    "#to_predict = X_train_scaled\n",
    "to_predict = X_test_scaled\n",
    "\n",
    "# we call the predict method\n",
    "predictions = model.predict(to_predict)\n",
    "# print the predictions\n",
    "print(predictions)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_test,model.predict(to_predict)))\n",
    "\n",
    "test_rmse=np.sqrt(mean_squared_error(y_test, model.predict(to_predict)))\n",
    "\n",
    "print(\"rmse=\", test_rmse)\n",
    "\n",
    "# output\n",
    "# array([[13.272537], [39.808475]], dtype=float32)\n",
    "# print the real values\n",
    "# array([15.2, 42.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Deep Learning (SSH)\n",
    "reshape_ocean_df_1993\n",
    "from sklearn.model_selection import train_test_split\n",
    "ocean_train, ocean_test = train_test_split(reshape_ocean_df_1993, test_size=0.2)\n",
    "\n",
    "ocean_train_surface_ssh=ocean_train[['0','SSH','10','20','30','50','75','100','125','150','200','250']]\n",
    "ocean_test_surface_ssh=ocean_test[['0','SSH','10','20','30','50','75','100','125','150','200','250']]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#ocean_train_surface_ssh\n",
    "ocean_test_surface_ssh\n",
    "\n",
    "X=ocean_train_surface_ssh[['0','SSH','10','20','30','50','75']]\n",
    "y=ocean_train_surface_ssh[['100','125','150','200','250']]\n",
    "\n",
    "X_test=ocean_test_surface_ssh[['0','SSH','10','20','30','50','75']]\n",
    "y_test=ocean_test_surface_ssh[['100','125','150','200','250']]\n",
    "\n",
    "# Regression Example With Boston Dataset: Baseline\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "# load dataset\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(22, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(5, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X_test, y_test, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "########################################################################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a sample output\n",
    "print(X_train_scaled[0])\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape[1])\n",
    "\n",
    "# array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
    "#      -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
    "#       1.14850044,  0.44807713,  0.8252202 ])\n",
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(7, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "# output layer\n",
    "model.add(layers.Dense(5))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=400)\n",
    "# we get a sample data (the first 2 inputs from the training data)\n",
    "\n",
    "#to_predict = X_train_scaled\n",
    "to_predict = X_test_scaled\n",
    "\n",
    "# we call the predict method\n",
    "predictions = model.predict(to_predict)\n",
    "# print the predictions\n",
    "print(predictions)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_test,model.predict(to_predict)))\n",
    "\n",
    "test_rmse=np.sqrt(mean_squared_error(y_test, model.predict(to_predict)))\n",
    "\n",
    "print(\"rmse=\", test_rmse)\n",
    "\n",
    "# output\n",
    "# array([[13.272537], [39.808475]], dtype=float32)\n",
    "# print the real values\n",
    "# array([15.2, 42.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "X=array(X_train)\n",
    "y=array(y_train)\n",
    "\n",
    "X=X.reshape((X.shape[0],X.shape[1],1))\n",
    "print(\"X.shape[0]\",X.shape[0])\n",
    "print(\"X.shape[1]\",X.shape[1])\n",
    "\n",
    "print(\"X.shape\",X.shape)\n",
    "print(\"Y.shape\",y.shape)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='sigmoid', input_shape=(7,1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='sigmoid'))\n",
    "model.add(Dense(units=64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "X_input=array(X_test)\n",
    "y_input=array(y_test)\n",
    "X_input=X_input.reshape(X_input.shape[0],X_input.shape[1],1)\n",
    "print(\"X_input.shape[0]\",X_input.shape[0])\n",
    "print(\"X_input.shape[1]\",X_input.shape[1])\n",
    "print(\"X_input.shape\",X_input.shape)\n",
    "print(\"Y_input.shape\",y_input.shape)\n",
    "\n",
    "\n",
    "y_hat=model.predict(X_input,verbose=1)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_input,y_hat))\n",
    "\n",
    "test_rmse=np.sqrt(mean_squared_error(y_input, y_hat))\n",
    "\n",
    "print(\"rmse=\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Deep Learning (No SSH)\n",
    "reshape_ocean_df_1993\n",
    "from sklearn.model_selection import train_test_split\n",
    "ocean_train, ocean_test = train_test_split(reshape_ocean_df_1993, test_size=0.2)\n",
    "\n",
    "ocean_train_surface_ssh=ocean_train[['0','10','20','30','50','75','100','125','150','200','250']]\n",
    "ocean_test_surface_ssh=ocean_test[['0','10','20','30','50','75','100','125','150','200','250']]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#ocean_train_surface_ssh\n",
    "ocean_test_surface_ssh\n",
    "\n",
    "X=ocean_train_surface_ssh[['0','10','20','30','50','75','100','125','150','200']]\n",
    "y=ocean_train_surface_ssh[['250']]\n",
    "\n",
    "X_test=ocean_test_surface_ssh[['0','10','20','30','50','75','100','125','150','200']]\n",
    "y_test=ocean_test_surface_ssh[['250']]\n",
    "\n",
    "# Regression Example With Boston Dataset: Baseline\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "# load dataset\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(22, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X_test, y_test, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "########################################################################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train=X\n",
    "y_train=y\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a sample output\n",
    "print(X_train_scaled[0])\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape[1])\n",
    "\n",
    "# array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
    "#      -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
    "#       1.14850044,  0.44807713,  0.8252202 ])\n",
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "# output layer\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=400)\n",
    "# we get a sample data (the first 2 inputs from the training data)\n",
    "\n",
    "#to_predict = X_train_scaled\n",
    "to_predict = X_test_scaled\n",
    "\n",
    "# we call the predict method\n",
    "predictions = model.predict(to_predict)\n",
    "# print the predictions\n",
    "print(predictions)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_test,model.predict(to_predict)))\n",
    "\n",
    "test_rmse=np.sqrt(mean_squared_error(y_test, model.predict(to_predict)))\n",
    "\n",
    "print(\"rmse=\", test_rmse)\n",
    "\n",
    "# output\n",
    "# array([[13.272537], [39.808475]], dtype=float32)\n",
    "# print the real values\n",
    "# array([15.2, 42.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "WIDTH=200\n",
    "HEIGHT=200\n",
    "\n",
    "plt.figure(figsize=(WIDTH, HEIGHT))\n",
    "map = Basemap(projection='merc', lat_0 = 37, lon_0 = 135,\n",
    "    resolution = 'h', area_thresh = 0.1,\n",
    "    llcrnrlon=120.25, llcrnrlat=30.0,\n",
    "    urcrnrlon=140.25, urcrnrlat=40.75)\n",
    "lat=station_point[['latitude']].values.flatten()\n",
    "lon=station_point[['longitude']].values.flatten()\n",
    "\n",
    "df_station_point_38=station_point[(df_station_point['latitude']<38) & (df_station_point['longitude']>129.8) & (df_station_point['longitude']<131)]\n",
    "\n",
    "print(df_station_point_38['Point'])\n",
    "\n",
    "lat_38=df_station_point_38[['latitude']].values.flatten()\n",
    "lon_38=df_station_point_38[['longitude']].values.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'gray')\n",
    "map.drawmapboundary()\n",
    "map.drawcoastlines()\n",
    "x,y = map(lon_38, lat_38)\n",
    "map.plot(x, y, 'bo', markersize=40)\n",
    "#map.bluemarble()\n",
    "map.etopo()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sst_point=(df_sst.dropna())[['lat','lon','sst']]\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point[['latitude','longitude']]\n",
    "print(df_station_point)\n",
    "#dataset_sst_lp=sst_dataset_lp.sst.sel(lon=114.125,lat=14.125, method='nearest',time='1993-03-01')\n",
    "i=1\n",
    "#for x, y in zip (df_sst['lon'], df_sst['lat']):\n",
    "for x, y in zip (df_station_point['longitude'], df_station_point['latitude']):\n",
    "    dataset_sst_1983=sst_dataset.sst.sel(lon=x,lat=y, method='nearest',time='1983-12-31')\n",
    "    #print(dataset_sst_1983.values)\n",
    "    #np_sst=np.stack(dataset_sst_1983)\n",
    "    #dataset_sst_1983.to_dataframe()\n",
    "    print(x,y,dataset_sst_1983.values)\n",
    "    #print(type(dataset_sst_1983))\n",
    "    \n",
    "dataset_sst_1983.values\n",
    "dataset_sst_1983.shape\n",
    "dataset_sst_1983.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range (1993,2019):\n",
    "            sst_nc_file ='/gpu_deep/Deep_Ocean/CMEMS/cmems_nwp_ssh_'+str(year)+'.nc'\n",
    "            print(sst_nc_file)\n",
    "            ssh_dataset=xr.open_dataset(sst_nc_file)\n",
    "            print(ssh_dataset)\n",
    "            #dataset_ssh=ssh_dataset.sla\n",
    "            dataset_ssh=ssh_dataset.adt\n",
    "            #dataset_ssh_lp = ssh_dataset.sla.sel(longitude=slice(120, 140), latitude=slice(30, 40))\n",
    "            dataset_ssh_lp = ssh_dataset.adt.sel(longitude=slice(120, 140), latitude=slice(30, 40))\n",
    "            #output_file='/gpu_deep/Deep_Ocean/CMEMS/cmems_ssh_east_sea_point_'+str(year)+'.nc'\n",
    "            output_file='/gpu_deep/Deep_Ocean/CMEMS/cmems_adt_east_sea_point_'+str(year)+'.nc'\n",
    "            print(output_file)\n",
    "            dataset_ssh_lp.to_netcdf(output_file)\n",
    "            #dataset_sst_lp.plot()\n",
    "\n",
    "dataset_ssh_lp.plot(col=\"time\",col_wrap=6)\n",
    "dataset_ssh_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH=20\n",
    "HEIGHT=16\n",
    "\n",
    "plt.figure(figsize=(WIDTH, HEIGHT))\n",
    "print(dataset_ssh_lp)\n",
    "dataset_ssh_20181231=dataset_ssh_lp.isel(time=364)\n",
    "dataset_ssh_20181231.plot()\n",
    "dataset_ssh_20181231.to_dataframe()\n",
    "df_dataset_ssh_20181231=dataset_ssh_20181231.to_dataframe()\n",
    "df_dataset_ssh_20181231.shape\n",
    "\n",
    "\n",
    "##########  SSH Nearest (lat,long)\n",
    "#날짜와, 위경도 변수를 받아서 nearest 로 추출\n",
    "print(dataset_ssh_lp.sel(time='2018-01-01', latitude=30.120, longitude=120.126, method=\"nearest\"))\n",
    "print(dataset_ssh_lp.sel(time='2018-02-01', latitude=30.120, longitude=120.126, method=\"nearest\"))\n",
    "print(dataset_ssh_lp.sel(time='2018-03-01', latitude=30.120, longitude=120.126, method=\"nearest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추출지점(lat,lon), 추출날짜\n",
    "\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "col_names=['AREA','Station','Point','Station-Point','Latitude','Longitude','DATE','DEPTH','TEMPERATURE','TEMP/QC','SALINITY','SALINITY/QC','DISOLVED-OXYGEN','DISOLVED-OXYGEN/QC','QC Level','PHOROSPATE',' nitrite nitrogen','nitric acid nitrogen','silicic acid silicon','pH','Transparency','Pressure','Ship']\n",
    "\n",
    "path = '/gpu_deep/Deep_Ocean/obs_east/obs_*_east.xls'\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "files=glob.glob(path)\n",
    "for file in sorted(files):\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file[30:43])\n",
    "        Name=file[30:43]\n",
    "        #print(Name)\n",
    "        statement='/gpu_deep/Deep_Ocean/obs_east/'+ Name + '.xls'\n",
    "        print(statement)\n",
    "#        pd.read_excel('/content/drive/My Drive/Deep_Ocean/obs_east/obs_1965_east.xls', header=1,names=col_names)\n",
    "        df=df.append(pd.read_excel(statement, header=1,names=col_names))\n",
    "\n",
    "import pandas as pd\n",
    "col_names=['AREA','Station','Point','Station-Point','Latitude','Longitude','DATE','DEPTH','TEMPERATURE','TEMP/QC','SALINITY','SALINITY/QC','DISOLVED-OXYGEN','DISOLVED-OXYGEN/QC','QC Level','PHOROSPATE',' nitrite nitrogen','nitric acid nitrogen','silicic acid silicon','pH','Transparency','Pressure','Ship']\n",
    "data=pd.read_excel('/gpu_deep/Deep_Ocean/obs_east/obs_1965_east.xls', header=1,names=col_names)\n",
    "\n",
    "Filtered_data=data[[\"Station-Point\",\"Latitude\",\"Longitude\",\"DATE\",\"DEPTH\",\"TEMPERATURE\"]]\n",
    "\n",
    "Filtered_data\n",
    "#print(Filtered_data['DATE'].dtypes)\n",
    "\n",
    "#Filtered_data[Filtered_data[\"Station-Point\"]==\"102-06\"]\n",
    "\n",
    "station_point=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls',header=0)\n",
    "print(station_point)\n",
    "print(station_point.shape)\n",
    "\n",
    "df_station_point=station_point['Point'].to_list()\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "for i in df_station_point:\n",
    "    #print(df[df[\"Station-Point\"]==i])\n",
    "    df=df.append(df[df[\"Station-Point\"]==i])\n",
    "\n",
    "#print(df)\n",
    "temp=df\n",
    "print(temp)\n",
    "\n",
    "\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ocean_df=temp[['DATE','Station','Point','Station-Point','Latitude','Longitude','DEPTH','TEMPERATURE','SALINITY']]\n",
    "ocean_df['DATE']=pd.to_datetime(ocean_df['DATE'],format='%Y-%m-%d %H:%M')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "#ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m')\n",
    "ocean_df['DATE']=ocean_df['DATE'].dt.strftime('%Y-%m')\n",
    "\n",
    "temp=df[df[\"Station-Point\"]==\"107-04\"]\n",
    "\n",
    "#select Feb\n",
    "#ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02|-03|-04|-06\")]\n",
    "ocean_df=ocean_df[ocean_df['DATE'].str.contains(\"-02\")]\n",
    "\n",
    "print(ocean_df)\n",
    "\n",
    "ls_ocean_dt=ocean_df['DATE'].unique()\n",
    "ls_station_df=ocean_df['Station-Point'].unique()\n",
    "print(ls_station_df.shape)\n",
    "obs_station_lst=ls_station_df.tolist()\n",
    "print(obs_station_lst)\n",
    "\n",
    "tmp=ls_ocean_dt\n",
    "print(ls_station_df)\n",
    "print(tmp)\n",
    "depth_criteria=[0,10,20,30,50,75,100,125,150,200,250,300,400,500]\n",
    "depth_criteria.sort()\n",
    "obs_year_lst=tmp.tolist()\n",
    "\n",
    "#temp=df[df[\"Station-Point\"]==\"107-04\"]\n",
    "\n",
    "#People_List = [['Jon','Smith',21],['Mark','Brown',38],['Maria','Lee',42],['Jill','Jones',28],['Jack','Ford',55]]\n",
    "\n",
    "#df = DataFrame (ocean_df,columns=['Suface','Depth(10)','Depth(20)','Depth(30)','Depth(50)','Depth(75)','Depth(100)','Depth(125)','Depth(150)','Depth(200)','Depth(250)','Depth(300)','Depth(400)','Depth(500)'])\n",
    "#print (df)\n",
    "\n",
    "\n",
    "df = DataFrame (ocean_df,columns=['Suface','Depth(10)','Depth(20)','Depth(30)','Depth(50)','Depth(75)','Depth(100)','Depth(125)','Depth(150)','Depth(200)','Depth(250)','Depth(300)','Depth(400)','Depth(500)'])\n",
    "\n",
    "\n",
    "\n",
    "#spot_obs_station_lst=['107-04']\n",
    "print(spot_obs_station_lst)\n",
    "\n",
    "for s in obs_station_lst:\n",
    "    ocean_df_temp=ocean_df[ocean_df['Station-Point']==s]\n",
    "    #print(ocean_df_temp)\n",
    "    for i in obs_year_lst:\n",
    "      # print(ocean_df['DEPTH'][ocean_df[\"DATE\"]==i & ocean[\"DATE\"].substr[6:7]=='02'])\n",
    "      #print(i)\n",
    "      DEPTH=ocean_df_temp['DEPTH'][ocean_df_temp[\"DATE\"]==i].to_list()\n",
    "      TEMP=ocean_df_temp['TEMPERATURE'][ocean_df_temp[\"DATE\"]==i].to_list()  \n",
    "        #  df = DataFrame (TEMP,columns=['Suface','Depth(10)','Depth(20)','Depth(30)','Depth(50)','Depth(75)','Depth(100)','Depth(125)','Depth(150)','Depth(200)','Depth(250)','Depth(300)','Depth(400)','Depth(500)'])\n",
    "        #print (TEMP)\n",
    "        #print (DEPTH)\n",
    "        #SALINITY=ocean_df['SALINITY'][ocean_df[\"DATE\"]==i].to_list()\n",
    "      DEPTH.sort()\n",
    "      #print(DEPTH)\n",
    "      if DEPTH == depth_criteria:\n",
    "        #print(TEMP)\n",
    "        #NewTempList= [[x] for x in TEMP]\n",
    "        #print(NewTempList)\n",
    "        #np_array = np.array(TEMP)\n",
    "        #NewTempList = np.reshape(np_array, (1, 14))\n",
    "        #tempdf = DataFrame (columns=['Suface','Depth(10)','Depth(20)','Depth(30)','Depth(50)','Depth(75)','Depth(100)','Depth(125)','Depth(150)','Depth(200)','Depth(250)','Depth(300)','Depth(400)','Depth(500)'])\n",
    "        #print(tempdf)\n",
    "        #print('==================')\n",
    "        #df=df.append(tempdf)\n",
    "    \n",
    "        #df = DataFrame (TEMP,columns=['Suface','Depth(10)','Depth(20)','Depth(30)','Depth(50)','Depth(75)','Depth(100)','Depth(125)','Depth(150)','Depth(200)','Depth(250)','Depth(300)','Depth(400)','Depth(500)'])\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(TEMP, DEPTH, 'go--')\n",
    "        ax.xaxis.tick_top()\n",
    "        #ax.set_title(s+'---'+i)\n",
    "        ax.set_ylabel('depth')\n",
    "        ax.set_ylim(500, 0)\n",
    "        ax.set_xlim(0, 25)\n",
    "        ax.set_xlabel('Temp [C]')\n",
    "        plt.show()\n",
    "        imgname='/gpu_deep/Deep_Ocean/obs_east_figure/'+s+'-'+i+'.jpg'\n",
    "        fig.savefig(imgname,figsize=(5,5))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of converting an image with the Keras API\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "# load the image\n",
    "img = load_img('/gpu_deep/Deep_Ocean/obs_east_figure/107-08-1977-02.jpg')\n",
    "print(\"Orignal:\" ,type(img))\n",
    "\n",
    "obs_station_lst\n",
    "\n",
    "# convert to numpy array\n",
    "img_array = img_to_array(img)\n",
    "print(\"NumPy array info:\") \n",
    "print(type(img_array))    \n",
    "\n",
    "print(\"type:\",img_array.dtype)\n",
    "print(\"shape:\",img_array.shape)\n",
    "# convert back to image\n",
    "\n",
    "img_pil = array_to_img(img_array)\n",
    "print(\"converting NumPy array:\",type(img_pil))\n",
    "print(img_array)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "print(obs_station_lst)\n",
    "\n",
    "month=['02']\n",
    "PATH='/gpu_deep/Deep_Ocean/obs_east_figure/'\n",
    "FILE='.jpg'\n",
    "\n",
    "fig=plt.figure()\n",
    "\n",
    "for y in range (1982,2018):\n",
    "    for mo in month:\n",
    "        for obs in obs_station_lst:\n",
    "            input=(PATH+str(obs)+'-'+str(y)+'-'+mo+FILE)\n",
    "            file=pathlib.Path(input)\n",
    "            #file=pathlib.Path('/gpu_deep/Deep_Ocean/obs_east_figure/103-11-2008-02-28.jpg')\n",
    "            #print(input)\n",
    "            if file.exists ():\n",
    "                print (\"File exist\")\n",
    "                im = np.array(Image.open(input).convert('L'))\n",
    "                plt.imshow(im)\n",
    "                #print(im)\n",
    "                #print(input)\n",
    "           #im = np.array(Image.open('/gpu_deep/Deep_Ocean/obs_east_figure/107-08-1977-02.jpg').convert('L')) #you can pass multiple arguments\n",
    "\n",
    "plt.show()        \n",
    "#print(type(im))\n",
    "#print(im.shape)\n",
    "#plt.imshow(im)\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_1=dataset_ssh_20181231.sel(longitude=133.875,latitude=39.875).values\n",
    "values_2=dataset_ssh_20181231.sel(longitude=133.875,latitude=39.875).values\n",
    "type(values_1)\n",
    "values_1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_example_nc_file = '/gpu_deep/Deep_Ocean/CMEMS/cmes_ssh_east_sea_month_1993.nc'\n",
    "nc = Dataset(my_example_nc_file, mode='r')\n",
    "print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sla = nc.variables['sla'][:]\n",
    "time = nc.variables['time'][:]\n",
    "lon = nc.variables['longitude'][:]\n",
    "lat = nc.variables['latitude'][:]\n",
    "lon, lat = np.meshgrid(lon, lat)\n",
    "print(nc.variables.keys()) # get all variable name\n",
    "sla = nc.variables['sla'] # Sea Surface Level anomaly variable\n",
    "lon = nc.variables['longitude']\n",
    "lat = nc.variables['latitude']\n",
    "print(sla)\n",
    "print(lon)\n",
    "print(lat)\n",
    "for d in nc.dimensions.items():\n",
    "  print(d)\n",
    "\n",
    "#sla.dimensions\n",
    "\n",
    "#sla.shape\n",
    "#lon.dimensions\n",
    "#lat.dimensions\n",
    "\n",
    "#time.shape\n",
    "#sla\n",
    "print(lon[0])\n",
    "print(lon[2])\n",
    "print(lon[3])\n",
    "print(lat[1])\n",
    "print(lat[59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_map(data):\n",
    "    m = Basemap(projection='mill',llcrnrlat=30,urcrnrlat=50,\\\n",
    "            llcrnrlon=120,urcrnrlon=140,lat_ts=20,resolution='c')\n",
    "    x, y = m(lon, lat)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    m.drawcoastlines()\n",
    "    m.drawparallels(np.arange(-80.,81.,20.))\n",
    "    m.drawmeridians(np.arange(-180.,181.,20.))\n",
    "    m.drawmapboundary(fill_color='white')\n",
    "    m.contourf(x,y,data,levels=np.linspace(-25,30,56), extend=\"both\");\n",
    "    plt.colorbar( orientation='horizontal', pad=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_map(sla[364,:,:])\n",
    "sla[364,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sst_1983.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lat, lon in station_point[['latitude','longitude']]:\n",
    "for latitude, longitude in station_point[['latitude','longitude']].items():\n",
    "    #print(latitude, longitude)\n",
    "    print(\"{} : {}\".format(latitude, longitude))\n",
    "\n",
    "#station_point[['latitude','longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#month=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month=['03']\n",
    "for year in range (1982,1984):\n",
    "        for mo in month:\n",
    "            sst_nc_file ='sst_east_sea_month_'+str(year)+mo+'.nc'\n",
    "            #print(sst_nc_file)\n",
    "            sst_dataset=xr.open_dataset(sst_nc_file)\n",
    "            dataset_sst_east=sst_dataset.sst\n",
    "            #dataset_sst_lp.plot()\n",
    "            dataset_sst_east.plot(col=\"time\",col_wrap=6)\n",
    "        #dataset_sst_east.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " dataset_sst_east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from matplotlib.axes import Axes\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "GeoAxes._pcolormesh_patched = Axes.pcolormesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dataset_sst_east.isel(time=4).plot(\n",
    "    subplot_kws=dict(projection=ccrs.Orthographic(100, 20), facecolor=\"gray\"),\n",
    "    transform=ccrs.PlateCarree(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.axes.set_global()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.axes.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dataset_sst_east.isel(time=[0,1,2,3,4,5]).plot(\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    col=\"time\",\n",
    "    subplot_kws={\"projection\": ccrs.Orthographic(100, 20)},\n",
    ")\n",
    "for ax in p.axes.flat:\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "col_names=['Area','Point','Latitude','Longitude']\n",
    "data=pd.read_excel('/gpu_deep/Deep_Ocean/AVHRR/station_104_107_loc.xls', header=1,names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Latitude','Longitude']]\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "m = Basemap(projection='ortho', resolution=None, lat_0=35, lon_0=130)\n",
    "m.bluemarble(scale=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data we're interested in\n",
    "lat = data['Latitude'].values\n",
    "lon = data['Longitude'].values\n",
    "#population = cities['population_total'].values\n",
    "#area = cities['area_total_km2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Draw the map background\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "m = Basemap(projection='ortho', resolution=None, \n",
    "            lat_0=37, lon_0=130)\n",
    "m.shadedrelief()\n",
    "#m.drawcoastlines(color='gray')\n",
    "#m.drawcountries(color='gray')\n",
    "#m.drawstates(color='gray')\n",
    "\n",
    "# 2. scatter city data, with color reflecting population\n",
    "# and size reflecting area\n",
    "#m.scatter(lon, lat, latlon=True, cmap='Reds', alpha=0.5)\n",
    "\n",
    "# 3. create colorbar and legend\n",
    "#plt.colorbar(label=r'$\\log_{10}({\\rm population})$')\n",
    "#plt.clim(3, 7)\n",
    "\n",
    "# make legend with dummy points\n",
    "#for a in [100, 300, 500]:\n",
    "#    plt.scatter([], [], c='k', alpha=0.5, s=a,\n",
    "#                label=str(a) + ' km$^2$')\n",
    "#plt.legend(scatterpoints=1, frameon=False,\n",
    "#           labelspacing=1, loc='lower left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install conda-forge geoplot\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "#df = pd.read_csv(\"Long_Lats.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(data['Longitude'], data['Latitude'])]\n",
    "gdf = GeoDataFrame(data, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(50, 30)), marker='o', color='red', markersize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install rasterio.io\n",
    "#import rasterio.io\n",
    "import geoplot\n",
    "asia = world.query('continent == \"Asia\"')\n",
    "ax = geoplot.cartogram(\n",
    "    asia, scale='pop_est', limits=(0.2, 1),\n",
    "    edgecolor='None', figsize=(7, 8)\n",
    ")\n",
    "geoplot.polyplot(asia, edgecolor='gray', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
